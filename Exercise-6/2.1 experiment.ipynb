{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d12e2393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(791023, 42)\n",
      "(240186, 42)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils.experiment import *\n",
    "\n",
    "file_path = './Processed_Data/C.txt'\n",
    "file_path2 = './Processed_Data/B.txt'\n",
    "datasetC = np.loadtxt(file_path)\n",
    "datasetB = np.loadtxt(file_path2)\n",
    "\n",
    "print(datasetC.shape)\n",
    "print(datasetB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e73cd2fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment C/C： 1/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.09420012007281184\n",
      "  Batch 2000/15821, Average Loss: 0.06206572003290057\n",
      "  Batch 3000/15821, Average Loss: 0.041858078529126944\n",
      "  Batch 4000/15821, Average Loss: 0.03635713073145598\n",
      "  Batch 5000/15821, Average Loss: 0.03307114521414042\n",
      "  Batch 6000/15821, Average Loss: 0.03204574590269476\n",
      "  Batch 7000/15821, Average Loss: 0.031553017374128106\n",
      "  Batch 8000/15821, Average Loss: 0.03209215337131172\n",
      "  Batch 9000/15821, Average Loss: 0.03159979884605855\n",
      "  Batch 10000/15821, Average Loss: 0.031660346370190384\n",
      "  Batch 11000/15821, Average Loss: 0.031801844914443794\n",
      "  Batch 12000/15821, Average Loss: 0.031072872108314185\n",
      "  Batch 13000/15821, Average Loss: 0.03123645621072501\n",
      "  Batch 14000/15821, Average Loss: 0.0315811353046447\n",
      "  Batch 15000/15821, Average Loss: 0.03134581868071109\n",
      "  Average Training Loss: 0.038531467357710326\n",
      "  Average Training Loss: 0.038531467357710326, Average Validation Loss: 0.03139040262768389\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.031191711220890282\n",
      "  Batch 2000/15821, Average Loss: 0.03158873250056058\n",
      "  Batch 3000/15821, Average Loss: 0.031961196900345386\n",
      "  Batch 4000/15821, Average Loss: 0.03171423246525228\n",
      "  Batch 5000/15821, Average Loss: 0.03143502550944686\n",
      "  Batch 6000/15821, Average Loss: 0.03164507548324764\n",
      "  Batch 7000/15821, Average Loss: 0.03129078069608659\n",
      "  Batch 8000/15821, Average Loss: 0.03138365107402206\n",
      "  Batch 9000/15821, Average Loss: 0.03130120766628534\n",
      "  Batch 10000/15821, Average Loss: 0.03084345481172204\n",
      "  Batch 11000/15821, Average Loss: 0.03137744191475213\n",
      "  Batch 12000/15821, Average Loss: 0.03072128337342292\n",
      "  Batch 13000/15821, Average Loss: 0.031224103599786758\n",
      "  Batch 14000/15821, Average Loss: 0.030885861289687455\n",
      "  Batch 15000/15821, Average Loss: 0.030926531572826207\n",
      "  Average Training Loss: 0.03129425118551639\n",
      "  Average Training Loss: 0.03129425118551639, Average Validation Loss: 0.030923701608576135\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.030549581172410398\n",
      "  Batch 2000/15821, Average Loss: 0.030801023206673564\n",
      "  Batch 3000/15821, Average Loss: 0.030595991257578135\n",
      "  Batch 4000/15821, Average Loss: 0.030866172396577896\n",
      "  Batch 5000/15821, Average Loss: 0.030743501037359237\n",
      "  Batch 6000/15821, Average Loss: 0.030144649592228234\n",
      "  Batch 7000/15821, Average Loss: 0.030618514734320344\n",
      "  Batch 8000/15821, Average Loss: 0.030825162479653955\n",
      "  Batch 9000/15821, Average Loss: 0.030592215755023064\n",
      "  Batch 10000/15821, Average Loss: 0.031147659356705843\n",
      "  Batch 11000/15821, Average Loss: 0.030282055646181107\n",
      "  Batch 12000/15821, Average Loss: 0.030696016979403794\n",
      "  Batch 13000/15821, Average Loss: 0.0303343854136765\n",
      "  Batch 14000/15821, Average Loss: 0.03079283962864429\n",
      "  Batch 15000/15821, Average Loss: 0.030593651129864157\n",
      "  Average Training Loss: 0.030612140179388462\n",
      "  Average Training Loss: 0.030612140179388462, Average Validation Loss: 0.030272366922472312\n",
      "  Average Test Loss: 0.030458654015003896\n",
      "Experiment C/C： 2/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.08164113148860633\n",
      "  Batch 2000/15821, Average Loss: 0.04376495476346463\n",
      "  Batch 3000/15821, Average Loss: 0.034942234767600895\n",
      "  Batch 4000/15821, Average Loss: 0.030759178723208606\n",
      "  Batch 5000/15821, Average Loss: 0.028467399476096035\n",
      "  Batch 6000/15821, Average Loss: 0.02880420845095068\n",
      "  Batch 7000/15821, Average Loss: 0.02787863310147077\n",
      "  Batch 8000/15821, Average Loss: 0.027300636951811612\n",
      "  Batch 9000/15821, Average Loss: 0.027934581028297545\n",
      "  Batch 10000/15821, Average Loss: 0.027539873462170364\n",
      "  Batch 11000/15821, Average Loss: 0.027042600001208485\n",
      "  Batch 12000/15821, Average Loss: 0.027564478521235287\n",
      "  Batch 13000/15821, Average Loss: 0.027690508759580554\n",
      "  Batch 14000/15821, Average Loss: 0.02770271479524672\n",
      "  Batch 15000/15821, Average Loss: 0.027292788915801794\n",
      "  Average Training Loss: 0.03275563721326353\n",
      "  Average Training Loss: 0.03275563721326353, Average Validation Loss: 0.027209635680988973\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.02687920587603003\n",
      "  Batch 2000/15821, Average Loss: 0.027564298500306905\n",
      "  Batch 3000/15821, Average Loss: 0.0269256413243711\n",
      "  Batch 4000/15821, Average Loss: 0.026876111935824155\n",
      "  Batch 5000/15821, Average Loss: 0.026685008585453033\n",
      "  Batch 6000/15821, Average Loss: 0.0274347671167925\n",
      "  Batch 7000/15821, Average Loss: 0.027110367592424155\n",
      "  Batch 8000/15821, Average Loss: 0.02678960111644119\n",
      "  Batch 9000/15821, Average Loss: 0.027443057619966568\n",
      "  Batch 10000/15821, Average Loss: 0.027388786090537905\n",
      "  Batch 11000/15821, Average Loss: 0.026919814524240793\n",
      "  Batch 12000/15821, Average Loss: 0.02734899320732802\n",
      "  Batch 13000/15821, Average Loss: 0.027059688185341657\n",
      "  Batch 14000/15821, Average Loss: 0.026783096954692154\n",
      "  Batch 15000/15821, Average Loss: 0.02754252603277564\n",
      "  Average Training Loss: 0.027110564830580466\n",
      "  Average Training Loss: 0.027110564830580466, Average Validation Loss: 0.026872832226641566\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.027066678832285106\n",
      "  Batch 2000/15821, Average Loss: 0.02728327508829534\n",
      "  Batch 3000/15821, Average Loss: 0.026968740611337125\n",
      "  Batch 4000/15821, Average Loss: 0.027157023176550867\n",
      "  Batch 5000/15821, Average Loss: 0.02668889823742211\n",
      "  Batch 6000/15821, Average Loss: 0.027033543970435857\n",
      "  Batch 7000/15821, Average Loss: 0.026772765649482607\n",
      "  Batch 8000/15821, Average Loss: 0.027507765529211612\n",
      "  Batch 9000/15821, Average Loss: 0.02729359230119735\n",
      "  Batch 10000/15821, Average Loss: 0.02648972353246063\n",
      "  Batch 11000/15821, Average Loss: 0.02648267322406173\n",
      "  Batch 12000/15821, Average Loss: 0.026974878964945673\n",
      "  Batch 13000/15821, Average Loss: 0.027239162866026165\n",
      "  Batch 14000/15821, Average Loss: 0.02725134050101042\n",
      "  Batch 15000/15821, Average Loss: 0.026969151523895563\n",
      "  Average Training Loss: 0.026990445971233002\n",
      "  Average Training Loss: 0.026990445971233002, Average Validation Loss: 0.027106755445515406\n",
      "  Average Test Loss: 0.02738049515173182\n",
      "Experiment C/C： 3/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.08365788242407143\n",
      "  Batch 2000/15821, Average Loss: 0.04508790149353445\n",
      "  Batch 3000/15821, Average Loss: 0.03524624158348888\n",
      "  Batch 4000/15821, Average Loss: 0.031459155194461345\n",
      "  Batch 5000/15821, Average Loss: 0.030407619933597745\n",
      "  Batch 6000/15821, Average Loss: 0.02918329088948667\n",
      "  Batch 7000/15821, Average Loss: 0.02843683017604053\n",
      "  Batch 8000/15821, Average Loss: 0.028195547674782575\n",
      "  Batch 9000/15821, Average Loss: 0.02787532245926559\n",
      "  Batch 10000/15821, Average Loss: 0.026925400676205753\n",
      "  Batch 11000/15821, Average Loss: 0.02705684477556497\n",
      "  Batch 12000/15821, Average Loss: 0.027133842543698848\n",
      "  Batch 13000/15821, Average Loss: 0.025915286575444042\n",
      "  Batch 14000/15821, Average Loss: 0.026010639117565005\n",
      "  Batch 15000/15821, Average Loss: 0.02605282932659611\n",
      "  Average Training Loss: 0.032832620662046216\n",
      "  Average Training Loss: 0.032832620662046216, Average Validation Loss: 0.025809206496313828\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.025308001627679915\n",
      "  Batch 2000/15821, Average Loss: 0.025342401534318923\n",
      "  Batch 3000/15821, Average Loss: 0.02559863744303584\n",
      "  Batch 4000/15821, Average Loss: 0.024956460535526276\n",
      "  Batch 5000/15821, Average Loss: 0.02528832368925214\n",
      "  Batch 6000/15821, Average Loss: 0.024917742064688355\n",
      "  Batch 7000/15821, Average Loss: 0.02489798309188336\n",
      "  Batch 8000/15821, Average Loss: 0.025141501545906067\n",
      "  Batch 9000/15821, Average Loss: 0.024776298698037862\n",
      "  Batch 10000/15821, Average Loss: 0.024778934709727765\n",
      "  Batch 11000/15821, Average Loss: 0.024522677406668664\n",
      "  Batch 12000/15821, Average Loss: 0.02435612903255969\n",
      "  Batch 13000/15821, Average Loss: 0.02491576016321778\n",
      "  Batch 14000/15821, Average Loss: 0.024732874050270765\n",
      "  Batch 15000/15821, Average Loss: 0.02471418178919703\n",
      "  Average Training Loss: 0.024960165291761597\n",
      "  Average Training Loss: 0.024960165291761597, Average Validation Loss: 0.024640044061904832\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.024724177580326797\n",
      "  Batch 2000/15821, Average Loss: 0.02471008887561038\n",
      "  Batch 3000/15821, Average Loss: 0.024497840428724885\n",
      "  Batch 4000/15821, Average Loss: 0.02472626274265349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5000/15821, Average Loss: 0.02482600113004446\n",
      "  Batch 6000/15821, Average Loss: 0.02427558250539005\n",
      "  Batch 7000/15821, Average Loss: 0.024513501045759768\n",
      "  Batch 8000/15821, Average Loss: 0.024360107418149708\n",
      "  Batch 9000/15821, Average Loss: 0.024530191801022738\n",
      "  Batch 10000/15821, Average Loss: 0.02425592167582363\n",
      "  Batch 11000/15821, Average Loss: 0.02395717217773199\n",
      "  Batch 12000/15821, Average Loss: 0.024290359454695137\n",
      "  Batch 13000/15821, Average Loss: 0.024092105624265968\n",
      "  Batch 14000/15821, Average Loss: 0.024185702455695717\n",
      "  Batch 15000/15821, Average Loss: 0.02460740700084716\n",
      "  Average Training Loss: 0.024431249014975558\n",
      "  Average Training Loss: 0.024431249014975558, Average Validation Loss: 0.02436802630737713\n",
      "  Average Test Loss: 0.024443606376959647\n",
      "Experiment C/C： 4/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.10381928842887282\n",
      "  Batch 2000/15821, Average Loss: 0.05960822114534676\n",
      "  Batch 3000/15821, Average Loss: 0.034856657371856274\n",
      "  Batch 4000/15821, Average Loss: 0.03152384277898818\n",
      "  Batch 5000/15821, Average Loss: 0.029866575597785414\n",
      "  Batch 6000/15821, Average Loss: 0.02906682353746146\n",
      "  Batch 7000/15821, Average Loss: 0.028856529899872838\n",
      "  Batch 8000/15821, Average Loss: 0.02774281281232834\n",
      "  Batch 9000/15821, Average Loss: 0.027572901754640042\n",
      "  Batch 10000/15821, Average Loss: 0.027125502097420394\n",
      "  Batch 11000/15821, Average Loss: 0.026823847404681148\n",
      "  Batch 12000/15821, Average Loss: 0.026143149097450077\n",
      "  Batch 13000/15821, Average Loss: 0.025548063334543257\n",
      "  Batch 14000/15821, Average Loss: 0.025546038422733546\n",
      "  Batch 15000/15821, Average Loss: 0.025351957022212445\n",
      "  Average Training Loss: 0.034788857330089844\n",
      "  Average Training Loss: 0.034788857330089844, Average Validation Loss: 0.024937760161159072\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.025081163158640264\n",
      "  Batch 2000/15821, Average Loss: 0.02470180863654241\n",
      "  Batch 3000/15821, Average Loss: 0.02455843469593674\n",
      "  Batch 4000/15821, Average Loss: 0.024410940567031504\n",
      "  Batch 5000/15821, Average Loss: 0.02458453505206853\n",
      "  Batch 6000/15821, Average Loss: 0.02450932790711522\n",
      "  Batch 7000/15821, Average Loss: 0.02437350698746741\n",
      "  Batch 8000/15821, Average Loss: 0.024209103248082103\n",
      "  Batch 9000/15821, Average Loss: 0.024071559901349248\n",
      "  Batch 10000/15821, Average Loss: 0.02413718715775758\n",
      "  Batch 11000/15821, Average Loss: 0.02418570702150464\n",
      "  Batch 12000/15821, Average Loss: 0.02407158939121291\n",
      "  Batch 13000/15821, Average Loss: 0.023608048820868133\n",
      "  Batch 14000/15821, Average Loss: 0.023858266313094647\n",
      "  Batch 15000/15821, Average Loss: 0.024049620203208177\n",
      "  Average Training Loss: 0.02425905763091614\n",
      "  Average Training Loss: 0.02425905763091614, Average Validation Loss: 0.023740219021808284\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.024015171945560722\n",
      "  Batch 2000/15821, Average Loss: 0.023693408545572312\n",
      "  Batch 3000/15821, Average Loss: 0.02369138720491901\n",
      "  Batch 4000/15821, Average Loss: 0.023699164723977446\n",
      "  Batch 5000/15821, Average Loss: 0.02332127255294472\n",
      "  Batch 6000/15821, Average Loss: 0.02336197418440133\n",
      "  Batch 7000/15821, Average Loss: 0.023193058281671255\n",
      "  Batch 8000/15821, Average Loss: 0.02303725858591497\n",
      "  Batch 9000/15821, Average Loss: 0.023464544913731514\n",
      "  Batch 10000/15821, Average Loss: 0.023508719881530853\n",
      "  Batch 11000/15821, Average Loss: 0.022905479231849313\n",
      "  Batch 12000/15821, Average Loss: 0.02316263121459633\n",
      "  Batch 13000/15821, Average Loss: 0.022743403712287544\n",
      "  Batch 14000/15821, Average Loss: 0.02272300615813583\n",
      "  Batch 15000/15821, Average Loss: 0.02258084383793175\n",
      "  Average Training Loss: 0.023240425415508033\n",
      "  Average Training Loss: 0.023240425415508033, Average Validation Loss: 0.022974808423660852\n",
      "  Average Test Loss: 0.023191262999783985\n",
      "Experiment C/C： 5/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.10732193971052766\n",
      "  Batch 2000/15821, Average Loss: 0.06686046974547208\n",
      "  Batch 3000/15821, Average Loss: 0.03553978970926255\n",
      "  Batch 4000/15821, Average Loss: 0.03263283962570131\n",
      "  Batch 5000/15821, Average Loss: 0.03005975178722292\n",
      "  Batch 6000/15821, Average Loss: 0.028939271815121174\n",
      "  Batch 7000/15821, Average Loss: 0.028358719433657826\n",
      "  Batch 8000/15821, Average Loss: 0.027678487034514546\n",
      "  Batch 9000/15821, Average Loss: 0.02726852700021118\n",
      "  Batch 10000/15821, Average Loss: 0.026773018716368824\n",
      "  Batch 11000/15821, Average Loss: 0.026266568437218665\n",
      "  Batch 12000/15821, Average Loss: 0.025960870917886496\n",
      "  Batch 13000/15821, Average Loss: 0.025886861726641654\n",
      "  Batch 14000/15821, Average Loss: 0.025676210169680417\n",
      "  Batch 15000/15821, Average Loss: 0.024846095636487008\n",
      "  Average Training Loss: 0.035441274987371804\n",
      "  Average Training Loss: 0.035441274987371804, Average Validation Loss: 0.02469250317293741\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.024870296880602837\n",
      "  Batch 2000/15821, Average Loss: 0.023976165377534928\n",
      "  Batch 3000/15821, Average Loss: 0.024013944699428975\n",
      "  Batch 4000/15821, Average Loss: 0.02382348864711821\n",
      "  Batch 5000/15821, Average Loss: 0.02359241415001452\n",
      "  Batch 6000/15821, Average Loss: 0.02324517207359895\n",
      "  Batch 7000/15821, Average Loss: 0.0230902943094261\n",
      "  Batch 8000/15821, Average Loss: 0.02284760667104274\n",
      "  Batch 9000/15821, Average Loss: 0.02248472049366683\n",
      "  Batch 10000/15821, Average Loss: 0.0226138623515144\n",
      "  Batch 11000/15821, Average Loss: 0.02253949423786253\n",
      "  Batch 12000/15821, Average Loss: 0.022081723461858927\n",
      "  Batch 13000/15821, Average Loss: 0.022191319360397758\n",
      "  Batch 14000/15821, Average Loss: 0.022705751575063913\n",
      "  Batch 15000/15821, Average Loss: 0.022423391268588604\n",
      "  Average Training Loss: 0.0230796415495999\n",
      "  Average Training Loss: 0.0230796415495999, Average Validation Loss: 0.022546798794780495\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.022210973689798266\n",
      "  Batch 2000/15821, Average Loss: 0.022085649737156928\n",
      "  Batch 3000/15821, Average Loss: 0.022300162724684925\n",
      "  Batch 4000/15821, Average Loss: 0.022115184729918837\n",
      "  Batch 5000/15821, Average Loss: 0.022083315272815526\n",
      "  Batch 6000/15821, Average Loss: 0.02203080892888829\n",
      "  Batch 7000/15821, Average Loss: 0.0220354071543552\n",
      "  Batch 8000/15821, Average Loss: 0.021769029974006118\n",
      "  Batch 9000/15821, Average Loss: 0.021489677275530993\n",
      "  Batch 10000/15821, Average Loss: 0.022014258356299252\n",
      "  Batch 11000/15821, Average Loss: 0.021484156883787364\n",
      "  Batch 12000/15821, Average Loss: 0.021564290165435523\n",
      "  Batch 13000/15821, Average Loss: 0.02163441009260714\n",
      "  Batch 14000/15821, Average Loss: 0.02135692112194374\n",
      "  Batch 15000/15821, Average Loss: 0.021400222810450942\n",
      "  Average Training Loss: 0.02181847615918161\n",
      "  Average Training Loss: 0.02181847615918161, Average Validation Loss: 0.02135889163194827\n",
      "  Average Test Loss: 0.02155223697990356\n",
      "Experiment C/C： 6/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.19251961390674113\n",
      "  Batch 2000/15821, Average Loss: 0.09824363841861486\n",
      "  Batch 3000/15821, Average Loss: 0.07637174526788294\n",
      "  Batch 4000/15821, Average Loss: 0.04115896510053426\n",
      "  Batch 5000/15821, Average Loss: 0.03376429823692888\n",
      "  Batch 6000/15821, Average Loss: 0.032435773859731856\n",
      "  Batch 7000/15821, Average Loss: 0.03068079165648669\n",
      "  Batch 8000/15821, Average Loss: 0.0300328985247761\n",
      "  Batch 9000/15821, Average Loss: 0.028560008907690643\n",
      "  Batch 10000/15821, Average Loss: 0.027554364586248995\n",
      "  Batch 11000/15821, Average Loss: 0.02719664853066206\n",
      "  Batch 12000/15821, Average Loss: 0.026202739753760397\n",
      "  Batch 13000/15821, Average Loss: 0.025332390966825186\n",
      "  Batch 14000/15821, Average Loss: 0.02488850662484765\n",
      "  Batch 15000/15821, Average Loss: 0.02403268318064511\n",
      "  Average Training Loss: 0.046718520749945217\n",
      "  Average Training Loss: 0.046718520749945217, Average Validation Loss: 0.023843135449825862\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.02401400134852156\n",
      "  Batch 2000/15821, Average Loss: 0.023529916057363154\n",
      "  Batch 3000/15821, Average Loss: 0.023668030739296228\n",
      "  Batch 4000/15821, Average Loss: 0.023282265739049762\n",
      "  Batch 5000/15821, Average Loss: 0.02313414827734232\n",
      "  Batch 6000/15821, Average Loss: 0.022765076840296387\n",
      "  Batch 7000/15821, Average Loss: 0.02349942171294242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 8000/15821, Average Loss: 0.023072204134892672\n",
      "  Batch 9000/15821, Average Loss: 0.022864742706064135\n",
      "  Batch 10000/15821, Average Loss: 0.02282267004530877\n",
      "  Batch 11000/15821, Average Loss: 0.022717428307048978\n",
      "  Batch 12000/15821, Average Loss: 0.023194736977107822\n",
      "  Batch 13000/15821, Average Loss: 0.02275174288265407\n",
      "  Batch 14000/15821, Average Loss: 0.02256892096810043\n",
      "  Batch 15000/15821, Average Loss: 0.02238189085526392\n",
      "  Average Training Loss: 0.02305426270931217\n",
      "  Average Training Loss: 0.02305426270931217, Average Validation Loss: 0.02256209757674446\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.022871436725370585\n",
      "  Batch 2000/15821, Average Loss: 0.022521635048557073\n",
      "  Batch 3000/15821, Average Loss: 0.022511112083215268\n",
      "  Batch 4000/15821, Average Loss: 0.022555476392619313\n",
      "  Batch 5000/15821, Average Loss: 0.022395460679661482\n",
      "  Batch 6000/15821, Average Loss: 0.02243664305144921\n",
      "  Batch 7000/15821, Average Loss: 0.02220275559509173\n",
      "  Batch 8000/15821, Average Loss: 0.022213658722583206\n",
      "  Batch 9000/15821, Average Loss: 0.02224330277694389\n",
      "  Batch 10000/15821, Average Loss: 0.021965572068467738\n",
      "  Batch 11000/15821, Average Loss: 0.02254750559432432\n",
      "  Batch 12000/15821, Average Loss: 0.021945946501567958\n",
      "  Batch 13000/15821, Average Loss: 0.0220842156955041\n",
      "  Batch 14000/15821, Average Loss: 0.022070529879070818\n",
      "  Batch 15000/15821, Average Loss: 0.021710561267565934\n",
      "  Average Training Loss: 0.022267605154040727\n",
      "  Average Training Loss: 0.022267605154040727, Average Validation Loss: 0.02192459860251475\n",
      "  Average Test Loss: 0.022076903239713702\n",
      "Experiment C/C： 7/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/15821, Average Loss: 0.09752665865048767\n",
      "  Batch 2000/15821, Average Loss: 0.04549554999917746\n",
      "  Batch 3000/15821, Average Loss: 0.03234634136967361\n",
      "  Batch 4000/15821, Average Loss: 0.029358983603306115\n",
      "  Batch 5000/15821, Average Loss: 0.02768574369698763\n",
      "  Batch 6000/15821, Average Loss: 0.027354557384736834\n",
      "  Batch 7000/15821, Average Loss: 0.027366000422276555\n",
      "  Batch 8000/15821, Average Loss: 0.026745188075117766\n",
      "  Batch 9000/15821, Average Loss: 0.026804583365097643\n",
      "  Batch 10000/15821, Average Loss: 0.025856503829360007\n",
      "  Batch 11000/15821, Average Loss: 0.025787426565773784\n",
      "  Batch 12000/15821, Average Loss: 0.02583533341065049\n",
      "  Batch 13000/15821, Average Loss: 0.02504599747667089\n",
      "  Batch 14000/15821, Average Loss: 0.02464307287428528\n",
      "  Batch 15000/15821, Average Loss: 0.024452009697444738\n",
      "  Average Training Loss: 0.03237710575417798\n",
      "  Average Training Loss: 0.03237710575417798, Average Validation Loss: 0.024350815898140525\n",
      "Epoch 2/3: \n",
      "  Batch 1000/15821, Average Loss: 0.02383849465986714\n",
      "  Batch 2000/15821, Average Loss: 0.02407616996858269\n",
      "  Batch 3000/15821, Average Loss: 0.023212626073043793\n",
      "  Batch 4000/15821, Average Loss: 0.023755471877753735\n",
      "  Batch 5000/15821, Average Loss: 0.023707759741228074\n",
      "  Batch 6000/15821, Average Loss: 0.023049608868081124\n",
      "  Batch 7000/15821, Average Loss: 0.0233711491455324\n",
      "  Batch 8000/15821, Average Loss: 0.02304163180384785\n",
      "  Batch 9000/15821, Average Loss: 0.0231553723141551\n",
      "  Batch 10000/15821, Average Loss: 0.023037286116275937\n",
      "  Batch 11000/15821, Average Loss: 0.022669444816187025\n",
      "  Batch 12000/15821, Average Loss: 0.02258148976881057\n",
      "  Batch 13000/15821, Average Loss: 0.022682583056855946\n",
      "  Batch 14000/15821, Average Loss: 0.022836565110832454\n",
      "  Batch 15000/15821, Average Loss: 0.02299397854367271\n",
      "  Average Training Loss: 0.023179671684590026\n",
      "  Average Training Loss: 0.023179671684590026, Average Validation Loss: 0.022603922027097652\n",
      "Epoch 3/3: \n",
      "  Batch 1000/15821, Average Loss: 0.022991437654942274\n",
      "  Batch 2000/15821, Average Loss: 0.022577848257496952\n",
      "  Batch 3000/15821, Average Loss: 0.022481070894747972\n",
      "  Batch 4000/15821, Average Loss: 0.02272629609005526\n",
      "  Batch 5000/15821, Average Loss: 0.022586799988988788\n",
      "  Batch 6000/15821, Average Loss: 0.022509929122403263\n",
      "  Batch 7000/15821, Average Loss: 0.02247698592208326\n",
      "  Batch 8000/15821, Average Loss: 0.022127107149455698\n",
      "  Batch 9000/15821, Average Loss: 0.022105783379636704\n",
      "  Batch 10000/15821, Average Loss: 0.022392552711535244\n",
      "  Batch 11000/15821, Average Loss: 0.022179062908515335\n",
      "  Batch 12000/15821, Average Loss: 0.021872114777565003\n",
      "  Batch 13000/15821, Average Loss: 0.021583640796598046\n",
      "  Batch 14000/15821, Average Loss: 0.02173280840506777\n",
      "  Batch 15000/15821, Average Loss: 0.02188916293904185\n",
      "  Average Training Loss: 0.022270072811582154\n",
      "  Average Training Loss: 0.022270072811582154, Average Validation Loss: 0.02240140256200673\n",
      "  Average Test Loss: 0.022547710732711314\n",
      "Experiment B/B： 8/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.7417569441348314\n",
      "  Batch 2000/4804, Average Loss: 0.09738786423951387\n",
      "  Batch 3000/4804, Average Loss: 0.08703828989714384\n",
      "  Batch 4000/4804, Average Loss: 0.0809496993161738\n",
      "  Average Training Loss: 0.22187487081819207\n",
      "  Average Training Loss: 0.22187487081819207, Average Validation Loss: 0.06823190979996867\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.06228147456049919\n",
      "  Batch 2000/4804, Average Loss: 0.050515755884349346\n",
      "  Batch 3000/4804, Average Loss: 0.042604137295857075\n",
      "  Batch 4000/4804, Average Loss: 0.03785872980393469\n",
      "  Average Training Loss: 0.046263325126992105\n",
      "  Average Training Loss: 0.046263325126992105, Average Validation Loss: 0.035571730637379714\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.03528875172603876\n",
      "  Batch 2000/4804, Average Loss: 0.03490379173029214\n",
      "  Batch 3000/4804, Average Loss: 0.03481440803501755\n",
      "  Batch 4000/4804, Average Loss: 0.034186627228744326\n",
      "  Average Training Loss: 0.0347321483042544\n",
      "  Average Training Loss: 0.0347321483042544, Average Validation Loss: 0.03464730404882869\n",
      "  Average Test Loss: 0.03443979244248059\n",
      "Experiment B/B： 9/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.09156272967159748\n",
      "  Batch 2000/4804, Average Loss: 0.06842638346925378\n",
      "  Batch 3000/4804, Average Loss: 0.05324804278463125\n",
      "  Batch 4000/4804, Average Loss: 0.04299075457081199\n",
      "  Average Training Loss: 0.0596401534069618\n",
      "  Average Training Loss: 0.0596401534069618, Average Validation Loss: 0.03580558789675725\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.03343625614605844\n",
      "  Batch 2000/4804, Average Loss: 0.02966111086960882\n",
      "  Batch 3000/4804, Average Loss: 0.02765065001696348\n",
      "  Batch 4000/4804, Average Loss: 0.0267959706755355\n",
      "  Average Training Loss: 0.02886105174695814\n",
      "  Average Training Loss: 0.02886105174695814, Average Validation Loss: 0.02624431903227251\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.02608397578820586\n",
      "  Batch 2000/4804, Average Loss: 0.025380437901243568\n",
      "  Batch 3000/4804, Average Loss: 0.025121315919794142\n",
      "  Batch 4000/4804, Average Loss: 0.024852542440406977\n",
      "  Average Training Loss: 0.025234313875779533\n",
      "  Average Training Loss: 0.025234313875779533, Average Validation Loss: 0.02491380077800385\n",
      "  Average Test Loss: 0.024505708842762403\n",
      "Experiment B/B： 10/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.13086708940565586\n",
      "  Batch 2000/4804, Average Loss: 0.0741588530316949\n",
      "  Batch 3000/4804, Average Loss: 0.06270461639389396\n",
      "  Batch 4000/4804, Average Loss: 0.05097402180731297\n",
      "  Average Training Loss: 0.07358795178660209\n",
      "  Average Training Loss: 0.07358795178660209, Average Validation Loss: 0.04025193454124797\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.036711887376382944\n",
      "  Batch 2000/4804, Average Loss: 0.03206569403782487\n",
      "  Batch 3000/4804, Average Loss: 0.028404241503216326\n",
      "  Batch 4000/4804, Average Loss: 0.02753587331995368\n",
      "  Average Training Loss: 0.03046985382026528\n",
      "  Average Training Loss: 0.03046985382026528, Average Validation Loss: 0.026844819109222186\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.026791058604605497\n",
      "  Batch 2000/4804, Average Loss: 0.02594173436425626\n",
      "  Batch 3000/4804, Average Loss: 0.025982299755327405\n",
      "  Batch 4000/4804, Average Loss: 0.025748691036365925\n",
      "  Average Training Loss: 0.026046276095437255\n",
      "  Average Training Loss: 0.026046276095437255, Average Validation Loss: 0.025732818791288112\n",
      "  Average Test Loss: 0.025336115982392795\n",
      "Experiment B/B： 11/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.09769124717265368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2000/4804, Average Loss: 0.07554442379623652\n",
      "  Batch 3000/4804, Average Loss: 0.05341719298623502\n",
      "  Batch 4000/4804, Average Loss: 0.04208901165984571\n",
      "  Average Training Loss: 0.06239593757950793\n",
      "  Average Training Loss: 0.06239593757950793, Average Validation Loss: 0.03744506497139461\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.03657979558687657\n",
      "  Batch 2000/4804, Average Loss: 0.03600236494187266\n",
      "  Batch 3000/4804, Average Loss: 0.03550861683767289\n",
      "  Batch 4000/4804, Average Loss: 0.03417877216357738\n",
      "  Average Training Loss: 0.03535294895379073\n",
      "  Average Training Loss: 0.03535294895379073, Average Validation Loss: 0.03436001664717073\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.033629999892786144\n",
      "  Batch 2000/4804, Average Loss: 0.033322002398781476\n",
      "  Batch 3000/4804, Average Loss: 0.03300635425373912\n",
      "  Batch 4000/4804, Average Loss: 0.032622280379757285\n",
      "  Average Training Loss: 0.03298174546252964\n",
      "  Average Training Loss: 0.03298174546252964, Average Validation Loss: 0.031858062364170936\n",
      "  Average Test Loss: 0.031452917292261254\n",
      "Experiment B/B： 12/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.1481608959622681\n",
      "  Batch 2000/4804, Average Loss: 0.10231468914449215\n",
      "  Batch 3000/4804, Average Loss: 0.09020427295565606\n",
      "  Batch 4000/4804, Average Loss: 0.07280393937416375\n",
      "  Average Training Loss: 0.09432070805527562\n",
      "  Average Training Loss: 0.09432070805527562, Average Validation Loss: 0.04205087891912532\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.04003614847175777\n",
      "  Batch 2000/4804, Average Loss: 0.037071763951331375\n",
      "  Batch 3000/4804, Average Loss: 0.03190403943601996\n",
      "  Batch 4000/4804, Average Loss: 0.02938333828840405\n",
      "  Average Training Loss: 0.033434696183799704\n",
      "  Average Training Loss: 0.033434696183799704, Average Validation Loss: 0.02794675262813542\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.027292463176883757\n",
      "  Batch 2000/4804, Average Loss: 0.026892165487632156\n",
      "  Batch 3000/4804, Average Loss: 0.026000325406901538\n",
      "  Batch 4000/4804, Average Loss: 0.025741503279656172\n",
      "  Average Training Loss: 0.026319456646152206\n",
      "  Average Training Loss: 0.026319456646152206, Average Validation Loss: 0.025689783018732197\n",
      "  Average Test Loss: 0.02540742519228633\n",
      "Experiment B/B： 13/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.20886618581414224\n",
      "  Batch 2000/4804, Average Loss: 0.1006383705586195\n",
      "  Batch 3000/4804, Average Loss: 0.09455720329284668\n",
      "  Batch 4000/4804, Average Loss: 0.08372856365889311\n",
      "  Average Training Loss: 0.11318680041157125\n",
      "  Average Training Loss: 0.11318680041157125, Average Validation Loss: 0.06285391645890459\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.053399089278653264\n",
      "  Batch 2000/4804, Average Loss: 0.04115351361595094\n",
      "  Batch 3000/4804, Average Loss: 0.03461692834272981\n",
      "  Batch 4000/4804, Average Loss: 0.031317871351726354\n",
      "  Average Training Loss: 0.03844629524533112\n",
      "  Average Training Loss: 0.03844629524533112, Average Validation Loss: 0.02999014102856315\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.029738216034136712\n",
      "  Batch 2000/4804, Average Loss: 0.02907639961782843\n",
      "  Batch 3000/4804, Average Loss: 0.028828601338900625\n",
      "  Batch 4000/4804, Average Loss: 0.028655243271961808\n",
      "  Average Training Loss: 0.028967753095361565\n",
      "  Average Training Loss: 0.028967753095361565, Average Validation Loss: 0.028362822379128197\n",
      "  Average Test Loss: 0.027989941381606633\n",
      "Experiment B/B： 14/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/4804, Average Loss: 0.0961958058513701\n",
      "  Batch 2000/4804, Average Loss: 0.07044314431585372\n",
      "  Batch 3000/4804, Average Loss: 0.04622379071265459\n",
      "  Batch 4000/4804, Average Loss: 0.039212202658876774\n",
      "  Average Training Loss: 0.058765755601453716\n",
      "  Average Training Loss: 0.058765755601453716, Average Validation Loss: 0.03690923917381348\n",
      "Epoch 2/3: \n",
      "  Batch 1000/4804, Average Loss: 0.03639950240124017\n",
      "  Batch 2000/4804, Average Loss: 0.03359283476881683\n",
      "  Batch 3000/4804, Average Loss: 0.030575330764986575\n",
      "  Batch 4000/4804, Average Loss: 0.028098165560513734\n",
      "  Average Training Loss: 0.03128456502402981\n",
      "  Average Training Loss: 0.03128456502402981, Average Validation Loss: 0.026616398078301417\n",
      "Epoch 3/3: \n",
      "  Batch 1000/4804, Average Loss: 0.025892203950323163\n",
      "  Batch 2000/4804, Average Loss: 0.025624911389313637\n",
      "  Batch 3000/4804, Average Loss: 0.024999824256636202\n",
      "  Batch 4000/4804, Average Loss: 0.02460613388940692\n",
      "  Average Training Loss: 0.02508365682796959\n",
      "  Average Training Loss: 0.02508365682796959, Average Validation Loss: 0.024465312076343122\n",
      "  Average Test Loss: 0.024047514765704055\n",
      "Experiment C/B： 15/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.10326856774836779\n",
      "  Batch 2000/19776, Average Loss: 0.06039236161671579\n",
      "  Batch 3000/19776, Average Loss: 0.03979486550576985\n",
      "  Batch 4000/19776, Average Loss: 0.03533006665762514\n",
      "  Batch 5000/19776, Average Loss: 0.03186299319937825\n",
      "  Batch 6000/19776, Average Loss: 0.031216759922914207\n",
      "  Batch 7000/19776, Average Loss: 0.03118696498963982\n",
      "  Batch 8000/19776, Average Loss: 0.030862787045538426\n",
      "  Batch 9000/19776, Average Loss: 0.031181393553502856\n",
      "  Batch 10000/19776, Average Loss: 0.031128238557837903\n",
      "  Batch 11000/19776, Average Loss: 0.03053591745533049\n",
      "  Batch 12000/19776, Average Loss: 0.030634058374911545\n",
      "  Batch 13000/19776, Average Loss: 0.0298542319284752\n",
      "  Batch 14000/19776, Average Loss: 0.030031599421519787\n",
      "  Batch 15000/19776, Average Loss: 0.030359943497925996\n",
      "  Batch 16000/19776, Average Loss: 0.03026352440100163\n",
      "  Batch 17000/19776, Average Loss: 0.030412165858782828\n",
      "  Batch 18000/19776, Average Loss: 0.029677760684862733\n",
      "  Batch 19000/19776, Average Loss: 0.03033579223137349\n",
      "  Average Training Loss: 0.03648478019601401\n",
      "  Average Training Loss: 0.03648478019601401, Average Validation Loss: 0.0301485355567823\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.03001585492119193\n",
      "  Batch 2000/19776, Average Loss: 0.030036864143796266\n",
      "  Batch 3000/19776, Average Loss: 0.030209640614688397\n",
      "  Batch 4000/19776, Average Loss: 0.029879390807822346\n",
      "  Batch 5000/19776, Average Loss: 0.030560206124559043\n",
      "  Batch 6000/19776, Average Loss: 0.029902734898962082\n",
      "  Batch 7000/19776, Average Loss: 0.030160601259209215\n",
      "  Batch 8000/19776, Average Loss: 0.029734916464425623\n",
      "  Batch 9000/19776, Average Loss: 0.029981160456314684\n",
      "  Batch 10000/19776, Average Loss: 0.030306317403912545\n",
      "  Batch 11000/19776, Average Loss: 0.030279193005524577\n",
      "  Batch 12000/19776, Average Loss: 0.030236553507857023\n",
      "  Batch 13000/19776, Average Loss: 0.030713392270728945\n",
      "  Batch 14000/19776, Average Loss: 0.029783687566407025\n",
      "  Batch 15000/19776, Average Loss: 0.029988237391225993\n",
      "  Batch 16000/19776, Average Loss: 0.030131015868857504\n",
      "  Batch 17000/19776, Average Loss: 0.029923011783510447\n",
      "  Batch 18000/19776, Average Loss: 0.030597762467339636\n",
      "  Batch 19000/19776, Average Loss: 0.03023683715239167\n",
      "  Average Training Loss: 0.030114986832469513\n",
      "  Average Training Loss: 0.030114986832469513, Average Validation Loss: 0.030180746014870426\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.0304362757736817\n",
      "  Batch 2000/19776, Average Loss: 0.030381427862215787\n",
      "  Batch 3000/19776, Average Loss: 0.0306383274840191\n",
      "  Batch 4000/19776, Average Loss: 0.029798071563243867\n",
      "  Batch 5000/19776, Average Loss: 0.030388388843275608\n",
      "  Batch 6000/19776, Average Loss: 0.029606977342627943\n",
      "  Batch 7000/19776, Average Loss: 0.030510879784822465\n",
      "  Batch 8000/19776, Average Loss: 0.030077846587635577\n",
      "  Batch 9000/19776, Average Loss: 0.03017814788967371\n",
      "  Batch 10000/19776, Average Loss: 0.029641416555270552\n",
      "  Batch 11000/19776, Average Loss: 0.029845677704084663\n",
      "  Batch 12000/19776, Average Loss: 0.030167588493786753\n",
      "  Batch 13000/19776, Average Loss: 0.03026351282466203\n",
      "  Batch 14000/19776, Average Loss: 0.029819657302461566\n",
      "  Batch 15000/19776, Average Loss: 0.030064982146024703\n",
      "  Batch 16000/19776, Average Loss: 0.030164300834760068\n",
      "  Batch 17000/19776, Average Loss: 0.029775408153422177\n",
      "  Batch 18000/19776, Average Loss: 0.02971570857428014\n",
      "  Batch 19000/19776, Average Loss: 0.029817034151405096\n",
      "  Average Training Loss: 0.030048184207153453\n",
      "  Average Training Loss: 0.030048184207153453, Average Validation Loss: 0.029900835115314855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Test Loss: 0.10526126922024356\n",
      "Experiment C/B： 16/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.08625344746746123\n",
      "  Batch 2000/19776, Average Loss: 0.043847069767303765\n",
      "  Batch 3000/19776, Average Loss: 0.033662733835168185\n",
      "  Batch 4000/19776, Average Loss: 0.030702496856451034\n",
      "  Batch 5000/19776, Average Loss: 0.028772402646951377\n",
      "  Batch 6000/19776, Average Loss: 0.02855177331343293\n",
      "  Batch 7000/19776, Average Loss: 0.028620615848340095\n",
      "  Batch 8000/19776, Average Loss: 0.027371344630606472\n",
      "  Batch 9000/19776, Average Loss: 0.027228610034100712\n",
      "  Batch 10000/19776, Average Loss: 0.027690837672911583\n",
      "  Batch 11000/19776, Average Loss: 0.02761056231148541\n",
      "  Batch 12000/19776, Average Loss: 0.02725580476410687\n",
      "  Batch 13000/19776, Average Loss: 0.027638058804906905\n",
      "  Batch 14000/19776, Average Loss: 0.026756124447099863\n",
      "  Batch 15000/19776, Average Loss: 0.02793335949536413\n",
      "  Batch 16000/19776, Average Loss: 0.02748162925057113\n",
      "  Batch 17000/19776, Average Loss: 0.027189642638899385\n",
      "  Batch 18000/19776, Average Loss: 0.027072423180565238\n",
      "  Batch 19000/19776, Average Loss: 0.02733602465316653\n",
      "  Average Training Loss: 0.03186809475672873\n",
      "  Average Training Loss: 0.03186809475672873, Average Validation Loss: 0.027294328828450762\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02697628231998533\n",
      "  Batch 2000/19776, Average Loss: 0.02708014300093055\n",
      "  Batch 3000/19776, Average Loss: 0.027300704014021904\n",
      "  Batch 4000/19776, Average Loss: 0.02728778388723731\n",
      "  Batch 5000/19776, Average Loss: 0.027228602844290434\n",
      "  Batch 6000/19776, Average Loss: 0.02723878609854728\n",
      "  Batch 7000/19776, Average Loss: 0.027045641018077732\n",
      "  Batch 8000/19776, Average Loss: 0.026627254484687002\n",
      "  Batch 9000/19776, Average Loss: 0.027401723173912616\n",
      "  Batch 10000/19776, Average Loss: 0.0268036576686427\n",
      "  Batch 11000/19776, Average Loss: 0.027344823574647306\n",
      "  Batch 12000/19776, Average Loss: 0.027330903462599962\n",
      "  Batch 13000/19776, Average Loss: 0.0270483211344108\n",
      "  Batch 14000/19776, Average Loss: 0.0273970184950158\n",
      "  Batch 15000/19776, Average Loss: 0.027174962766468524\n",
      "  Batch 16000/19776, Average Loss: 0.02679070492228493\n",
      "  Batch 17000/19776, Average Loss: 0.02715877932542935\n",
      "  Batch 18000/19776, Average Loss: 0.027737712340429425\n",
      "  Batch 19000/19776, Average Loss: 0.027165618159808218\n",
      "  Average Training Loss: 0.02717343735694923\n",
      "  Average Training Loss: 0.02717343735694923, Average Validation Loss: 0.02703386812757559\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.027267214285209774\n",
      "  Batch 2000/19776, Average Loss: 0.026768978926353156\n",
      "  Batch 3000/19776, Average Loss: 0.027176004866138102\n",
      "  Batch 4000/19776, Average Loss: 0.02679002769757062\n",
      "  Batch 5000/19776, Average Loss: 0.027263243068009615\n",
      "  Batch 6000/19776, Average Loss: 0.026792417013086378\n",
      "  Batch 7000/19776, Average Loss: 0.02691061591077596\n",
      "  Batch 8000/19776, Average Loss: 0.027437003614380957\n",
      "  Batch 9000/19776, Average Loss: 0.02667325657280162\n",
      "  Batch 10000/19776, Average Loss: 0.026991845520213248\n",
      "  Batch 11000/19776, Average Loss: 0.02708693378139287\n",
      "  Batch 12000/19776, Average Loss: 0.026856815606355666\n",
      "  Batch 13000/19776, Average Loss: 0.02706589236855507\n",
      "  Batch 14000/19776, Average Loss: 0.02707726365979761\n",
      "  Batch 15000/19776, Average Loss: 0.026522351036779582\n",
      "  Batch 16000/19776, Average Loss: 0.027013641906902194\n",
      "  Batch 17000/19776, Average Loss: 0.02716629406157881\n",
      "  Batch 18000/19776, Average Loss: 0.027094653039239348\n",
      "  Batch 19000/19776, Average Loss: 0.027508877897635103\n",
      "  Average Training Loss: 0.027012814079315446\n",
      "  Average Training Loss: 0.027012814079315446, Average Validation Loss: 0.02705364243658998\n",
      "  Average Test Loss: 0.1008991718544361\n",
      "Experiment C/B： 17/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.22082271038740872\n",
      "  Batch 2000/19776, Average Loss: 0.07989069265499711\n",
      "  Batch 3000/19776, Average Loss: 0.06260124233923853\n",
      "  Batch 4000/19776, Average Loss: 0.04324199784640223\n",
      "  Batch 5000/19776, Average Loss: 0.035008995013311506\n",
      "  Batch 6000/19776, Average Loss: 0.03124906148854643\n",
      "  Batch 7000/19776, Average Loss: 0.0294591432036832\n",
      "  Batch 8000/19776, Average Loss: 0.027606561277993023\n",
      "  Batch 9000/19776, Average Loss: 0.027074035052210094\n",
      "  Batch 10000/19776, Average Loss: 0.02660740963788703\n",
      "  Batch 11000/19776, Average Loss: 0.02595495409052819\n",
      "  Batch 12000/19776, Average Loss: 0.025877949531190096\n",
      "  Batch 13000/19776, Average Loss: 0.02592289612116292\n",
      "  Batch 14000/19776, Average Loss: 0.02619099471345544\n",
      "  Batch 15000/19776, Average Loss: 0.02605281195230782\n",
      "  Batch 16000/19776, Average Loss: 0.025755965147167444\n",
      "  Batch 17000/19776, Average Loss: 0.02585804047761485\n",
      "  Batch 18000/19776, Average Loss: 0.025310785265173764\n",
      "  Batch 19000/19776, Average Loss: 0.025594480857253073\n",
      "  Average Training Loss: 0.042253487338456075\n",
      "  Average Training Loss: 0.042253487338456075, Average Validation Loss: 0.02577932620448178\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.026035681115463376\n",
      "  Batch 2000/19776, Average Loss: 0.025292563199996947\n",
      "  Batch 3000/19776, Average Loss: 0.025926828398369253\n",
      "  Batch 4000/19776, Average Loss: 0.024967732957564295\n",
      "  Batch 5000/19776, Average Loss: 0.02559823072841391\n",
      "  Batch 6000/19776, Average Loss: 0.025591124412603677\n",
      "  Batch 7000/19776, Average Loss: 0.02525078297778964\n",
      "  Batch 8000/19776, Average Loss: 0.02522750049876049\n",
      "  Batch 9000/19776, Average Loss: 0.02534982458781451\n",
      "  Batch 10000/19776, Average Loss: 0.024938859067857266\n",
      "  Batch 11000/19776, Average Loss: 0.0251175630139187\n",
      "  Batch 12000/19776, Average Loss: 0.0254351219041273\n",
      "  Batch 13000/19776, Average Loss: 0.025595377110876144\n",
      "  Batch 14000/19776, Average Loss: 0.024967233843170105\n",
      "  Batch 15000/19776, Average Loss: 0.02559636383038014\n",
      "  Batch 16000/19776, Average Loss: 0.025297480380162597\n",
      "  Batch 17000/19776, Average Loss: 0.02540812807017937\n",
      "  Batch 18000/19776, Average Loss: 0.025069726350717247\n",
      "  Batch 19000/19776, Average Loss: 0.02510894538136199\n",
      "  Average Training Loss: 0.025332384181252613\n",
      "  Average Training Loss: 0.025332384181252613, Average Validation Loss: 0.025055456608643115\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02492164973448962\n",
      "  Batch 2000/19776, Average Loss: 0.024818749447353183\n",
      "  Batch 3000/19776, Average Loss: 0.02488810761924833\n",
      "  Batch 4000/19776, Average Loss: 0.02534240574669093\n",
      "  Batch 5000/19776, Average Loss: 0.025443100386299194\n",
      "  Batch 6000/19776, Average Loss: 0.024909410217776894\n",
      "  Batch 7000/19776, Average Loss: 0.024626514093019067\n",
      "  Batch 8000/19776, Average Loss: 0.02467241894453764\n",
      "  Batch 9000/19776, Average Loss: 0.024930661985650658\n",
      "  Batch 10000/19776, Average Loss: 0.024412048523314297\n",
      "  Batch 11000/19776, Average Loss: 0.02498872119234875\n",
      "  Batch 12000/19776, Average Loss: 0.024714863140136004\n",
      "  Batch 13000/19776, Average Loss: 0.024564756449311973\n",
      "  Batch 14000/19776, Average Loss: 0.0248037616815418\n",
      "  Batch 15000/19776, Average Loss: 0.02486708810739219\n",
      "  Batch 16000/19776, Average Loss: 0.02481385742733255\n",
      "  Batch 17000/19776, Average Loss: 0.024888033701106906\n",
      "  Batch 18000/19776, Average Loss: 0.024459933287929742\n",
      "  Batch 19000/19776, Average Loss: 0.0252016963083297\n",
      "  Average Training Loss: 0.024857118884741042\n",
      "  Average Training Loss: 0.024857118884741042, Average Validation Loss: 0.024741946910033275\n",
      "  Average Test Loss: 0.10639827019128528\n",
      "Experiment C/B： 18/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.198172972869128\n",
      "  Batch 2000/19776, Average Loss: 0.10407267724350094\n",
      "  Batch 3000/19776, Average Loss: 0.09807418892905116\n",
      "  Batch 4000/19776, Average Loss: 0.06942272207513452\n",
      "  Batch 5000/19776, Average Loss: 0.039003994456492365\n",
      "  Batch 6000/19776, Average Loss: 0.034025968950241806\n",
      "  Batch 7000/19776, Average Loss: 0.030342159892432392\n",
      "  Batch 8000/19776, Average Loss: 0.028133388655260206\n",
      "  Batch 9000/19776, Average Loss: 0.027760000295937063\n",
      "  Batch 10000/19776, Average Loss: 0.02715014872280881\n",
      "  Batch 11000/19776, Average Loss: 0.02684170890133828\n",
      "  Batch 12000/19776, Average Loss: 0.0265082995085977\n",
      "  Batch 13000/19776, Average Loss: 0.02660497181583196\n",
      "  Batch 14000/19776, Average Loss: 0.026208504100330175\n",
      "  Batch 15000/19776, Average Loss: 0.02600211723148823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 16000/19776, Average Loss: 0.025714906649664044\n",
      "  Batch 17000/19776, Average Loss: 0.025104237413965166\n",
      "  Batch 18000/19776, Average Loss: 0.025145760328508914\n",
      "  Batch 19000/19776, Average Loss: 0.02488640692457557\n",
      "  Average Training Loss: 0.045927608147529886\n",
      "  Average Training Loss: 0.045927608147529886, Average Validation Loss: 0.024773159966006132\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02471117947343737\n",
      "  Batch 2000/19776, Average Loss: 0.02405973931681365\n",
      "  Batch 3000/19776, Average Loss: 0.0246183035238646\n",
      "  Batch 4000/19776, Average Loss: 0.024494346055202187\n",
      "  Batch 5000/19776, Average Loss: 0.024603990704752506\n",
      "  Batch 6000/19776, Average Loss: 0.024643530944362283\n",
      "  Batch 7000/19776, Average Loss: 0.024198189753107727\n",
      "  Batch 8000/19776, Average Loss: 0.024004063569009304\n",
      "  Batch 9000/19776, Average Loss: 0.024145184237509967\n",
      "  Batch 10000/19776, Average Loss: 0.02365150347352028\n",
      "  Batch 11000/19776, Average Loss: 0.02349221788812429\n",
      "  Batch 12000/19776, Average Loss: 0.023779593637678773\n",
      "  Batch 13000/19776, Average Loss: 0.02431070031505078\n",
      "  Batch 14000/19776, Average Loss: 0.023676273166202008\n",
      "  Batch 15000/19776, Average Loss: 0.023730298398062587\n",
      "  Batch 16000/19776, Average Loss: 0.02347767221275717\n",
      "  Batch 17000/19776, Average Loss: 0.023548462179489433\n",
      "  Batch 18000/19776, Average Loss: 0.023040407992899418\n",
      "  Batch 19000/19776, Average Loss: 0.02326902169082314\n",
      "  Average Training Loss: 0.023965777958325836\n",
      "  Average Training Loss: 0.023965777958325836, Average Validation Loss: 0.023573292352158944\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.023212646883912384\n",
      "  Batch 2000/19776, Average Loss: 0.02366838444210589\n",
      "  Batch 3000/19776, Average Loss: 0.023030027038417757\n",
      "  Batch 4000/19776, Average Loss: 0.023480517310090362\n",
      "  Batch 5000/19776, Average Loss: 0.023204577033407985\n",
      "  Batch 6000/19776, Average Loss: 0.023195984030142426\n",
      "  Batch 7000/19776, Average Loss: 0.023559203988872467\n",
      "  Batch 8000/19776, Average Loss: 0.023238415161147715\n",
      "  Batch 9000/19776, Average Loss: 0.02304636253742501\n",
      "  Batch 10000/19776, Average Loss: 0.02315447554504499\n",
      "  Batch 11000/19776, Average Loss: 0.023430657234042883\n",
      "  Batch 12000/19776, Average Loss: 0.023228286400437354\n",
      "  Batch 13000/19776, Average Loss: 0.023436844384297727\n",
      "  Batch 14000/19776, Average Loss: 0.023141651394777\n",
      "  Batch 15000/19776, Average Loss: 0.02311813895823434\n",
      "  Batch 16000/19776, Average Loss: 0.022915841120760887\n",
      "  Batch 17000/19776, Average Loss: 0.023471166929230094\n",
      "  Batch 18000/19776, Average Loss: 0.023562410457991064\n",
      "  Batch 19000/19776, Average Loss: 0.023206839257851244\n",
      "  Average Training Loss: 0.02326118954224983\n",
      "  Average Training Loss: 0.02326118954224983, Average Validation Loss: 0.023189636613814985\n",
      "  Average Test Loss: 0.09921212825178331\n",
      "Experiment C/B： 19/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.13941613563150168\n",
      "  Batch 2000/19776, Average Loss: 0.09294965874776244\n",
      "  Batch 3000/19776, Average Loss: 0.05179368069116026\n",
      "  Batch 4000/19776, Average Loss: 0.034772410235367715\n",
      "  Batch 5000/19776, Average Loss: 0.02973836948443204\n",
      "  Batch 6000/19776, Average Loss: 0.027961645407602192\n",
      "  Batch 7000/19776, Average Loss: 0.026881996140815316\n",
      "  Batch 8000/19776, Average Loss: 0.026917871124111117\n",
      "  Batch 9000/19776, Average Loss: 0.026276844243053345\n",
      "  Batch 10000/19776, Average Loss: 0.026417522137053312\n",
      "  Batch 11000/19776, Average Loss: 0.02605748817231506\n",
      "  Batch 12000/19776, Average Loss: 0.02559804491326213\n",
      "  Batch 13000/19776, Average Loss: 0.02535878294473514\n",
      "  Batch 14000/19776, Average Loss: 0.025493214945308863\n",
      "  Batch 15000/19776, Average Loss: 0.025061693515628577\n",
      "  Batch 16000/19776, Average Loss: 0.02503547717630863\n",
      "  Batch 17000/19776, Average Loss: 0.024688310225494205\n",
      "  Batch 18000/19776, Average Loss: 0.024663480919785798\n",
      "  Batch 19000/19776, Average Loss: 0.024371528020128607\n",
      "  Average Training Loss: 0.03681604134192589\n",
      "  Average Training Loss: 0.03681604134192589, Average Validation Loss: 0.02405866995123426\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02405659205187112\n",
      "  Batch 2000/19776, Average Loss: 0.02395727563649416\n",
      "  Batch 3000/19776, Average Loss: 0.023486203604377805\n",
      "  Batch 4000/19776, Average Loss: 0.023943457543849946\n",
      "  Batch 5000/19776, Average Loss: 0.023995294532738624\n",
      "  Batch 6000/19776, Average Loss: 0.02381907746195793\n",
      "  Batch 7000/19776, Average Loss: 0.023705791717395185\n",
      "  Batch 8000/19776, Average Loss: 0.02359058771375567\n",
      "  Batch 9000/19776, Average Loss: 0.023227412171196192\n",
      "  Batch 10000/19776, Average Loss: 0.023308950264006853\n",
      "  Batch 11000/19776, Average Loss: 0.02353825134644285\n",
      "  Batch 12000/19776, Average Loss: 0.023777671748772263\n",
      "  Batch 13000/19776, Average Loss: 0.023403448375873267\n",
      "  Batch 14000/19776, Average Loss: 0.023542111935094\n",
      "  Batch 15000/19776, Average Loss: 0.02287047919910401\n",
      "  Batch 16000/19776, Average Loss: 0.023149361475370823\n",
      "  Batch 17000/19776, Average Loss: 0.02331643822649494\n",
      "  Batch 18000/19776, Average Loss: 0.02322052123956382\n",
      "  Batch 19000/19776, Average Loss: 0.023558093624189497\n",
      "  Average Training Loss: 0.023534244941790516\n",
      "  Average Training Loss: 0.023534244941790516, Average Validation Loss: 0.02324985495156648\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02289651822904125\n",
      "  Batch 2000/19776, Average Loss: 0.023404105925001206\n",
      "  Batch 3000/19776, Average Loss: 0.023650162954814733\n",
      "  Batch 4000/19776, Average Loss: 0.02323034369153902\n",
      "  Batch 5000/19776, Average Loss: 0.02285038403235376\n",
      "  Batch 6000/19776, Average Loss: 0.02329723459435627\n",
      "  Batch 7000/19776, Average Loss: 0.022895422196947037\n",
      "  Batch 8000/19776, Average Loss: 0.0234614587421529\n",
      "  Batch 9000/19776, Average Loss: 0.022858008042909207\n",
      "  Batch 10000/19776, Average Loss: 0.022642021879553796\n",
      "  Batch 11000/19776, Average Loss: 0.022857897992711516\n",
      "  Batch 12000/19776, Average Loss: 0.022780819358769803\n",
      "  Batch 13000/19776, Average Loss: 0.02294824772980064\n",
      "  Batch 14000/19776, Average Loss: 0.023145869106519966\n",
      "  Batch 15000/19776, Average Loss: 0.022935844875872136\n",
      "  Batch 16000/19776, Average Loss: 0.02278047889098525\n",
      "  Batch 17000/19776, Average Loss: 0.023072823032736778\n",
      "  Batch 18000/19776, Average Loss: 0.022667720240075143\n",
      "  Batch 19000/19776, Average Loss: 0.02305649985698983\n",
      "  Average Training Loss: 0.02301661289932054\n",
      "  Average Training Loss: 0.02301661289932054, Average Validation Loss: 0.022773773826298555\n",
      "  Average Test Loss: 0.09909194441126472\n",
      "Experiment C/B： 20/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.21819276605173946\n",
      "  Batch 2000/19776, Average Loss: 0.10163052057847381\n",
      "  Batch 3000/19776, Average Loss: 0.0939537234120071\n",
      "  Batch 4000/19776, Average Loss: 0.06960808958858251\n",
      "  Batch 5000/19776, Average Loss: 0.039987469665706155\n",
      "  Batch 6000/19776, Average Loss: 0.03538921692594886\n",
      "  Batch 7000/19776, Average Loss: 0.03416867446061224\n",
      "  Batch 8000/19776, Average Loss: 0.0320986507050693\n",
      "  Batch 9000/19776, Average Loss: 0.031022944896947593\n",
      "  Batch 10000/19776, Average Loss: 0.02902607009373605\n",
      "  Batch 11000/19776, Average Loss: 0.029136283290572464\n",
      "  Batch 12000/19776, Average Loss: 0.02800025387201458\n",
      "  Batch 13000/19776, Average Loss: 0.02736942063458264\n",
      "  Batch 14000/19776, Average Loss: 0.026319273004308343\n",
      "  Batch 15000/19776, Average Loss: 0.02532695806818083\n",
      "  Batch 16000/19776, Average Loss: 0.025113244927488267\n",
      "  Batch 17000/19776, Average Loss: 0.024250730492174626\n",
      "  Batch 18000/19776, Average Loss: 0.02381438719900325\n",
      "  Batch 19000/19776, Average Loss: 0.02352324068779126\n",
      "  Average Training Loss: 0.04732448608836974\n",
      "  Average Training Loss: 0.04732448608836974, Average Validation Loss: 0.023156671639581582\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02333849603869021\n",
      "  Batch 2000/19776, Average Loss: 0.022801191684789956\n",
      "  Batch 3000/19776, Average Loss: 0.022849509647581725\n",
      "  Batch 4000/19776, Average Loss: 0.02290583169972524\n",
      "  Batch 5000/19776, Average Loss: 0.023025137070566415\n",
      "  Batch 6000/19776, Average Loss: 0.022435963978990912\n",
      "  Batch 7000/19776, Average Loss: 0.022672111849300562\n",
      "  Batch 8000/19776, Average Loss: 0.02206384631153196\n",
      "  Batch 9000/19776, Average Loss: 0.0223989712218754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10000/19776, Average Loss: 0.021810538973659276\n",
      "  Batch 11000/19776, Average Loss: 0.02211862789420411\n",
      "  Batch 12000/19776, Average Loss: 0.021867469266522676\n",
      "  Batch 13000/19776, Average Loss: 0.021938718611840158\n",
      "  Batch 14000/19776, Average Loss: 0.021996513531077653\n",
      "  Batch 15000/19776, Average Loss: 0.021130728396587074\n",
      "  Batch 16000/19776, Average Loss: 0.02153411924187094\n",
      "  Batch 17000/19776, Average Loss: 0.02157619410706684\n",
      "  Batch 18000/19776, Average Loss: 0.021888708461541683\n",
      "  Batch 19000/19776, Average Loss: 0.021523543649818747\n",
      "  Average Training Loss: 0.02216532168608791\n",
      "  Average Training Loss: 0.02216532168608791, Average Validation Loss: 0.021348807954438134\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.020945457441732287\n",
      "  Batch 2000/19776, Average Loss: 0.021107940637972205\n",
      "  Batch 3000/19776, Average Loss: 0.021363934288267047\n",
      "  Batch 4000/19776, Average Loss: 0.02105799497058615\n",
      "  Batch 5000/19776, Average Loss: 0.020874569467734545\n",
      "  Batch 6000/19776, Average Loss: 0.021286552539095283\n",
      "  Batch 7000/19776, Average Loss: 0.021525440277531744\n",
      "  Batch 8000/19776, Average Loss: 0.021203673182055353\n",
      "  Batch 9000/19776, Average Loss: 0.02108105847099796\n",
      "  Batch 10000/19776, Average Loss: 0.020572157097049057\n",
      "  Batch 11000/19776, Average Loss: 0.020908184008672833\n",
      "  Batch 12000/19776, Average Loss: 0.02095761435292661\n",
      "  Batch 13000/19776, Average Loss: 0.020820602707099168\n",
      "  Batch 14000/19776, Average Loss: 0.020870564511977135\n",
      "  Batch 15000/19776, Average Loss: 0.020857766962144524\n",
      "  Batch 16000/19776, Average Loss: 0.02073128756927326\n",
      "  Batch 17000/19776, Average Loss: 0.02075561382807791\n",
      "  Batch 18000/19776, Average Loss: 0.020554338978137823\n",
      "  Batch 19000/19776, Average Loss: 0.020640607942361385\n",
      "  Average Training Loss: 0.02094030239217889\n",
      "  Average Training Loss: 0.02094030239217889, Average Validation Loss: 0.02048759118807924\n",
      "  Average Test Loss: 0.09850819385281717\n",
      "Experiment C/B： 21/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19776, Average Loss: 0.11634507044404745\n",
      "  Batch 2000/19776, Average Loss: 0.07690018808655441\n",
      "  Batch 3000/19776, Average Loss: 0.038275801161304114\n",
      "  Batch 4000/19776, Average Loss: 0.03215475280024111\n",
      "  Batch 5000/19776, Average Loss: 0.02994716555904597\n",
      "  Batch 6000/19776, Average Loss: 0.028288877295330167\n",
      "  Batch 7000/19776, Average Loss: 0.027487594702281057\n",
      "  Batch 8000/19776, Average Loss: 0.0270120396213606\n",
      "  Batch 9000/19776, Average Loss: 0.026358575065620244\n",
      "  Batch 10000/19776, Average Loss: 0.026486175158061086\n",
      "  Batch 11000/19776, Average Loss: 0.02593552469648421\n",
      "  Batch 12000/19776, Average Loss: 0.02564836882986128\n",
      "  Batch 13000/19776, Average Loss: 0.02555399436596781\n",
      "  Batch 14000/19776, Average Loss: 0.025498966671526433\n",
      "  Batch 15000/19776, Average Loss: 0.02546675795223564\n",
      "  Batch 16000/19776, Average Loss: 0.025201067324727773\n",
      "  Batch 17000/19776, Average Loss: 0.02531527743069455\n",
      "  Batch 18000/19776, Average Loss: 0.024548788189888\n",
      "  Batch 19000/19776, Average Loss: 0.024600671963766217\n",
      "  Average Training Loss: 0.03418089718830378\n",
      "  Average Training Loss: 0.03418089718830378, Average Validation Loss: 0.024542601648936634\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19776, Average Loss: 0.02442513485439122\n",
      "  Batch 2000/19776, Average Loss: 0.024266543069854377\n",
      "  Batch 3000/19776, Average Loss: 0.023876027377322317\n",
      "  Batch 4000/19776, Average Loss: 0.024264387965202333\n",
      "  Batch 5000/19776, Average Loss: 0.023851946253329516\n",
      "  Batch 6000/19776, Average Loss: 0.02349710672395304\n",
      "  Batch 7000/19776, Average Loss: 0.023633036869578062\n",
      "  Batch 8000/19776, Average Loss: 0.02396131004858762\n",
      "  Batch 9000/19776, Average Loss: 0.02342296561365947\n",
      "  Batch 10000/19776, Average Loss: 0.02356527524907142\n",
      "  Batch 11000/19776, Average Loss: 0.0229971109572798\n",
      "  Batch 12000/19776, Average Loss: 0.023187213624827563\n",
      "  Batch 13000/19776, Average Loss: 0.022893053276464344\n",
      "  Batch 14000/19776, Average Loss: 0.02296155527792871\n",
      "  Batch 15000/19776, Average Loss: 0.022539926933124663\n",
      "  Batch 16000/19776, Average Loss: 0.02251254356279969\n",
      "  Batch 17000/19776, Average Loss: 0.02300954820169136\n",
      "  Batch 18000/19776, Average Loss: 0.022943312958814202\n",
      "  Batch 19000/19776, Average Loss: 0.02254944556392729\n",
      "  Average Training Loss: 0.023362197634105835\n",
      "  Average Training Loss: 0.023362197634105835, Average Validation Loss: 0.02284230137225783\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19776, Average Loss: 0.022608024404384197\n",
      "  Batch 2000/19776, Average Loss: 0.022489867311436682\n",
      "  Batch 3000/19776, Average Loss: 0.02320619931211695\n",
      "  Batch 4000/19776, Average Loss: 0.02253050846606493\n",
      "  Batch 5000/19776, Average Loss: 0.021880542549770326\n",
      "  Batch 6000/19776, Average Loss: 0.022405735514126716\n",
      "  Batch 7000/19776, Average Loss: 0.02217338040145114\n",
      "  Batch 8000/19776, Average Loss: 0.02208871399704367\n",
      "  Batch 9000/19776, Average Loss: 0.022304904920980334\n",
      "  Batch 10000/19776, Average Loss: 0.02224709520675242\n",
      "  Batch 11000/19776, Average Loss: 0.02224444310972467\n",
      "  Batch 12000/19776, Average Loss: 0.02172943892888725\n",
      "  Batch 13000/19776, Average Loss: 0.02208585783606395\n",
      "  Batch 14000/19776, Average Loss: 0.021698942348361016\n",
      "  Batch 15000/19776, Average Loss: 0.02213465205859393\n",
      "  Batch 16000/19776, Average Loss: 0.022037227259483188\n",
      "  Batch 17000/19776, Average Loss: 0.02196429401729256\n",
      "  Batch 18000/19776, Average Loss: 0.02208305017789826\n",
      "  Batch 19000/19776, Average Loss: 0.021825375472195447\n",
      "  Average Training Loss: 0.022162453743161423\n",
      "  Average Training Loss: 0.022162453743161423, Average Validation Loss: 0.02203683649567649\n",
      "  Average Test Loss: 0.10178330658976949\n",
      "Experiment B/C： 22/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.19142317289114\n",
      "  Batch 2000/6005, Average Loss: 0.07340733294934035\n",
      "  Batch 3000/6005, Average Loss: 0.06801064850762487\n",
      "  Batch 4000/6005, Average Loss: 0.06023972177505493\n",
      "  Batch 5000/6005, Average Loss: 0.05214678015746176\n",
      "  Batch 6000/6005, Average Loss: 0.04460667363367975\n",
      "  Average Training Loss: 0.08161189151830121\n",
      "  Average Training Loss: 0.08161189151830121, Average Validation Loss: 0.04236956193581839\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.04000225585512817\n",
      "  Batch 2000/6005, Average Loss: 0.03606954750046134\n",
      "  Batch 3000/6005, Average Loss: 0.034737590716220436\n",
      "  Batch 4000/6005, Average Loss: 0.034240878898650406\n",
      "  Batch 5000/6005, Average Loss: 0.0341384264016524\n",
      "  Batch 6000/6005, Average Loss: 0.03427947377692908\n",
      "  Average Training Loss: 0.03557096766594869\n",
      "  Average Training Loss: 0.03557096766594869, Average Validation Loss: 0.03399238245097918\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03358633814752102\n",
      "  Batch 2000/6005, Average Loss: 0.03388116436358541\n",
      "  Batch 3000/6005, Average Loss: 0.03389682300295681\n",
      "  Batch 4000/6005, Average Loss: 0.03377619815524668\n",
      "  Batch 5000/6005, Average Loss: 0.033439910273067655\n",
      "  Batch 6000/6005, Average Loss: 0.03418917204625905\n",
      "  Average Training Loss: 0.033797165637310105\n",
      "  Average Training Loss: 0.033797165637310105, Average Validation Loss: 0.03359353858927952\n",
      "  Average Test Loss: 0.10421460702151443\n",
      "Experiment B/C： 23/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.1311609213873744\n",
      "  Batch 2000/6005, Average Loss: 0.08874640877917409\n",
      "  Batch 3000/6005, Average Loss: 0.07408597071282566\n",
      "  Batch 4000/6005, Average Loss: 0.055011138090863825\n",
      "  Batch 5000/6005, Average Loss: 0.04207916658557952\n",
      "  Batch 6000/6005, Average Loss: 0.03535006496682763\n",
      "  Average Training Loss: 0.07104091686999223\n",
      "  Average Training Loss: 0.07104091686999223, Average Validation Loss: 0.033046964834383456\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.031364788090810176\n",
      "  Batch 2000/6005, Average Loss: 0.029327656312845646\n",
      "  Batch 3000/6005, Average Loss: 0.02787453228607774\n",
      "  Batch 4000/6005, Average Loss: 0.027367254180833696\n",
      "  Batch 5000/6005, Average Loss: 0.026605570706538857\n",
      "  Batch 6000/6005, Average Loss: 0.026715846002101896\n",
      "  Average Training Loss: 0.028207661418072093\n",
      "  Average Training Loss: 0.028207661418072093, Average Validation Loss: 0.0270451975089762\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.026952871717512608\n",
      "  Batch 2000/6005, Average Loss: 0.026367667806334795\n",
      "  Batch 3000/6005, Average Loss: 0.026254275273531677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4000/6005, Average Loss: 0.026347464578226207\n",
      "  Batch 5000/6005, Average Loss: 0.02575516079645604\n",
      "  Batch 6000/6005, Average Loss: 0.02609587658569217\n",
      "  Average Training Loss: 0.026298940947262275\n",
      "  Average Training Loss: 0.026298940947262275, Average Validation Loss: 0.026312629468416605\n",
      "  Average Test Loss: 0.10507671134497422\n",
      "Experiment B/C： 24/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.08507964170351624\n",
      "  Batch 2000/6005, Average Loss: 0.06365600792877377\n",
      "  Batch 3000/6005, Average Loss: 0.04984144065901637\n",
      "  Batch 4000/6005, Average Loss: 0.0421574197281152\n",
      "  Batch 5000/6005, Average Loss: 0.037122717750258744\n",
      "  Batch 6000/6005, Average Loss: 0.03399731804989278\n",
      "  Average Training Loss: 0.051951648281277664\n",
      "  Average Training Loss: 0.051951648281277664, Average Validation Loss: 0.03326610642573368\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03128335542604327\n",
      "  Batch 2000/6005, Average Loss: 0.029744828977622092\n",
      "  Batch 3000/6005, Average Loss: 0.028194877168163657\n",
      "  Batch 4000/6005, Average Loss: 0.02751953222602606\n",
      "  Batch 5000/6005, Average Loss: 0.026883056864142416\n",
      "  Batch 6000/6005, Average Loss: 0.026693835054524242\n",
      "  Average Training Loss: 0.028380322315692156\n",
      "  Average Training Loss: 0.028380322315692156, Average Validation Loss: 0.026455960452268903\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.02611652315594256\n",
      "  Batch 2000/6005, Average Loss: 0.025511652812827378\n",
      "  Batch 3000/6005, Average Loss: 0.02555825257115066\n",
      "  Batch 4000/6005, Average Loss: 0.025224891301244497\n",
      "  Batch 5000/6005, Average Loss: 0.025455347161740065\n",
      "  Batch 6000/6005, Average Loss: 0.024906889140605928\n",
      "  Average Training Loss: 0.02546796431488898\n",
      "  Average Training Loss: 0.02546796431488898, Average Validation Loss: 0.02544150119628782\n",
      "  Average Test Loss: 0.10257598152794818\n",
      "Experiment B/C： 25/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.10380779563635588\n",
      "  Batch 2000/6005, Average Loss: 0.07848167166113854\n",
      "  Batch 3000/6005, Average Loss: 0.059362704623490574\n",
      "  Batch 4000/6005, Average Loss: 0.0430003756582737\n",
      "  Batch 5000/6005, Average Loss: 0.03718312422931194\n",
      "  Batch 6000/6005, Average Loss: 0.036250823160633446\n",
      "  Average Training Loss: 0.05965296908667145\n",
      "  Average Training Loss: 0.05965296908667145, Average Validation Loss: 0.035639570110766454\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03590310088451952\n",
      "  Batch 2000/6005, Average Loss: 0.03484369245450944\n",
      "  Batch 3000/6005, Average Loss: 0.03447731071989983\n",
      "  Batch 4000/6005, Average Loss: 0.034155405113473536\n",
      "  Batch 5000/6005, Average Loss: 0.033641296717338266\n",
      "  Batch 6000/6005, Average Loss: 0.03372882763389498\n",
      "  Average Training Loss: 0.03445796515128818\n",
      "  Average Training Loss: 0.03445796515128818, Average Validation Loss: 0.033884004383445895\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03332926898635924\n",
      "  Batch 2000/6005, Average Loss: 0.033312370567582544\n",
      "  Batch 3000/6005, Average Loss: 0.032717462928965685\n",
      "  Batch 4000/6005, Average Loss: 0.03225287026073784\n",
      "  Batch 5000/6005, Average Loss: 0.03203612318029627\n",
      "  Batch 6000/6005, Average Loss: 0.0318265326526016\n",
      "  Average Training Loss: 0.032584378081641406\n",
      "  Average Training Loss: 0.032584378081641406, Average Validation Loss: 0.03195862731922383\n",
      "  Average Test Loss: 0.10272900219398255\n",
      "Experiment B/C： 26/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.10199832936376334\n",
      "  Batch 2000/6005, Average Loss: 0.08940634578466415\n",
      "  Batch 3000/6005, Average Loss: 0.07507027196139097\n",
      "  Batch 4000/6005, Average Loss: 0.0619660755302757\n",
      "  Batch 5000/6005, Average Loss: 0.049112152986228466\n",
      "  Batch 6000/6005, Average Loss: 0.04182467938587069\n",
      "  Average Training Loss: 0.06986928808779691\n",
      "  Average Training Loss: 0.06986928808779691, Average Validation Loss: 0.03903864833095995\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03694212588388473\n",
      "  Batch 2000/6005, Average Loss: 0.034372986001893875\n",
      "  Batch 3000/6005, Average Loss: 0.031759958625771106\n",
      "  Batch 4000/6005, Average Loss: 0.030286301187239587\n",
      "  Batch 5000/6005, Average Loss: 0.030252657019533218\n",
      "  Batch 6000/6005, Average Loss: 0.028823844694532454\n",
      "  Average Training Loss: 0.032067187007463895\n",
      "  Average Training Loss: 0.032067187007463895, Average Validation Loss: 0.028713880330956627\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.02823955880664289\n",
      "  Batch 2000/6005, Average Loss: 0.02825658082496375\n",
      "  Batch 3000/6005, Average Loss: 0.02753304340224713\n",
      "  Batch 4000/6005, Average Loss: 0.028183880735188724\n",
      "  Batch 5000/6005, Average Loss: 0.027072655381634832\n",
      "  Batch 6000/6005, Average Loss: 0.026970118407160043\n",
      "  Average Training Loss: 0.02770771121793891\n",
      "  Average Training Loss: 0.02770771121793891, Average Validation Loss: 0.027283028279680425\n",
      "  Average Test Loss: 0.10751692344450382\n",
      "Experiment B/C： 27/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.12289460632950068\n",
      "  Batch 2000/6005, Average Loss: 0.09521183696389199\n",
      "  Batch 3000/6005, Average Loss: 0.07940480857342481\n",
      "  Batch 4000/6005, Average Loss: 0.059619626585394145\n",
      "  Batch 5000/6005, Average Loss: 0.04416982075199485\n",
      "  Batch 6000/6005, Average Loss: 0.03760547837987542\n",
      "  Average Training Loss: 0.07312135439145873\n",
      "  Average Training Loss: 0.07312135439145873, Average Validation Loss: 0.03444433706278073\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.032995381835848096\n",
      "  Batch 2000/6005, Average Loss: 0.03158029056806117\n",
      "  Batch 3000/6005, Average Loss: 0.03047989131975919\n",
      "  Batch 4000/6005, Average Loss: 0.02945281282160431\n",
      "  Batch 5000/6005, Average Loss: 0.028549667944200337\n",
      "  Batch 6000/6005, Average Loss: 0.028342046037316323\n",
      "  Average Training Loss: 0.03022973108773029\n",
      "  Average Training Loss: 0.03022973108773029, Average Validation Loss: 0.02836071133561502\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.028438434009440242\n",
      "  Batch 2000/6005, Average Loss: 0.027261280614882706\n",
      "  Batch 3000/6005, Average Loss: 0.02780983763653785\n",
      "  Batch 4000/6005, Average Loss: 0.027821241779252888\n",
      "  Batch 5000/6005, Average Loss: 0.027446365694515407\n",
      "  Batch 6000/6005, Average Loss: 0.027458125764504075\n",
      "  Average Training Loss: 0.02770506240269301\n",
      "  Average Training Loss: 0.02770506240269301, Average Validation Loss: 0.027340754859779434\n",
      "  Average Test Loss: 0.1025989491913338\n",
      "Experiment B/C： 28/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/6005, Average Loss: 0.10724561894312501\n",
      "  Batch 2000/6005, Average Loss: 0.08144579836353659\n",
      "  Batch 3000/6005, Average Loss: 0.06249391627497971\n",
      "  Batch 4000/6005, Average Loss: 0.04579658511653543\n",
      "  Batch 5000/6005, Average Loss: 0.03822952225059271\n",
      "  Batch 6000/6005, Average Loss: 0.03650467150751501\n",
      "  Average Training Loss: 0.061920524602112274\n",
      "  Average Training Loss: 0.061920524602112274, Average Validation Loss: 0.035600645002597735\n",
      "Epoch 2/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03504787391331047\n",
      "  Batch 2000/6005, Average Loss: 0.03430557722877711\n",
      "  Batch 3000/6005, Average Loss: 0.03468442632537335\n",
      "  Batch 4000/6005, Average Loss: 0.03434367877803743\n",
      "  Batch 5000/6005, Average Loss: 0.03377456127852201\n",
      "  Batch 6000/6005, Average Loss: 0.03222937477752566\n",
      "  Average Training Loss: 0.034060596995874555\n",
      "  Average Training Loss: 0.034060596995874555, Average Validation Loss: 0.031141640395843752\n",
      "Epoch 3/3: \n",
      "  Batch 1000/6005, Average Loss: 0.03004623647686094\n",
      "  Batch 2000/6005, Average Loss: 0.02766575066372752\n",
      "  Batch 3000/6005, Average Loss: 0.026417296762578188\n",
      "  Batch 4000/6005, Average Loss: 0.025004932397976517\n",
      "  Batch 5000/6005, Average Loss: 0.024266679475083947\n",
      "  Batch 6000/6005, Average Loss: 0.02343329911865294\n",
      "  Average Training Loss: 0.02613888710836487\n",
      "  Average Training Loss: 0.02613888710836487, Average Validation Loss: 0.02356375238334837\n",
      "  Average Test Loss: 0.10929894171397858\n",
      "Experiment C+B/C： 29/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.10394339552894234\n",
      "  Batch 2000/19848, Average Loss: 0.07472441488131881\n",
      "  Batch 3000/19848, Average Loss: 0.05677338738925755\n",
      "  Batch 4000/19848, Average Loss: 0.05076358106546104\n",
      "  Batch 5000/19848, Average Loss: 0.04753071985393763\n",
      "  Batch 6000/19848, Average Loss: 0.04696542341075838\n",
      "  Batch 7000/19848, Average Loss: 0.04673113553319126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 8000/19848, Average Loss: 0.04547688538953662\n",
      "  Batch 9000/19848, Average Loss: 0.04636056281812489\n",
      "  Batch 10000/19848, Average Loss: 0.046200835390947756\n",
      "  Batch 11000/19848, Average Loss: 0.04580537623353303\n",
      "  Batch 12000/19848, Average Loss: 0.04609101481363177\n",
      "  Batch 13000/19848, Average Loss: 0.04624010382220149\n",
      "  Batch 14000/19848, Average Loss: 0.04573984134756029\n",
      "  Batch 15000/19848, Average Loss: 0.04628302634228021\n",
      "  Batch 16000/19848, Average Loss: 0.04550162703357637\n",
      "  Batch 17000/19848, Average Loss: 0.04576921515911818\n",
      "  Batch 18000/19848, Average Loss: 0.0461296771094203\n",
      "  Batch 19000/19848, Average Loss: 0.04564845394715667\n",
      "  Average Training Loss: 0.05124783945418722\n",
      "  Average Training Loss: 0.05124783945418722, Average Validation Loss: 0.046007805685589674\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.04614448735397309\n",
      "  Batch 2000/19848, Average Loss: 0.045521844148635866\n",
      "  Batch 3000/19848, Average Loss: 0.045260339085012674\n",
      "  Batch 4000/19848, Average Loss: 0.04662168393470347\n",
      "  Batch 5000/19848, Average Loss: 0.04538957917690277\n",
      "  Batch 6000/19848, Average Loss: 0.046075911139138045\n",
      "  Batch 7000/19848, Average Loss: 0.04595986360684037\n",
      "  Batch 8000/19848, Average Loss: 0.04500899911671877\n",
      "  Batch 9000/19848, Average Loss: 0.04561149669997394\n",
      "  Batch 10000/19848, Average Loss: 0.04587267024070024\n",
      "  Batch 11000/19848, Average Loss: 0.04628618920221925\n",
      "  Batch 12000/19848, Average Loss: 0.046186179926618935\n",
      "  Batch 13000/19848, Average Loss: 0.046459925617091356\n",
      "  Batch 14000/19848, Average Loss: 0.04620945894625038\n",
      "  Batch 15000/19848, Average Loss: 0.04548131910711527\n",
      "  Batch 16000/19848, Average Loss: 0.04606595597602427\n",
      "  Batch 17000/19848, Average Loss: 0.04560141418967396\n",
      "  Batch 18000/19848, Average Loss: 0.04579994052555412\n",
      "  Batch 19000/19848, Average Loss: 0.04603065897524357\n",
      "  Average Training Loss: 0.04586373091012229\n",
      "  Average Training Loss: 0.04586373091012229, Average Validation Loss: 0.04572542486258124\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.04543484350852668\n",
      "  Batch 2000/19848, Average Loss: 0.04588710639812052\n",
      "  Batch 3000/19848, Average Loss: 0.04561381365731359\n",
      "  Batch 4000/19848, Average Loss: 0.045670420192182065\n",
      "  Batch 5000/19848, Average Loss: 0.04524521222617477\n",
      "  Batch 6000/19848, Average Loss: 0.04631274172663689\n",
      "  Batch 7000/19848, Average Loss: 0.04624591955915094\n",
      "  Batch 8000/19848, Average Loss: 0.046410936111584304\n",
      "  Batch 9000/19848, Average Loss: 0.04543618755973876\n",
      "  Batch 10000/19848, Average Loss: 0.04507496506161988\n",
      "  Batch 11000/19848, Average Loss: 0.04580306551232934\n",
      "  Batch 12000/19848, Average Loss: 0.045614452046342194\n",
      "  Batch 13000/19848, Average Loss: 0.04526079985871911\n",
      "  Batch 14000/19848, Average Loss: 0.045704035066068176\n",
      "  Batch 15000/19848, Average Loss: 0.046371621074154976\n",
      "  Batch 16000/19848, Average Loss: 0.04598661118559539\n",
      "  Batch 17000/19848, Average Loss: 0.0459318168843165\n",
      "  Batch 18000/19848, Average Loss: 0.04590042240731418\n",
      "  Batch 19000/19848, Average Loss: 0.045842933511361476\n",
      "  Average Training Loss: 0.04579887277896583\n",
      "  Average Training Loss: 0.04579887277896583, Average Validation Loss: 0.045796309106790674\n",
      "  Average Test Loss: 0.03751637967566617\n",
      "Experiment C+B/C： 30/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.11015183193981648\n",
      "  Batch 2000/19848, Average Loss: 0.07091538015566766\n",
      "  Batch 3000/19848, Average Loss: 0.05534792160987854\n",
      "  Batch 4000/19848, Average Loss: 0.05001054879836738\n",
      "  Batch 5000/19848, Average Loss: 0.04753698823880404\n",
      "  Batch 6000/19848, Average Loss: 0.045896761829033496\n",
      "  Batch 7000/19848, Average Loss: 0.04560120248422027\n",
      "  Batch 8000/19848, Average Loss: 0.043851034296676514\n",
      "  Batch 9000/19848, Average Loss: 0.04383636170439422\n",
      "  Batch 10000/19848, Average Loss: 0.04382506124489009\n",
      "  Batch 11000/19848, Average Loss: 0.04399711932055652\n",
      "  Batch 12000/19848, Average Loss: 0.04351632739603519\n",
      "  Batch 13000/19848, Average Loss: 0.04382269522827119\n",
      "  Batch 14000/19848, Average Loss: 0.04308416789583862\n",
      "  Batch 15000/19848, Average Loss: 0.04279871012829244\n",
      "  Batch 16000/19848, Average Loss: 0.04279705285467207\n",
      "  Batch 17000/19848, Average Loss: 0.04261519145779312\n",
      "  Batch 18000/19848, Average Loss: 0.04211166201531887\n",
      "  Batch 19000/19848, Average Loss: 0.04281226169411093\n",
      "  Average Training Loss: 0.04938441255021576\n",
      "  Average Training Loss: 0.04938441255021576, Average Validation Loss: 0.04262276575515848\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.042673331558704376\n",
      "  Batch 2000/19848, Average Loss: 0.042096217409707606\n",
      "  Batch 3000/19848, Average Loss: 0.041786418491974474\n",
      "  Batch 4000/19848, Average Loss: 0.041975115209817886\n",
      "  Batch 5000/19848, Average Loss: 0.04166811267100275\n",
      "  Batch 6000/19848, Average Loss: 0.042417640396393834\n",
      "  Batch 7000/19848, Average Loss: 0.041436057163402436\n",
      "  Batch 8000/19848, Average Loss: 0.041382430671714245\n",
      "  Batch 9000/19848, Average Loss: 0.04218259056098759\n",
      "  Batch 10000/19848, Average Loss: 0.04154435400571674\n",
      "  Batch 11000/19848, Average Loss: 0.04216376637481153\n",
      "  Batch 12000/19848, Average Loss: 0.041439214894548056\n",
      "  Batch 13000/19848, Average Loss: 0.04214554655738175\n",
      "  Batch 14000/19848, Average Loss: 0.04121997428499162\n",
      "  Batch 15000/19848, Average Loss: 0.04124871561303735\n",
      "  Batch 16000/19848, Average Loss: 0.04164097628556192\n",
      "  Batch 17000/19848, Average Loss: 0.04091298795491457\n",
      "  Batch 18000/19848, Average Loss: 0.04169414383545518\n",
      "  Batch 19000/19848, Average Loss: 0.04098188777267933\n",
      "  Average Training Loss: 0.04167606482670426\n",
      "  Average Training Loss: 0.04167606482670426, Average Validation Loss: 0.04158710441126044\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.04195392702333629\n",
      "  Batch 2000/19848, Average Loss: 0.04107269525900483\n",
      "  Batch 3000/19848, Average Loss: 0.04114246437139809\n",
      "  Batch 4000/19848, Average Loss: 0.041065662118606266\n",
      "  Batch 5000/19848, Average Loss: 0.041604108083993195\n",
      "  Batch 6000/19848, Average Loss: 0.04077895282767713\n",
      "  Batch 7000/19848, Average Loss: 0.04104079707525671\n",
      "  Batch 8000/19848, Average Loss: 0.04063461111672222\n",
      "  Batch 9000/19848, Average Loss: 0.04132924414053559\n",
      "  Batch 10000/19848, Average Loss: 0.0421917151901871\n",
      "  Batch 11000/19848, Average Loss: 0.04119344175048172\n",
      "  Batch 12000/19848, Average Loss: 0.04174735331349075\n",
      "  Batch 13000/19848, Average Loss: 0.04103278974909336\n",
      "  Batch 14000/19848, Average Loss: 0.04130121130030602\n",
      "  Batch 15000/19848, Average Loss: 0.041206594018265603\n",
      "  Batch 16000/19848, Average Loss: 0.040722849844023586\n",
      "  Batch 17000/19848, Average Loss: 0.04150193294137716\n",
      "  Batch 18000/19848, Average Loss: 0.041012516349554065\n",
      "  Batch 19000/19848, Average Loss: 0.041451870262622835\n",
      "  Average Training Loss: 0.041202168327178885\n",
      "  Average Training Loss: 0.041202168327178885, Average Validation Loss: 0.04118192952399543\n",
      "  Average Test Loss: 0.033896614544363865\n",
      "Experiment C+B/C： 31/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.23410421134904028\n",
      "  Batch 2000/19848, Average Loss: 0.1069161202609539\n",
      "  Batch 3000/19848, Average Loss: 0.09072744931653141\n",
      "  Batch 4000/19848, Average Loss: 0.06845010609738529\n",
      "  Batch 5000/19848, Average Loss: 0.05470999329537153\n",
      "  Batch 6000/19848, Average Loss: 0.04921165722422302\n",
      "  Batch 7000/19848, Average Loss: 0.047278706428594885\n",
      "  Batch 8000/19848, Average Loss: 0.046295379526913163\n",
      "  Batch 9000/19848, Average Loss: 0.04357658200524747\n",
      "  Batch 10000/19848, Average Loss: 0.04274885788653046\n",
      "  Batch 11000/19848, Average Loss: 0.042688188237138096\n",
      "  Batch 12000/19848, Average Loss: 0.042177899818867444\n",
      "  Batch 13000/19848, Average Loss: 0.041332713660784066\n",
      "  Batch 14000/19848, Average Loss: 0.04087189718708396\n",
      "  Batch 15000/19848, Average Loss: 0.0404225390618667\n",
      "  Batch 16000/19848, Average Loss: 0.04042715772241354\n",
      "  Batch 17000/19848, Average Loss: 0.04074679094646126\n",
      "  Batch 18000/19848, Average Loss: 0.04054296456463635\n",
      "  Batch 19000/19848, Average Loss: 0.04027606363780797\n",
      "  Average Training Loss: 0.05978896533147871\n",
      "  Average Training Loss: 0.05978896533147871, Average Validation Loss: 0.04012158533508699\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.040123209223151206\n",
      "  Batch 2000/19848, Average Loss: 0.039445489974692466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3000/19848, Average Loss: 0.039494654634967444\n",
      "  Batch 4000/19848, Average Loss: 0.03984785677306354\n",
      "  Batch 5000/19848, Average Loss: 0.03924347974546254\n",
      "  Batch 6000/19848, Average Loss: 0.039773580422624945\n",
      "  Batch 7000/19848, Average Loss: 0.04000585480686277\n",
      "  Batch 8000/19848, Average Loss: 0.040116828645579514\n",
      "  Batch 9000/19848, Average Loss: 0.0390040724799037\n",
      "  Batch 10000/19848, Average Loss: 0.03908166152890771\n",
      "  Batch 11000/19848, Average Loss: 0.03905775023531169\n",
      "  Batch 12000/19848, Average Loss: 0.0386708533288911\n",
      "  Batch 13000/19848, Average Loss: 0.039168010214343664\n",
      "  Batch 14000/19848, Average Loss: 0.037917338324710725\n",
      "  Batch 15000/19848, Average Loss: 0.03912866451684385\n",
      "  Batch 16000/19848, Average Loss: 0.03950507745333016\n",
      "  Batch 17000/19848, Average Loss: 0.03871591809764505\n",
      "  Batch 18000/19848, Average Loss: 0.03929452111013234\n",
      "  Batch 19000/19848, Average Loss: 0.03932721765246242\n",
      "  Average Training Loss: 0.03929870390842441\n",
      "  Average Training Loss: 0.03929870390842441, Average Validation Loss: 0.03875993996170146\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.038667680989950895\n",
      "  Batch 2000/19848, Average Loss: 0.03900005275756121\n",
      "  Batch 3000/19848, Average Loss: 0.03848173811286688\n",
      "  Batch 4000/19848, Average Loss: 0.037508967308327555\n",
      "  Batch 5000/19848, Average Loss: 0.03837840951047838\n",
      "  Batch 6000/19848, Average Loss: 0.039026479039341214\n",
      "  Batch 7000/19848, Average Loss: 0.038190857445821166\n",
      "  Batch 8000/19848, Average Loss: 0.03790650845970958\n",
      "  Batch 9000/19848, Average Loss: 0.03835016184393317\n",
      "  Batch 10000/19848, Average Loss: 0.03826520929578692\n",
      "  Batch 11000/19848, Average Loss: 0.038581293985247615\n",
      "  Batch 12000/19848, Average Loss: 0.03860041918046773\n",
      "  Batch 13000/19848, Average Loss: 0.03793488206341863\n",
      "  Batch 14000/19848, Average Loss: 0.038161116349510846\n",
      "  Batch 15000/19848, Average Loss: 0.038280143350362776\n",
      "  Batch 16000/19848, Average Loss: 0.03837554132565856\n",
      "  Batch 17000/19848, Average Loss: 0.03850823485851288\n",
      "  Batch 18000/19848, Average Loss: 0.03814618459064514\n",
      "  Batch 19000/19848, Average Loss: 0.03824326893221587\n",
      "  Average Training Loss: 0.03834824640867299\n",
      "  Average Training Loss: 0.03834824640867299, Average Validation Loss: 0.038101879343599175\n",
      "  Average Test Loss: 0.030370710190888415\n",
      "Experiment C+B/C： 32/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.11631129741668701\n",
      "  Batch 2000/19848, Average Loss: 0.08684879518672824\n",
      "  Batch 3000/19848, Average Loss: 0.056812620859593155\n",
      "  Batch 4000/19848, Average Loss: 0.05029789665527642\n",
      "  Batch 5000/19848, Average Loss: 0.0486281395573169\n",
      "  Batch 6000/19848, Average Loss: 0.046384955095127224\n",
      "  Batch 7000/19848, Average Loss: 0.04506219847500324\n",
      "  Batch 8000/19848, Average Loss: 0.0437347923591733\n",
      "  Batch 9000/19848, Average Loss: 0.0426011282466352\n",
      "  Batch 10000/19848, Average Loss: 0.041768074108287694\n",
      "  Batch 11000/19848, Average Loss: 0.04202636015787721\n",
      "  Batch 12000/19848, Average Loss: 0.04074501417391002\n",
      "  Batch 13000/19848, Average Loss: 0.04102459361776709\n",
      "  Batch 14000/19848, Average Loss: 0.04012805910408497\n",
      "  Batch 15000/19848, Average Loss: 0.03894709619134665\n",
      "  Batch 16000/19848, Average Loss: 0.038475026219151914\n",
      "  Batch 17000/19848, Average Loss: 0.03872249398939311\n",
      "  Batch 18000/19848, Average Loss: 0.039080033547244965\n",
      "  Batch 19000/19848, Average Loss: 0.038335718021728096\n",
      "  Average Training Loss: 0.048761465866475284\n",
      "  Average Training Loss: 0.048761465866475284, Average Validation Loss: 0.03796154451836163\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.0378097625374794\n",
      "  Batch 2000/19848, Average Loss: 0.037549053583294155\n",
      "  Batch 3000/19848, Average Loss: 0.03736202716454864\n",
      "  Batch 4000/19848, Average Loss: 0.037764451226219534\n",
      "  Batch 5000/19848, Average Loss: 0.03779887450393289\n",
      "  Batch 6000/19848, Average Loss: 0.037506288283504545\n",
      "  Batch 7000/19848, Average Loss: 0.03687298486940563\n",
      "  Batch 8000/19848, Average Loss: 0.03768818901665509\n",
      "  Batch 9000/19848, Average Loss: 0.036806639722548426\n",
      "  Batch 10000/19848, Average Loss: 0.03714209162630141\n",
      "  Batch 11000/19848, Average Loss: 0.03711324982903898\n",
      "  Batch 12000/19848, Average Loss: 0.03739309576153755\n",
      "  Batch 13000/19848, Average Loss: 0.0374279346880503\n",
      "  Batch 14000/19848, Average Loss: 0.03700980045087635\n",
      "  Batch 15000/19848, Average Loss: 0.037312162357382474\n",
      "  Batch 16000/19848, Average Loss: 0.03727004022244364\n",
      "  Batch 17000/19848, Average Loss: 0.03687222041375935\n",
      "  Batch 18000/19848, Average Loss: 0.0366972065763548\n",
      "  Batch 19000/19848, Average Loss: 0.036824167487211526\n",
      "  Average Training Loss: 0.03723739272769273\n",
      "  Average Training Loss: 0.03723739272769273, Average Validation Loss: 0.037345958868937296\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.03668984125740826\n",
      "  Batch 2000/19848, Average Loss: 0.03735484641045332\n",
      "  Batch 3000/19848, Average Loss: 0.03725516317877919\n",
      "  Batch 4000/19848, Average Loss: 0.03672427536174655\n",
      "  Batch 5000/19848, Average Loss: 0.03654326327145099\n",
      "  Batch 6000/19848, Average Loss: 0.03629229297395795\n",
      "  Batch 7000/19848, Average Loss: 0.036668948604725304\n",
      "  Batch 8000/19848, Average Loss: 0.03727808471489698\n",
      "  Batch 9000/19848, Average Loss: 0.03678730260115117\n",
      "  Batch 10000/19848, Average Loss: 0.03655431658960879\n",
      "  Batch 11000/19848, Average Loss: 0.03672771386615932\n",
      "  Batch 12000/19848, Average Loss: 0.036988930640742185\n",
      "  Batch 13000/19848, Average Loss: 0.036871800053864714\n",
      "  Batch 14000/19848, Average Loss: 0.03643943553883582\n",
      "  Batch 15000/19848, Average Loss: 0.03634488785732538\n",
      "  Batch 16000/19848, Average Loss: 0.036605037704110145\n",
      "  Batch 17000/19848, Average Loss: 0.03705480382684618\n",
      "  Batch 18000/19848, Average Loss: 0.036211756391450765\n",
      "  Batch 19000/19848, Average Loss: 0.03655366942938417\n",
      "  Average Training Loss: 0.036733842774253775\n",
      "  Average Training Loss: 0.036733842774253775, Average Validation Loss: 0.036676322143593953\n",
      "  Average Test Loss: 0.02880798435497209\n",
      "Experiment C+B/C： 33/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.27871807143837213\n",
      "  Batch 2000/19848, Average Loss: 0.12000528595224023\n",
      "  Batch 3000/19848, Average Loss: 0.11504433726519346\n",
      "  Batch 4000/19848, Average Loss: 0.08945819438621402\n",
      "  Batch 5000/19848, Average Loss: 0.058905895555391906\n",
      "  Batch 6000/19848, Average Loss: 0.05071741525549442\n",
      "  Batch 7000/19848, Average Loss: 0.04848046132363379\n",
      "  Batch 8000/19848, Average Loss: 0.04653399552963674\n",
      "  Batch 9000/19848, Average Loss: 0.045944424622692166\n",
      "  Batch 10000/19848, Average Loss: 0.04539649298787117\n",
      "  Batch 11000/19848, Average Loss: 0.045379248851910234\n",
      "  Batch 12000/19848, Average Loss: 0.04493693533912301\n",
      "  Batch 13000/19848, Average Loss: 0.04480961879529059\n",
      "  Batch 14000/19848, Average Loss: 0.04477401548810303\n",
      "  Batch 15000/19848, Average Loss: 0.04416555441357196\n",
      "  Batch 16000/19848, Average Loss: 0.043683700429275635\n",
      "  Batch 17000/19848, Average Loss: 0.04390223028324544\n",
      "  Batch 18000/19848, Average Loss: 0.043251106591895225\n",
      "  Batch 19000/19848, Average Loss: 0.042415843620896336\n",
      "  Average Training Loss: 0.06713276364322915\n",
      "  Average Training Loss: 0.06713276364322915, Average Validation Loss: 0.04263929335992815\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.04262734223715961\n",
      "  Batch 2000/19848, Average Loss: 0.042106637096032504\n",
      "  Batch 3000/19848, Average Loss: 0.042327155962586405\n",
      "  Batch 4000/19848, Average Loss: 0.04117677459679544\n",
      "  Batch 5000/19848, Average Loss: 0.04133338666334748\n",
      "  Batch 6000/19848, Average Loss: 0.0409592542918399\n",
      "  Batch 7000/19848, Average Loss: 0.03980580186005682\n",
      "  Batch 8000/19848, Average Loss: 0.03950246598478407\n",
      "  Batch 9000/19848, Average Loss: 0.03926164109539241\n",
      "  Batch 10000/19848, Average Loss: 0.038315992715768514\n",
      "  Batch 11000/19848, Average Loss: 0.03880796365067363\n",
      "  Batch 12000/19848, Average Loss: 0.03779892422351986\n",
      "  Batch 13000/19848, Average Loss: 0.037930209546349944\n",
      "  Batch 14000/19848, Average Loss: 0.037238320740871134\n",
      "  Batch 15000/19848, Average Loss: 0.03706097056437284\n",
      "  Batch 16000/19848, Average Loss: 0.037093111817725004\n",
      "  Batch 17000/19848, Average Loss: 0.03643601698242128\n",
      "  Batch 18000/19848, Average Loss: 0.03589734442625195\n",
      "  Batch 19000/19848, Average Loss: 0.036362157822586595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.03895585672387081\n",
      "  Average Training Loss: 0.03895585672387081, Average Validation Loss: 0.03631785321670649\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.03620659342594445\n",
      "  Batch 2000/19848, Average Loss: 0.036032582823187116\n",
      "  Batch 3000/19848, Average Loss: 0.036186439277604225\n",
      "  Batch 4000/19848, Average Loss: 0.03562751933094114\n",
      "  Batch 5000/19848, Average Loss: 0.03551177077833563\n",
      "  Batch 6000/19848, Average Loss: 0.03512663279380649\n",
      "  Batch 7000/19848, Average Loss: 0.035425985417328776\n",
      "  Batch 8000/19848, Average Loss: 0.035699676935561\n",
      "  Batch 9000/19848, Average Loss: 0.03531800069287419\n",
      "  Batch 10000/19848, Average Loss: 0.035438563631847504\n",
      "  Batch 11000/19848, Average Loss: 0.03536215520091355\n",
      "  Batch 12000/19848, Average Loss: 0.03537946267612278\n",
      "  Batch 13000/19848, Average Loss: 0.035655985556542875\n",
      "  Batch 14000/19848, Average Loss: 0.03498109780158848\n",
      "  Batch 15000/19848, Average Loss: 0.03529166688863188\n",
      "  Batch 16000/19848, Average Loss: 0.03533809563424438\n",
      "  Batch 17000/19848, Average Loss: 0.03563936228398234\n",
      "  Batch 18000/19848, Average Loss: 0.03439628726337105\n",
      "  Batch 19000/19848, Average Loss: 0.03493201129697263\n",
      "  Average Training Loss: 0.035440235573966215\n",
      "  Average Training Loss: 0.035440235573966215, Average Validation Loss: 0.03514062122005259\n",
      "  Average Test Loss: 0.028690343103200767\n",
      "Experiment C+B/C： 34/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.11848625764250756\n",
      "  Batch 2000/19848, Average Loss: 0.09894588367268443\n",
      "  Batch 3000/19848, Average Loss: 0.07084045271202923\n",
      "  Batch 4000/19848, Average Loss: 0.05420750955119729\n",
      "  Batch 5000/19848, Average Loss: 0.05139634726941585\n",
      "  Batch 6000/19848, Average Loss: 0.0507447825204581\n",
      "  Batch 7000/19848, Average Loss: 0.047988706098869446\n",
      "  Batch 8000/19848, Average Loss: 0.04611948698759079\n",
      "  Batch 9000/19848, Average Loss: 0.04535273565258831\n",
      "  Batch 10000/19848, Average Loss: 0.04482370196282864\n",
      "  Batch 11000/19848, Average Loss: 0.04433298290334642\n",
      "  Batch 12000/19848, Average Loss: 0.04403671629074961\n",
      "  Batch 13000/19848, Average Loss: 0.04421219226624817\n",
      "  Batch 14000/19848, Average Loss: 0.04336599242966622\n",
      "  Batch 15000/19848, Average Loss: 0.042728791492059826\n",
      "  Batch 16000/19848, Average Loss: 0.04341709227487445\n",
      "  Batch 17000/19848, Average Loss: 0.04212204955052584\n",
      "  Batch 18000/19848, Average Loss: 0.042214583402499554\n",
      "  Batch 19000/19848, Average Loss: 0.0421121277064085\n",
      "  Average Training Loss: 0.053080740606795934\n",
      "  Average Training Loss: 0.053080740606795934, Average Validation Loss: 0.041611913679087294\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.04191760850511491\n",
      "  Batch 2000/19848, Average Loss: 0.041068363586440684\n",
      "  Batch 3000/19848, Average Loss: 0.040816743449307975\n",
      "  Batch 4000/19848, Average Loss: 0.04030972649436444\n",
      "  Batch 5000/19848, Average Loss: 0.04025243507977575\n",
      "  Batch 6000/19848, Average Loss: 0.039723432762548326\n",
      "  Batch 7000/19848, Average Loss: 0.03917573745176196\n",
      "  Batch 8000/19848, Average Loss: 0.038544972454197704\n",
      "  Batch 9000/19848, Average Loss: 0.03821964086778462\n",
      "  Batch 10000/19848, Average Loss: 0.03837899901717901\n",
      "  Batch 11000/19848, Average Loss: 0.038131653322838244\n",
      "  Batch 12000/19848, Average Loss: 0.0373564002905041\n",
      "  Batch 13000/19848, Average Loss: 0.037853708690032364\n",
      "  Batch 14000/19848, Average Loss: 0.037403255023993554\n",
      "  Batch 15000/19848, Average Loss: 0.03653747176378965\n",
      "  Batch 16000/19848, Average Loss: 0.03683165136538446\n",
      "  Batch 17000/19848, Average Loss: 0.03669519812148064\n",
      "  Batch 18000/19848, Average Loss: 0.03690958609990776\n",
      "  Batch 19000/19848, Average Loss: 0.03659327810164541\n",
      "  Average Training Loss: 0.038470883300514416\n",
      "  Average Training Loss: 0.038470883300514416, Average Validation Loss: 0.03717418102861346\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.03661665196344256\n",
      "  Batch 2000/19848, Average Loss: 0.03603688732068986\n",
      "  Batch 3000/19848, Average Loss: 0.03658097508177161\n",
      "  Batch 4000/19848, Average Loss: 0.036770187879912555\n",
      "  Batch 5000/19848, Average Loss: 0.036079735375009477\n",
      "  Batch 6000/19848, Average Loss: 0.03612731043808162\n",
      "  Batch 7000/19848, Average Loss: 0.03607065411191434\n",
      "  Batch 8000/19848, Average Loss: 0.03578553330432624\n",
      "  Batch 9000/19848, Average Loss: 0.036411888281814755\n",
      "  Batch 10000/19848, Average Loss: 0.03549681365210563\n",
      "  Batch 11000/19848, Average Loss: 0.03592056457605213\n",
      "  Batch 12000/19848, Average Loss: 0.03583018379472196\n",
      "  Batch 13000/19848, Average Loss: 0.03446948012616485\n",
      "  Batch 14000/19848, Average Loss: 0.035543982367031275\n",
      "  Batch 15000/19848, Average Loss: 0.03605285727977753\n",
      "  Batch 16000/19848, Average Loss: 0.03485807898174971\n",
      "  Batch 17000/19848, Average Loss: 0.03511593884043396\n",
      "  Batch 18000/19848, Average Loss: 0.03509269370790571\n",
      "  Batch 19000/19848, Average Loss: 0.034759104370139536\n",
      "  Average Training Loss: 0.035703523866723395\n",
      "  Average Training Loss: 0.035703523866723395, Average Validation Loss: 0.03532587197515025\n",
      "  Average Test Loss: 0.03126645033403994\n",
      "Experiment C+B/C： 35/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/19848, Average Loss: 0.1332038599252701\n",
      "  Batch 2000/19848, Average Loss: 0.09208877968788147\n",
      "  Batch 3000/19848, Average Loss: 0.06116719141416252\n",
      "  Batch 4000/19848, Average Loss: 0.050693240823224184\n",
      "  Batch 5000/19848, Average Loss: 0.04703691512811929\n",
      "  Batch 6000/19848, Average Loss: 0.044288599418476224\n",
      "  Batch 7000/19848, Average Loss: 0.043916303001344206\n",
      "  Batch 8000/19848, Average Loss: 0.04253035508282483\n",
      "  Batch 9000/19848, Average Loss: 0.042160664843395354\n",
      "  Batch 10000/19848, Average Loss: 0.04139336170256138\n",
      "  Batch 11000/19848, Average Loss: 0.04014102109987289\n",
      "  Batch 12000/19848, Average Loss: 0.03961052752193064\n",
      "  Batch 13000/19848, Average Loss: 0.03964624939765781\n",
      "  Batch 14000/19848, Average Loss: 0.03930536285508424\n",
      "  Batch 15000/19848, Average Loss: 0.03843398355320096\n",
      "  Batch 16000/19848, Average Loss: 0.03922889449167997\n",
      "  Batch 17000/19848, Average Loss: 0.037382021468132735\n",
      "  Batch 18000/19848, Average Loss: 0.03800385418813676\n",
      "  Batch 19000/19848, Average Loss: 0.03778206303715706\n",
      "  Average Training Loss: 0.04933798076368715\n",
      "  Average Training Loss: 0.04933798076368715, Average Validation Loss: 0.03745968091849302\n",
      "Epoch 2/3: \n",
      "  Batch 1000/19848, Average Loss: 0.03829383179545402\n",
      "  Batch 2000/19848, Average Loss: 0.03697195094078779\n",
      "  Batch 3000/19848, Average Loss: 0.03674671065248549\n",
      "  Batch 4000/19848, Average Loss: 0.03695335765648633\n",
      "  Batch 5000/19848, Average Loss: 0.03651994007639587\n",
      "  Batch 6000/19848, Average Loss: 0.03641626724321395\n",
      "  Batch 7000/19848, Average Loss: 0.03718944129068404\n",
      "  Batch 8000/19848, Average Loss: 0.036996487416327\n",
      "  Batch 9000/19848, Average Loss: 0.03744485398661345\n",
      "  Batch 10000/19848, Average Loss: 0.03723296567425132\n",
      "  Batch 11000/19848, Average Loss: 0.03662507390696555\n",
      "  Batch 12000/19848, Average Loss: 0.036193644081242385\n",
      "  Batch 13000/19848, Average Loss: 0.03622970748506486\n",
      "  Batch 14000/19848, Average Loss: 0.03655648689996451\n",
      "  Batch 15000/19848, Average Loss: 0.036482676211744544\n",
      "  Batch 16000/19848, Average Loss: 0.03681009782850742\n",
      "  Batch 17000/19848, Average Loss: 0.036611548316664994\n",
      "  Batch 18000/19848, Average Loss: 0.035830120083875955\n",
      "  Batch 19000/19848, Average Loss: 0.03611654463969171\n",
      "  Average Training Loss: 0.03670879342908198\n",
      "  Average Training Loss: 0.03670879342908198, Average Validation Loss: 0.036452550291061606\n",
      "Epoch 3/3: \n",
      "  Batch 1000/19848, Average Loss: 0.036412968968972566\n",
      "  Batch 2000/19848, Average Loss: 0.0363187764538452\n",
      "  Batch 3000/19848, Average Loss: 0.03568845999706537\n",
      "  Batch 4000/19848, Average Loss: 0.0362606887947768\n",
      "  Batch 5000/19848, Average Loss: 0.036033168780617415\n",
      "  Batch 6000/19848, Average Loss: 0.03562214568816125\n",
      "  Batch 7000/19848, Average Loss: 0.03677245117072016\n",
      "  Batch 8000/19848, Average Loss: 0.03593925509694964\n",
      "  Batch 9000/19848, Average Loss: 0.035547097553499045\n",
      "  Batch 10000/19848, Average Loss: 0.036132335659116506\n",
      "  Batch 11000/19848, Average Loss: 0.035891334163956345\n",
      "  Batch 12000/19848, Average Loss: 0.03555621906556189\n",
      "  Batch 13000/19848, Average Loss: 0.036216351469047364\n",
      "  Batch 14000/19848, Average Loss: 0.035884521279484036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 15000/19848, Average Loss: 0.03555573335848749\n",
      "  Batch 16000/19848, Average Loss: 0.03586214438080788\n",
      "  Batch 17000/19848, Average Loss: 0.03588084703125059\n",
      "  Batch 18000/19848, Average Loss: 0.03554472411889583\n",
      "  Batch 19000/19848, Average Loss: 0.035593881557695564\n",
      "  Average Training Loss: 0.03591459114433907\n",
      "  Average Training Loss: 0.03591459114433907, Average Validation Loss: 0.03569879175975756\n",
      "  Average Test Loss: 0.029246490329409576\n",
      "Experiment C+B/B： 36/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.10442589898407459\n",
      "  Batch 2000/23979, Average Loss: 0.07837917445600033\n",
      "  Batch 3000/23979, Average Loss: 0.05665564600192011\n",
      "  Batch 4000/23979, Average Loss: 0.04894566727988422\n",
      "  Batch 5000/23979, Average Loss: 0.04689990120194852\n",
      "  Batch 6000/23979, Average Loss: 0.0440091182757169\n",
      "  Batch 7000/23979, Average Loss: 0.04337141398526728\n",
      "  Batch 8000/23979, Average Loss: 0.041966154133901\n",
      "  Batch 9000/23979, Average Loss: 0.04237409273535013\n",
      "  Batch 10000/23979, Average Loss: 0.04165448973327875\n",
      "  Batch 11000/23979, Average Loss: 0.04087612703256309\n",
      "  Batch 12000/23979, Average Loss: 0.04098761445656419\n",
      "  Batch 13000/23979, Average Loss: 0.041592538228258494\n",
      "  Batch 14000/23979, Average Loss: 0.04122958677075803\n",
      "  Batch 15000/23979, Average Loss: 0.0405781480288133\n",
      "  Batch 16000/23979, Average Loss: 0.04126916979905218\n",
      "  Batch 17000/23979, Average Loss: 0.040508763543330134\n",
      "  Batch 18000/23979, Average Loss: 0.04090188258420676\n",
      "  Batch 19000/23979, Average Loss: 0.04073621172178537\n",
      "  Batch 20000/23979, Average Loss: 0.0409612692873925\n",
      "  Batch 21000/23979, Average Loss: 0.04114430698379874\n",
      "  Batch 22000/23979, Average Loss: 0.04036452456191182\n",
      "  Batch 23000/23979, Average Loss: 0.04043973131850362\n",
      "  Average Training Loss: 0.046727985560249596\n",
      "  Average Training Loss: 0.046727985560249596, Average Validation Loss: 0.040770996315521287\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03990288243442774\n",
      "  Batch 2000/23979, Average Loss: 0.04030624411068857\n",
      "  Batch 3000/23979, Average Loss: 0.04016595372371375\n",
      "  Batch 4000/23979, Average Loss: 0.04074654556065798\n",
      "  Batch 5000/23979, Average Loss: 0.040560797399841246\n",
      "  Batch 6000/23979, Average Loss: 0.04090721448510885\n",
      "  Batch 7000/23979, Average Loss: 0.04157909257616848\n",
      "  Batch 8000/23979, Average Loss: 0.040790012448094784\n",
      "  Batch 9000/23979, Average Loss: 0.040470850807614624\n",
      "  Batch 10000/23979, Average Loss: 0.04076628987956792\n",
      "  Batch 11000/23979, Average Loss: 0.04011535471305251\n",
      "  Batch 12000/23979, Average Loss: 0.040881232177838685\n",
      "  Batch 13000/23979, Average Loss: 0.04034848576318473\n",
      "  Batch 14000/23979, Average Loss: 0.04010100271273404\n",
      "  Batch 15000/23979, Average Loss: 0.040036390188150105\n",
      "  Batch 16000/23979, Average Loss: 0.040400740209966896\n",
      "  Batch 17000/23979, Average Loss: 0.04031310631148517\n",
      "  Batch 18000/23979, Average Loss: 0.0411052666362375\n",
      "  Batch 19000/23979, Average Loss: 0.04109203433804214\n",
      "  Batch 20000/23979, Average Loss: 0.040842036014422774\n",
      "  Batch 21000/23979, Average Loss: 0.04019827843084931\n",
      "  Batch 22000/23979, Average Loss: 0.04037026042677462\n",
      "  Batch 23000/23979, Average Loss: 0.040286043141037224\n",
      "  Average Training Loss: 0.04053729223320853\n",
      "  Average Training Loss: 0.04053729223320853, Average Validation Loss: 0.040475095843459735\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.040947658920660615\n",
      "  Batch 2000/23979, Average Loss: 0.04020288661494851\n",
      "  Batch 3000/23979, Average Loss: 0.03957761687319726\n",
      "  Batch 4000/23979, Average Loss: 0.04017970461770892\n",
      "  Batch 5000/23979, Average Loss: 0.040346680809743705\n",
      "  Batch 6000/23979, Average Loss: 0.040648835334926844\n",
      "  Batch 7000/23979, Average Loss: 0.04032456513494253\n",
      "  Batch 8000/23979, Average Loss: 0.04085039994958788\n",
      "  Batch 9000/23979, Average Loss: 0.0395809482159093\n",
      "  Batch 10000/23979, Average Loss: 0.040523603203706446\n",
      "  Batch 11000/23979, Average Loss: 0.03998085492663085\n",
      "  Batch 12000/23979, Average Loss: 0.04014069815352559\n",
      "  Batch 13000/23979, Average Loss: 0.03960478092171252\n",
      "  Batch 14000/23979, Average Loss: 0.03958161683753133\n",
      "  Batch 15000/23979, Average Loss: 0.039721005821600555\n",
      "  Batch 16000/23979, Average Loss: 0.04031593239121139\n",
      "  Batch 17000/23979, Average Loss: 0.03918897627945989\n",
      "  Batch 18000/23979, Average Loss: 0.04002350132633001\n",
      "  Batch 19000/23979, Average Loss: 0.04015723233297467\n",
      "  Batch 20000/23979, Average Loss: 0.04047217259090394\n",
      "  Batch 21000/23979, Average Loss: 0.039682006480172276\n",
      "  Batch 22000/23979, Average Loss: 0.04003210030589253\n",
      "  Batch 23000/23979, Average Loss: 0.039883045732975006\n",
      "  Average Training Loss: 0.04008057073009264\n",
      "  Average Training Loss: 0.04008057073009264, Average Validation Loss: 0.039976776930021184\n",
      "  Average Test Loss: 0.07849840364678635\n",
      "Experiment C+B/B： 37/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.2040275510046631\n",
      "  Batch 2000/23979, Average Loss: 0.08270613550394773\n",
      "  Batch 3000/23979, Average Loss: 0.06894019511714577\n",
      "  Batch 4000/23979, Average Loss: 0.0549325364716351\n",
      "  Batch 5000/23979, Average Loss: 0.04899655423406511\n",
      "  Batch 6000/23979, Average Loss: 0.04467023143079132\n",
      "  Batch 7000/23979, Average Loss: 0.04256853619962931\n",
      "  Batch 8000/23979, Average Loss: 0.03995193370152265\n",
      "  Batch 9000/23979, Average Loss: 0.039372040507383646\n",
      "  Batch 10000/23979, Average Loss: 0.03866275160107761\n",
      "  Batch 11000/23979, Average Loss: 0.03844153802935034\n",
      "  Batch 12000/23979, Average Loss: 0.038244269996881484\n",
      "  Batch 13000/23979, Average Loss: 0.03867097592353821\n",
      "  Batch 14000/23979, Average Loss: 0.038738399023190144\n",
      "  Batch 15000/23979, Average Loss: 0.03843196752574295\n",
      "  Batch 16000/23979, Average Loss: 0.03735542423930019\n",
      "  Batch 17000/23979, Average Loss: 0.03785981083009392\n",
      "  Batch 18000/23979, Average Loss: 0.03861575032118708\n",
      "  Batch 19000/23979, Average Loss: 0.03749748450797051\n",
      "  Batch 20000/23979, Average Loss: 0.03792587308399379\n",
      "  Batch 21000/23979, Average Loss: 0.03734480473678559\n",
      "  Batch 22000/23979, Average Loss: 0.03759215526934713\n",
      "  Batch 23000/23979, Average Loss: 0.03815262449439615\n",
      "  Average Training Loss: 0.049915049708133714\n",
      "  Average Training Loss: 0.049915049708133714, Average Validation Loss: 0.037744436819289656\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03790186440199614\n",
      "  Batch 2000/23979, Average Loss: 0.03778321921173483\n",
      "  Batch 3000/23979, Average Loss: 0.037018386407755316\n",
      "  Batch 4000/23979, Average Loss: 0.037371400942094624\n",
      "  Batch 5000/23979, Average Loss: 0.0378312230668962\n",
      "  Batch 6000/23979, Average Loss: 0.03830521708074957\n",
      "  Batch 7000/23979, Average Loss: 0.037561362049542364\n",
      "  Batch 8000/23979, Average Loss: 0.03783227149862796\n",
      "  Batch 9000/23979, Average Loss: 0.038007949934341013\n",
      "  Batch 10000/23979, Average Loss: 0.03767250252980739\n",
      "  Batch 11000/23979, Average Loss: 0.03760084179509431\n",
      "  Batch 12000/23979, Average Loss: 0.037655083964578805\n",
      "  Batch 13000/23979, Average Loss: 0.03771042929310352\n",
      "  Batch 14000/23979, Average Loss: 0.03743999986629933\n",
      "  Batch 15000/23979, Average Loss: 0.03704708915576339\n",
      "  Batch 16000/23979, Average Loss: 0.037847621025517586\n",
      "  Batch 17000/23979, Average Loss: 0.037018695298582316\n",
      "  Batch 18000/23979, Average Loss: 0.03763125250209123\n",
      "  Batch 19000/23979, Average Loss: 0.038102525641210375\n",
      "  Batch 20000/23979, Average Loss: 0.03669499317090958\n",
      "  Batch 21000/23979, Average Loss: 0.03794547347165644\n",
      "  Batch 22000/23979, Average Loss: 0.037337910714559254\n",
      "  Batch 23000/23979, Average Loss: 0.03730198555532843\n",
      "  Average Training Loss: 0.03756978551467346\n",
      "  Average Training Loss: 0.03756978551467346, Average Validation Loss: 0.03776805044335932\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03784160503000021\n",
      "  Batch 2000/23979, Average Loss: 0.037826235248707235\n",
      "  Batch 3000/23979, Average Loss: 0.03713532230257988\n",
      "  Batch 4000/23979, Average Loss: 0.03741733897477388\n",
      "  Batch 5000/23979, Average Loss: 0.03748214986547828\n",
      "  Batch 6000/23979, Average Loss: 0.03764916336163879\n",
      "  Batch 7000/23979, Average Loss: 0.0373198690507561\n",
      "  Batch 8000/23979, Average Loss: 0.037135512419044975\n",
      "  Batch 9000/23979, Average Loss: 0.03753636691439897\n",
      "  Batch 10000/23979, Average Loss: 0.03741150572244078\n",
      "  Batch 11000/23979, Average Loss: 0.03715122947376221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 12000/23979, Average Loss: 0.036781905690208075\n",
      "  Batch 13000/23979, Average Loss: 0.03774887490365654\n",
      "  Batch 14000/23979, Average Loss: 0.03732934091798961\n",
      "  Batch 15000/23979, Average Loss: 0.036643342351540924\n",
      "  Batch 16000/23979, Average Loss: 0.03771401237230748\n",
      "  Batch 17000/23979, Average Loss: 0.03706129995640367\n",
      "  Batch 18000/23979, Average Loss: 0.03721898824721575\n",
      "  Batch 19000/23979, Average Loss: 0.03714998689014465\n",
      "  Batch 20000/23979, Average Loss: 0.03794217705074698\n",
      "  Batch 21000/23979, Average Loss: 0.0378732980908826\n",
      "  Batch 22000/23979, Average Loss: 0.037207655563019214\n",
      "  Batch 23000/23979, Average Loss: 0.036738585343584415\n",
      "  Average Training Loss: 0.03732527251184419\n",
      "  Average Training Loss: 0.03732527251184419, Average Validation Loss: 0.03728460456066772\n",
      "  Average Test Loss: 0.07117307037743699\n",
      "Experiment C+B/B： 38/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.08898890277743339\n",
      "  Batch 2000/23979, Average Loss: 0.05308679444622248\n",
      "  Batch 3000/23979, Average Loss: 0.044653808815404775\n",
      "  Batch 4000/23979, Average Loss: 0.04143053155206144\n",
      "  Batch 5000/23979, Average Loss: 0.039399925077334044\n",
      "  Batch 6000/23979, Average Loss: 0.03896976655349135\n",
      "  Batch 7000/23979, Average Loss: 0.038287798944860695\n",
      "  Batch 8000/23979, Average Loss: 0.03759098139405251\n",
      "  Batch 9000/23979, Average Loss: 0.03696408526692539\n",
      "  Batch 10000/23979, Average Loss: 0.037198565796017644\n",
      "  Batch 11000/23979, Average Loss: 0.03729027264006436\n",
      "  Batch 12000/23979, Average Loss: 0.037178596651181575\n",
      "  Batch 13000/23979, Average Loss: 0.037040529841557146\n",
      "  Batch 14000/23979, Average Loss: 0.03669744796026498\n",
      "  Batch 15000/23979, Average Loss: 0.03678479897230864\n",
      "  Batch 16000/23979, Average Loss: 0.03682378685101867\n",
      "  Batch 17000/23979, Average Loss: 0.03676452787127346\n",
      "  Batch 18000/23979, Average Loss: 0.036148901221342386\n",
      "  Batch 19000/23979, Average Loss: 0.03628059688489884\n",
      "  Batch 20000/23979, Average Loss: 0.03621908208169043\n",
      "  Batch 21000/23979, Average Loss: 0.036150216456502675\n",
      "  Batch 22000/23979, Average Loss: 0.036296368474140764\n",
      "  Batch 23000/23979, Average Loss: 0.03569384248368442\n",
      "  Average Training Loss: 0.04032770254224672\n",
      "  Average Training Loss: 0.04032770254224672, Average Validation Loss: 0.03597634486058685\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03524990676995367\n",
      "  Batch 2000/23979, Average Loss: 0.03510760187637061\n",
      "  Batch 3000/23979, Average Loss: 0.036008893216028806\n",
      "  Batch 4000/23979, Average Loss: 0.03592241251002997\n",
      "  Batch 5000/23979, Average Loss: 0.034906419487670065\n",
      "  Batch 6000/23979, Average Loss: 0.0352253633486107\n",
      "  Batch 7000/23979, Average Loss: 0.035202270495705304\n",
      "  Batch 8000/23979, Average Loss: 0.035299788477830586\n",
      "  Batch 9000/23979, Average Loss: 0.03475723064225167\n",
      "  Batch 10000/23979, Average Loss: 0.035019025167450306\n",
      "  Batch 11000/23979, Average Loss: 0.03516920454520732\n",
      "  Batch 12000/23979, Average Loss: 0.03561096482444555\n",
      "  Batch 13000/23979, Average Loss: 0.034988608452491465\n",
      "  Batch 14000/23979, Average Loss: 0.03512256774213165\n",
      "  Batch 15000/23979, Average Loss: 0.034893638604320584\n",
      "  Batch 16000/23979, Average Loss: 0.03490109512582421\n",
      "  Batch 17000/23979, Average Loss: 0.035355718014761804\n",
      "  Batch 18000/23979, Average Loss: 0.03515199638064951\n",
      "  Batch 19000/23979, Average Loss: 0.03461683635506779\n",
      "  Batch 20000/23979, Average Loss: 0.03454007385764271\n",
      "  Batch 21000/23979, Average Loss: 0.03489958897046745\n",
      "  Batch 22000/23979, Average Loss: 0.03465310290083289\n",
      "  Batch 23000/23979, Average Loss: 0.0345636541005224\n",
      "  Average Training Loss: 0.03504407714313336\n",
      "  Average Training Loss: 0.03504407714313336, Average Validation Loss: 0.034767969648467935\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03383558472525328\n",
      "  Batch 2000/23979, Average Loss: 0.03488397011999041\n",
      "  Batch 3000/23979, Average Loss: 0.03478623336646706\n",
      "  Batch 4000/23979, Average Loss: 0.03396892143785953\n",
      "  Batch 5000/23979, Average Loss: 0.0343925152849406\n",
      "  Batch 6000/23979, Average Loss: 0.034404725004918875\n",
      "  Batch 7000/23979, Average Loss: 0.0344971370883286\n",
      "  Batch 8000/23979, Average Loss: 0.03438398922327906\n",
      "  Batch 9000/23979, Average Loss: 0.034445279656909404\n",
      "  Batch 10000/23979, Average Loss: 0.03479896832909435\n",
      "  Batch 11000/23979, Average Loss: 0.033917138144373894\n",
      "  Batch 12000/23979, Average Loss: 0.03439332515187561\n",
      "  Batch 13000/23979, Average Loss: 0.034780941498465834\n",
      "  Batch 14000/23979, Average Loss: 0.03427826415840536\n",
      "  Batch 15000/23979, Average Loss: 0.03439527911692858\n",
      "  Batch 16000/23979, Average Loss: 0.0344530958533287\n",
      "  Batch 17000/23979, Average Loss: 0.034700756282545625\n",
      "  Batch 18000/23979, Average Loss: 0.034392852168530226\n",
      "  Batch 19000/23979, Average Loss: 0.03420119718182832\n",
      "  Batch 20000/23979, Average Loss: 0.03412136884685606\n",
      "  Batch 21000/23979, Average Loss: 0.0339794428255409\n",
      "  Batch 22000/23979, Average Loss: 0.03374285094160587\n",
      "  Batch 23000/23979, Average Loss: 0.03398551119863987\n",
      "  Average Training Loss: 0.03431667222139724\n",
      "  Average Training Loss: 0.03431667222139724, Average Validation Loss: 0.03445455485937792\n",
      "  Average Test Loss: 0.07269165218273446\n",
      "Experiment C+B/B： 39/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.15721109939739109\n",
      "  Batch 2000/23979, Average Loss: 0.11488129932805896\n",
      "  Batch 3000/23979, Average Loss: 0.10137089459225536\n",
      "  Batch 4000/23979, Average Loss: 0.07469430288858711\n",
      "  Batch 5000/23979, Average Loss: 0.05117199134081602\n",
      "  Batch 6000/23979, Average Loss: 0.04419531655870378\n",
      "  Batch 7000/23979, Average Loss: 0.04103574254643172\n",
      "  Batch 8000/23979, Average Loss: 0.03895112757757306\n",
      "  Batch 9000/23979, Average Loss: 0.037990969130769374\n",
      "  Batch 10000/23979, Average Loss: 0.03707246590591967\n",
      "  Batch 11000/23979, Average Loss: 0.03586984191276133\n",
      "  Batch 12000/23979, Average Loss: 0.03578901178948581\n",
      "  Batch 13000/23979, Average Loss: 0.03576707686670125\n",
      "  Batch 14000/23979, Average Loss: 0.035021372391842304\n",
      "  Batch 15000/23979, Average Loss: 0.03525137023907155\n",
      "  Batch 16000/23979, Average Loss: 0.035317135946825146\n",
      "  Batch 17000/23979, Average Loss: 0.03506238510459662\n",
      "  Batch 18000/23979, Average Loss: 0.03486067723017186\n",
      "  Batch 19000/23979, Average Loss: 0.0353091086987406\n",
      "  Batch 20000/23979, Average Loss: 0.034264015491120514\n",
      "  Batch 21000/23979, Average Loss: 0.0345972707234323\n",
      "  Batch 22000/23979, Average Loss: 0.034390704312361776\n",
      "  Batch 23000/23979, Average Loss: 0.0334916638052091\n",
      "  Average Training Loss: 0.04946369372619433\n",
      "  Average Training Loss: 0.04946369372619433, Average Validation Loss: 0.03408332156379388\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03369493253901601\n",
      "  Batch 2000/23979, Average Loss: 0.03324399496521801\n",
      "  Batch 3000/23979, Average Loss: 0.033740027331747115\n",
      "  Batch 4000/23979, Average Loss: 0.03414231305103749\n",
      "  Batch 5000/23979, Average Loss: 0.03354832905251533\n",
      "  Batch 6000/23979, Average Loss: 0.03303751217760146\n",
      "  Batch 7000/23979, Average Loss: 0.03321545268921182\n",
      "  Batch 8000/23979, Average Loss: 0.03334154208097607\n",
      "  Batch 9000/23979, Average Loss: 0.03378780262544751\n",
      "  Batch 10000/23979, Average Loss: 0.03324140550848097\n",
      "  Batch 11000/23979, Average Loss: 0.033315963044762614\n",
      "  Batch 12000/23979, Average Loss: 0.03320922564063221\n",
      "  Batch 13000/23979, Average Loss: 0.03326206121314317\n",
      "  Batch 14000/23979, Average Loss: 0.033144129214808346\n",
      "  Batch 15000/23979, Average Loss: 0.032788600228726865\n",
      "  Batch 16000/23979, Average Loss: 0.03313249044213444\n",
      "  Batch 17000/23979, Average Loss: 0.0338547924477607\n",
      "  Batch 18000/23979, Average Loss: 0.03308953226450831\n",
      "  Batch 19000/23979, Average Loss: 0.032951692828908564\n",
      "  Batch 20000/23979, Average Loss: 0.033397018214687706\n",
      "  Batch 21000/23979, Average Loss: 0.03324093951843679\n",
      "  Batch 22000/23979, Average Loss: 0.03297551538795233\n",
      "  Batch 23000/23979, Average Loss: 0.03333172509633005\n",
      "  Average Training Loss: 0.0333147609817165\n",
      "  Average Training Loss: 0.0333147609817165, Average Validation Loss: 0.03355082046190856\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03305410433374345\n",
      "  Batch 2000/23979, Average Loss: 0.03303891125787049\n",
      "  Batch 3000/23979, Average Loss: 0.03299180422630161\n",
      "  Batch 4000/23979, Average Loss: 0.03333141969051212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 5000/23979, Average Loss: 0.03334510768391192\n",
      "  Batch 6000/23979, Average Loss: 0.03276642506476492\n",
      "  Batch 7000/23979, Average Loss: 0.0329176916833967\n",
      "  Batch 8000/23979, Average Loss: 0.032911257404834034\n",
      "  Batch 9000/23979, Average Loss: 0.032959914840757845\n",
      "  Batch 10000/23979, Average Loss: 0.03273525526747108\n",
      "  Batch 11000/23979, Average Loss: 0.03289482039865106\n",
      "  Batch 12000/23979, Average Loss: 0.03331813392229378\n",
      "  Batch 13000/23979, Average Loss: 0.03313272415846586\n",
      "  Batch 14000/23979, Average Loss: 0.032241100509651004\n",
      "  Batch 15000/23979, Average Loss: 0.033150076164864005\n",
      "  Batch 16000/23979, Average Loss: 0.03362448631227016\n",
      "  Batch 17000/23979, Average Loss: 0.03275361435953528\n",
      "  Batch 18000/23979, Average Loss: 0.032784526153467594\n",
      "  Batch 19000/23979, Average Loss: 0.03272892697155476\n",
      "  Batch 20000/23979, Average Loss: 0.032843271254561844\n",
      "  Batch 21000/23979, Average Loss: 0.03284054550062865\n",
      "  Batch 22000/23979, Average Loss: 0.032677232465706765\n",
      "  Batch 23000/23979, Average Loss: 0.0323524811686948\n",
      "  Average Training Loss: 0.032923321662569116\n",
      "  Average Training Loss: 0.032923321662569116, Average Validation Loss: 0.033030438233163824\n",
      "  Average Test Loss: 0.06576841797094071\n",
      "Experiment C+B/B： 40/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.11637205710634589\n",
      "  Batch 2000/23979, Average Loss: 0.07585620623268187\n",
      "  Batch 3000/23979, Average Loss: 0.04680831409804523\n",
      "  Batch 4000/23979, Average Loss: 0.0431171501101926\n",
      "  Batch 5000/23979, Average Loss: 0.03916130360402167\n",
      "  Batch 6000/23979, Average Loss: 0.03891537703294307\n",
      "  Batch 7000/23979, Average Loss: 0.03728011855483055\n",
      "  Batch 8000/23979, Average Loss: 0.03736749025527388\n",
      "  Batch 9000/23979, Average Loss: 0.03708248351700604\n",
      "  Batch 10000/23979, Average Loss: 0.03641834862250835\n",
      "  Batch 11000/23979, Average Loss: 0.036166145102120933\n",
      "  Batch 12000/23979, Average Loss: 0.03594814427383244\n",
      "  Batch 13000/23979, Average Loss: 0.03556775886937976\n",
      "  Batch 14000/23979, Average Loss: 0.035363591542467473\n",
      "  Batch 15000/23979, Average Loss: 0.03612589926831424\n",
      "  Batch 16000/23979, Average Loss: 0.0352128840200603\n",
      "  Batch 17000/23979, Average Loss: 0.03587761739920825\n",
      "  Batch 18000/23979, Average Loss: 0.035010110668838024\n",
      "  Batch 19000/23979, Average Loss: 0.03483873611781746\n",
      "  Batch 20000/23979, Average Loss: 0.035231625341810284\n",
      "  Batch 21000/23979, Average Loss: 0.03499460966791958\n",
      "  Batch 22000/23979, Average Loss: 0.034543328310363\n",
      "  Batch 23000/23979, Average Loss: 0.03506043779011816\n",
      "  Average Training Loss: 0.04179813623827766\n",
      "  Average Training Loss: 0.04179813623827766, Average Validation Loss: 0.0350286083114182\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.03422368273139\n",
      "  Batch 2000/23979, Average Loss: 0.03499589590355754\n",
      "  Batch 3000/23979, Average Loss: 0.034428328379988674\n",
      "  Batch 4000/23979, Average Loss: 0.034272691685706375\n",
      "  Batch 5000/23979, Average Loss: 0.03433396837953478\n",
      "  Batch 6000/23979, Average Loss: 0.033699274021200834\n",
      "  Batch 7000/23979, Average Loss: 0.03466901870537549\n",
      "  Batch 8000/23979, Average Loss: 0.03458844880852848\n",
      "  Batch 9000/23979, Average Loss: 0.03415212238021195\n",
      "  Batch 10000/23979, Average Loss: 0.03390892308205366\n",
      "  Batch 11000/23979, Average Loss: 0.03407922992203385\n",
      "  Batch 12000/23979, Average Loss: 0.033265005636028945\n",
      "  Batch 13000/23979, Average Loss: 0.03407242706418037\n",
      "  Batch 14000/23979, Average Loss: 0.03348867397941649\n",
      "  Batch 15000/23979, Average Loss: 0.033481298743747175\n",
      "  Batch 16000/23979, Average Loss: 0.03361867484822869\n",
      "  Batch 17000/23979, Average Loss: 0.034115184633992615\n",
      "  Batch 18000/23979, Average Loss: 0.03341779192537069\n",
      "  Batch 19000/23979, Average Loss: 0.03369635639060289\n",
      "  Batch 20000/23979, Average Loss: 0.03304518096148968\n",
      "  Batch 21000/23979, Average Loss: 0.03340395677834749\n",
      "  Batch 22000/23979, Average Loss: 0.03311210506688803\n",
      "  Batch 23000/23979, Average Loss: 0.03287108194828033\n",
      "  Average Training Loss: 0.033826810471352185\n",
      "  Average Training Loss: 0.033826810471352185, Average Validation Loss: 0.03355531011093672\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.033675316453911364\n",
      "  Batch 2000/23979, Average Loss: 0.03317934112623334\n",
      "  Batch 3000/23979, Average Loss: 0.03302397244796157\n",
      "  Batch 4000/23979, Average Loss: 0.0327811733931303\n",
      "  Batch 5000/23979, Average Loss: 0.03289707189891487\n",
      "  Batch 6000/23979, Average Loss: 0.03317596279457211\n",
      "  Batch 7000/23979, Average Loss: 0.032856862487271425\n",
      "  Batch 8000/23979, Average Loss: 0.03280093276966363\n",
      "  Batch 9000/23979, Average Loss: 0.03325479408632964\n",
      "  Batch 10000/23979, Average Loss: 0.03271764255035669\n",
      "  Batch 11000/23979, Average Loss: 0.03275381795410067\n",
      "  Batch 12000/23979, Average Loss: 0.033036454456858336\n",
      "  Batch 13000/23979, Average Loss: 0.03249734824709594\n",
      "  Batch 14000/23979, Average Loss: 0.032943223809823396\n",
      "  Batch 15000/23979, Average Loss: 0.032808650507591665\n",
      "  Batch 16000/23979, Average Loss: 0.03295268107950687\n",
      "  Batch 17000/23979, Average Loss: 0.0328100550416857\n",
      "  Batch 18000/23979, Average Loss: 0.03336577190645039\n",
      "  Batch 19000/23979, Average Loss: 0.03299985582474619\n",
      "  Batch 20000/23979, Average Loss: 0.03299753850512206\n",
      "  Batch 21000/23979, Average Loss: 0.032969243173487484\n",
      "  Batch 22000/23979, Average Loss: 0.03252008828520775\n",
      "  Batch 23000/23979, Average Loss: 0.033196216834709046\n",
      "  Average Training Loss: 0.03298813719996082\n",
      "  Average Training Loss: 0.03298813719996082, Average Validation Loss: 0.03306762051564316\n",
      "  Average Test Loss: 0.07100499476112597\n",
      "Experiment C+B/B： 41/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.10461537853255867\n",
      "  Batch 2000/23979, Average Loss: 0.05978649328742176\n",
      "  Batch 3000/23979, Average Loss: 0.04438686346821487\n",
      "  Batch 4000/23979, Average Loss: 0.04146298651583493\n",
      "  Batch 5000/23979, Average Loss: 0.04076713046617806\n",
      "  Batch 6000/23979, Average Loss: 0.03851743795722723\n",
      "  Batch 7000/23979, Average Loss: 0.03889148815348745\n",
      "  Batch 8000/23979, Average Loss: 0.03845771972369403\n",
      "  Batch 9000/23979, Average Loss: 0.03787989698257297\n",
      "  Batch 10000/23979, Average Loss: 0.03704107302520424\n",
      "  Batch 11000/23979, Average Loss: 0.03860571881756186\n",
      "  Batch 12000/23979, Average Loss: 0.03660196293145418\n",
      "  Batch 13000/23979, Average Loss: 0.03614955951925367\n",
      "  Batch 14000/23979, Average Loss: 0.036491695318371056\n",
      "  Batch 15000/23979, Average Loss: 0.03562083987146616\n",
      "  Batch 16000/23979, Average Loss: 0.035312703025527296\n",
      "  Batch 17000/23979, Average Loss: 0.03555545763671398\n",
      "  Batch 18000/23979, Average Loss: 0.03524056761618704\n",
      "  Batch 19000/23979, Average Loss: 0.03534245917014778\n",
      "  Batch 20000/23979, Average Loss: 0.03448362440615892\n",
      "  Batch 21000/23979, Average Loss: 0.03487398563325405\n",
      "  Batch 22000/23979, Average Loss: 0.03401039757858962\n",
      "  Batch 23000/23979, Average Loss: 0.03413913944549859\n",
      "  Average Training Loss: 0.04078755104613926\n",
      "  Average Training Loss: 0.04078755104613926, Average Validation Loss: 0.03432663007498757\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.0342228541681543\n",
      "  Batch 2000/23979, Average Loss: 0.03354312155488878\n",
      "  Batch 3000/23979, Average Loss: 0.034122267649509014\n",
      "  Batch 4000/23979, Average Loss: 0.03383563041966409\n",
      "  Batch 5000/23979, Average Loss: 0.03411133845616132\n",
      "  Batch 6000/23979, Average Loss: 0.03419991239439696\n",
      "  Batch 7000/23979, Average Loss: 0.0331806026827544\n",
      "  Batch 8000/23979, Average Loss: 0.033592018820345404\n",
      "  Batch 9000/23979, Average Loss: 0.032963660800829526\n",
      "  Batch 10000/23979, Average Loss: 0.033972996573895214\n",
      "  Batch 11000/23979, Average Loss: 0.033442413821816445\n",
      "  Batch 12000/23979, Average Loss: 0.033497586246579884\n",
      "  Batch 13000/23979, Average Loss: 0.03374592254031449\n",
      "  Batch 14000/23979, Average Loss: 0.03346971750073135\n",
      "  Batch 15000/23979, Average Loss: 0.033353601186536255\n",
      "  Batch 16000/23979, Average Loss: 0.033833104875870046\n",
      "  Batch 17000/23979, Average Loss: 0.03360339045058936\n",
      "  Batch 18000/23979, Average Loss: 0.033163714130874725\n",
      "  Batch 19000/23979, Average Loss: 0.033602229638025165\n",
      "  Batch 20000/23979, Average Loss: 0.03381328746303916\n",
      "  Batch 21000/23979, Average Loss: 0.032894701343029735\n",
      "  Batch 22000/23979, Average Loss: 0.03262763714045286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 23000/23979, Average Loss: 0.032985213689506056\n",
      "  Average Training Loss: 0.0335373563202873\n",
      "  Average Training Loss: 0.0335373563202873, Average Validation Loss: 0.033277638946614035\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.033295349849388006\n",
      "  Batch 2000/23979, Average Loss: 0.0327170535903424\n",
      "  Batch 3000/23979, Average Loss: 0.033307253216393294\n",
      "  Batch 4000/23979, Average Loss: 0.03278836988471449\n",
      "  Batch 5000/23979, Average Loss: 0.03267102025821805\n",
      "  Batch 6000/23979, Average Loss: 0.03304217271134257\n",
      "  Batch 7000/23979, Average Loss: 0.03352528536319733\n",
      "  Batch 8000/23979, Average Loss: 0.03288214668817818\n",
      "  Batch 9000/23979, Average Loss: 0.03307229412812739\n",
      "  Batch 10000/23979, Average Loss: 0.032763990118168294\n",
      "  Batch 11000/23979, Average Loss: 0.03222079453524202\n",
      "  Batch 12000/23979, Average Loss: 0.032986497811973096\n",
      "  Batch 13000/23979, Average Loss: 0.03282971279229969\n",
      "  Batch 14000/23979, Average Loss: 0.03305692618805915\n",
      "  Batch 15000/23979, Average Loss: 0.032462566224858165\n",
      "  Batch 16000/23979, Average Loss: 0.033338963753543796\n",
      "  Batch 17000/23979, Average Loss: 0.03317941648885608\n",
      "  Batch 18000/23979, Average Loss: 0.03322637401800603\n",
      "  Batch 19000/23979, Average Loss: 0.03327957958728075\n",
      "  Batch 20000/23979, Average Loss: 0.033379047252237795\n",
      "  Batch 21000/23979, Average Loss: 0.03249477105401456\n",
      "  Batch 22000/23979, Average Loss: 0.03298893488012254\n",
      "  Batch 23000/23979, Average Loss: 0.03300383434724063\n",
      "  Average Training Loss: 0.03298157288028083\n",
      "  Average Training Loss: 0.03298157288028083, Average Validation Loss: 0.03294149248226172\n",
      "  Average Test Loss: 0.06451808751707973\n",
      "Experiment C+B/B： 42/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/23979, Average Loss: 0.19614618903025985\n",
      "  Batch 2000/23979, Average Loss: 0.10497097426652909\n",
      "  Batch 3000/23979, Average Loss: 0.08381539563834667\n",
      "  Batch 4000/23979, Average Loss: 0.051297567198984324\n",
      "  Batch 5000/23979, Average Loss: 0.045571033844724294\n",
      "  Batch 6000/23979, Average Loss: 0.04265599870122969\n",
      "  Batch 7000/23979, Average Loss: 0.04067515373788774\n",
      "  Batch 8000/23979, Average Loss: 0.03974765587411821\n",
      "  Batch 9000/23979, Average Loss: 0.03855432185344398\n",
      "  Batch 10000/23979, Average Loss: 0.038197210364043714\n",
      "  Batch 11000/23979, Average Loss: 0.03763720152527094\n",
      "  Batch 12000/23979, Average Loss: 0.036309910764917734\n",
      "  Batch 13000/23979, Average Loss: 0.03599615686293691\n",
      "  Batch 14000/23979, Average Loss: 0.03555247610900551\n",
      "  Batch 15000/23979, Average Loss: 0.035438299504108726\n",
      "  Batch 16000/23979, Average Loss: 0.035000563751906154\n",
      "  Batch 17000/23979, Average Loss: 0.034895541618578135\n",
      "  Batch 18000/23979, Average Loss: 0.03461348757334053\n",
      "  Batch 19000/23979, Average Loss: 0.03472751828469336\n",
      "  Batch 20000/23979, Average Loss: 0.033625699586234986\n",
      "  Batch 21000/23979, Average Loss: 0.03478469092864543\n",
      "  Batch 22000/23979, Average Loss: 0.03383968990761787\n",
      "  Batch 23000/23979, Average Loss: 0.03390060961712152\n",
      "  Average Training Loss: 0.04880805433487348\n",
      "  Average Training Loss: 0.04880805433487348, Average Validation Loss: 0.03370053533859259\n",
      "Epoch 2/3: \n",
      "  Batch 1000/23979, Average Loss: 0.033369662969373166\n",
      "  Batch 2000/23979, Average Loss: 0.03350997501797974\n",
      "  Batch 3000/23979, Average Loss: 0.033682772879488766\n",
      "  Batch 4000/23979, Average Loss: 0.033362914294935764\n",
      "  Batch 5000/23979, Average Loss: 0.033620052695274356\n",
      "  Batch 6000/23979, Average Loss: 0.03297208312526345\n",
      "  Batch 7000/23979, Average Loss: 0.033489023271948096\n",
      "  Batch 8000/23979, Average Loss: 0.03301477157697082\n",
      "  Batch 9000/23979, Average Loss: 0.0331814337996766\n",
      "  Batch 10000/23979, Average Loss: 0.03268911205232143\n",
      "  Batch 11000/23979, Average Loss: 0.033344360747374596\n",
      "  Batch 12000/23979, Average Loss: 0.03258721610065549\n",
      "  Batch 13000/23979, Average Loss: 0.032824389370158316\n",
      "  Batch 14000/23979, Average Loss: 0.032833508015610274\n",
      "  Batch 15000/23979, Average Loss: 0.033009804654866456\n",
      "  Batch 16000/23979, Average Loss: 0.03257920871768147\n",
      "  Batch 17000/23979, Average Loss: 0.03309780218452215\n",
      "  Batch 18000/23979, Average Loss: 0.03255983690917492\n",
      "  Batch 19000/23979, Average Loss: 0.03227744699269533\n",
      "  Batch 20000/23979, Average Loss: 0.03271631145011634\n",
      "  Batch 21000/23979, Average Loss: 0.032057585488073526\n",
      "  Batch 22000/23979, Average Loss: 0.03220740888174623\n",
      "  Batch 23000/23979, Average Loss: 0.033353706858120856\n",
      "  Average Training Loss: 0.032952545003240685\n",
      "  Average Training Loss: 0.032952545003240685, Average Validation Loss: 0.03274969563410121\n",
      "Epoch 3/3: \n",
      "  Batch 1000/23979, Average Loss: 0.032338809826411304\n",
      "  Batch 2000/23979, Average Loss: 0.03209401687141508\n",
      "  Batch 3000/23979, Average Loss: 0.032125367601402104\n",
      "  Batch 4000/23979, Average Loss: 0.032038663735613225\n",
      "  Batch 5000/23979, Average Loss: 0.032086120712570845\n",
      "  Batch 6000/23979, Average Loss: 0.03274113953486085\n",
      "  Batch 7000/23979, Average Loss: 0.032633313710801304\n",
      "  Batch 8000/23979, Average Loss: 0.03206337532028556\n",
      "  Batch 9000/23979, Average Loss: 0.03285735668614507\n",
      "  Batch 10000/23979, Average Loss: 0.032363355855457486\n",
      "  Batch 11000/23979, Average Loss: 0.031850474246777595\n",
      "  Batch 12000/23979, Average Loss: 0.030991165491752327\n",
      "  Batch 13000/23979, Average Loss: 0.031161887002177535\n",
      "  Batch 14000/23979, Average Loss: 0.03167202827706933\n",
      "  Batch 15000/23979, Average Loss: 0.03195691183209419\n",
      "  Batch 16000/23979, Average Loss: 0.03174115565791726\n",
      "  Batch 17000/23979, Average Loss: 0.03175261709373444\n",
      "  Batch 18000/23979, Average Loss: 0.031913659968413416\n",
      "  Batch 19000/23979, Average Loss: 0.03230556571763009\n",
      "  Batch 20000/23979, Average Loss: 0.031214431814849376\n",
      "  Batch 21000/23979, Average Loss: 0.03161430671531707\n",
      "  Batch 22000/23979, Average Loss: 0.031905387454666194\n",
      "  Batch 23000/23979, Average Loss: 0.03186894468497485\n",
      "  Average Training Loss: 0.03195672341355549\n",
      "  Average Training Loss: 0.03195672341355549, Average Validation Loss: 0.03135193810294117\n",
      "  Average Test Loss: 0.06552133956828136\n",
      "Experiment C+B/C+B： 43/7  ---   Model: [2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.22154757564887403\n",
      "  Batch 2000/20625, Average Loss: 0.0980407787039876\n",
      "  Batch 3000/20625, Average Loss: 0.0826379166059196\n",
      "  Batch 4000/20625, Average Loss: 0.06357835122197866\n",
      "  Batch 5000/20625, Average Loss: 0.05290618001110852\n",
      "  Batch 6000/20625, Average Loss: 0.04726563701219857\n",
      "  Batch 7000/20625, Average Loss: 0.04438868476264179\n",
      "  Batch 8000/20625, Average Loss: 0.04392121733538806\n",
      "  Batch 9000/20625, Average Loss: 0.04384525426849723\n",
      "  Batch 10000/20625, Average Loss: 0.04415559770166874\n",
      "  Batch 11000/20625, Average Loss: 0.043673797151073813\n",
      "  Batch 12000/20625, Average Loss: 0.04358964050002396\n",
      "  Batch 13000/20625, Average Loss: 0.04384588751196861\n",
      "  Batch 14000/20625, Average Loss: 0.0444045904558152\n",
      "  Batch 15000/20625, Average Loss: 0.04329015707783401\n",
      "  Batch 16000/20625, Average Loss: 0.04280468303337693\n",
      "  Batch 17000/20625, Average Loss: 0.0432298317970708\n",
      "  Batch 18000/20625, Average Loss: 0.04337654006667435\n",
      "  Batch 19000/20625, Average Loss: 0.04442895366251469\n",
      "  Batch 20000/20625, Average Loss: 0.043796773014590146\n",
      "  Average Training Loss: 0.05847485963968616\n",
      "  Average Training Loss: 0.05847485963968616, Average Validation Loss: 0.04315248162613091\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.04302634294703603\n",
      "  Batch 2000/20625, Average Loss: 0.04431688874773681\n",
      "  Batch 3000/20625, Average Loss: 0.04466475629433989\n",
      "  Batch 4000/20625, Average Loss: 0.04337787268403918\n",
      "  Batch 5000/20625, Average Loss: 0.043352946171537045\n",
      "  Batch 6000/20625, Average Loss: 0.04307502013165504\n",
      "  Batch 7000/20625, Average Loss: 0.04338887819368392\n",
      "  Batch 8000/20625, Average Loss: 0.04314835735037923\n",
      "  Batch 9000/20625, Average Loss: 0.04249631279334426\n",
      "  Batch 10000/20625, Average Loss: 0.04277200672309846\n",
      "  Batch 11000/20625, Average Loss: 0.043211088865064085\n",
      "  Batch 12000/20625, Average Loss: 0.04357243078760803\n",
      "  Batch 13000/20625, Average Loss: 0.04326672156341374\n",
      "  Batch 14000/20625, Average Loss: 0.04294656814262271\n",
      "  Batch 15000/20625, Average Loss: 0.04360934272408486\n",
      "  Batch 16000/20625, Average Loss: 0.04323700338602066\n",
      "  Batch 17000/20625, Average Loss: 0.042464753489010035\n",
      "  Batch 18000/20625, Average Loss: 0.04379709451459348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 19000/20625, Average Loss: 0.043669491671025754\n",
      "  Batch 20000/20625, Average Loss: 0.04301424683723599\n",
      "  Average Training Loss: 0.043323711130790636\n",
      "  Average Training Loss: 0.043323711130790636, Average Validation Loss: 0.043103464450854764\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.042580855349078774\n",
      "  Batch 2000/20625, Average Loss: 0.0431285106241703\n",
      "  Batch 3000/20625, Average Loss: 0.04257579936273396\n",
      "  Batch 4000/20625, Average Loss: 0.04279322647489607\n",
      "  Batch 5000/20625, Average Loss: 0.042905341092497114\n",
      "  Batch 6000/20625, Average Loss: 0.04264820143952966\n",
      "  Batch 7000/20625, Average Loss: 0.04334626610018313\n",
      "  Batch 8000/20625, Average Loss: 0.043421597249805925\n",
      "  Batch 9000/20625, Average Loss: 0.04315423227101564\n",
      "  Batch 10000/20625, Average Loss: 0.04367880265600979\n",
      "  Batch 11000/20625, Average Loss: 0.0443336309241131\n",
      "  Batch 12000/20625, Average Loss: 0.04326146824564785\n",
      "  Batch 13000/20625, Average Loss: 0.04316909045074135\n",
      "  Batch 14000/20625, Average Loss: 0.042966503596864644\n",
      "  Batch 15000/20625, Average Loss: 0.0430937036536634\n",
      "  Batch 16000/20625, Average Loss: 0.04334040682949126\n",
      "  Batch 17000/20625, Average Loss: 0.04309453598409891\n",
      "  Batch 18000/20625, Average Loss: 0.043290105486288664\n",
      "  Batch 19000/20625, Average Loss: 0.04310301733016968\n",
      "  Batch 20000/20625, Average Loss: 0.04269657196477056\n",
      "  Average Training Loss: 0.04314322391658118\n",
      "  Average Training Loss: 0.04314322391658118, Average Validation Loss: 0.04297012035606942\n",
      "  Average Test Loss: 0.043236664982159716\n",
      "Experiment C+B/C+B： 44/7  ---   Model: [3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.11078485503047705\n",
      "  Batch 2000/20625, Average Loss: 0.07414241662621499\n",
      "  Batch 3000/20625, Average Loss: 0.05274285246804357\n",
      "  Batch 4000/20625, Average Loss: 0.047533055141568185\n",
      "  Batch 5000/20625, Average Loss: 0.04493485969491303\n",
      "  Batch 6000/20625, Average Loss: 0.0439094204287976\n",
      "  Batch 7000/20625, Average Loss: 0.04293153248634189\n",
      "  Batch 8000/20625, Average Loss: 0.04244411728903651\n",
      "  Batch 9000/20625, Average Loss: 0.04119886958040297\n",
      "  Batch 10000/20625, Average Loss: 0.041092276895418764\n",
      "  Batch 11000/20625, Average Loss: 0.04130565352365374\n",
      "  Batch 12000/20625, Average Loss: 0.04045070820488036\n",
      "  Batch 13000/20625, Average Loss: 0.04050625056400895\n",
      "  Batch 14000/20625, Average Loss: 0.04012909310311079\n",
      "  Batch 15000/20625, Average Loss: 0.040411481958813965\n",
      "  Batch 16000/20625, Average Loss: 0.040033686774782834\n",
      "  Batch 17000/20625, Average Loss: 0.04017867175489664\n",
      "  Batch 18000/20625, Average Loss: 0.04004968037083745\n",
      "  Batch 19000/20625, Average Loss: 0.04019378348905593\n",
      "  Batch 20000/20625, Average Loss: 0.03900243154168129\n",
      "  Average Training Loss: 0.046990387294870435\n",
      "  Average Training Loss: 0.046990387294870435, Average Validation Loss: 0.03955340991547985\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.04010644267126918\n",
      "  Batch 2000/20625, Average Loss: 0.04016256367042661\n",
      "  Batch 3000/20625, Average Loss: 0.040216210248880085\n",
      "  Batch 4000/20625, Average Loss: 0.03968879399355501\n",
      "  Batch 5000/20625, Average Loss: 0.03948890431877226\n",
      "  Batch 6000/20625, Average Loss: 0.039212062302045526\n",
      "  Batch 7000/20625, Average Loss: 0.03954640325717628\n",
      "  Batch 8000/20625, Average Loss: 0.03897283307183534\n",
      "  Batch 9000/20625, Average Loss: 0.03930077814683318\n",
      "  Batch 10000/20625, Average Loss: 0.04004669038299471\n",
      "  Batch 11000/20625, Average Loss: 0.03878218405041844\n",
      "  Batch 12000/20625, Average Loss: 0.039774355520494284\n",
      "  Batch 13000/20625, Average Loss: 0.038513907834887506\n",
      "  Batch 14000/20625, Average Loss: 0.03970753794256598\n",
      "  Batch 15000/20625, Average Loss: 0.03918213756009936\n",
      "  Batch 16000/20625, Average Loss: 0.03921518749929965\n",
      "  Batch 17000/20625, Average Loss: 0.03878663473855704\n",
      "  Batch 18000/20625, Average Loss: 0.03893472857028246\n",
      "  Batch 19000/20625, Average Loss: 0.03851738651841879\n",
      "  Batch 20000/20625, Average Loss: 0.03918388293217868\n",
      "  Average Training Loss: 0.039371268214195065\n",
      "  Average Training Loss: 0.039371268214195065, Average Validation Loss: 0.038926334394846517\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.03887510289810598\n",
      "  Batch 2000/20625, Average Loss: 0.038809633938595656\n",
      "  Batch 3000/20625, Average Loss: 0.03854983676970005\n",
      "  Batch 4000/20625, Average Loss: 0.0386543331053108\n",
      "  Batch 5000/20625, Average Loss: 0.03884378330083564\n",
      "  Batch 6000/20625, Average Loss: 0.03956013356335461\n",
      "  Batch 7000/20625, Average Loss: 0.039258235404267906\n",
      "  Batch 8000/20625, Average Loss: 0.03856701081991196\n",
      "  Batch 9000/20625, Average Loss: 0.03836508195847273\n",
      "  Batch 10000/20625, Average Loss: 0.03870213043969124\n",
      "  Batch 11000/20625, Average Loss: 0.038418267792090774\n",
      "  Batch 12000/20625, Average Loss: 0.03839950939733535\n",
      "  Batch 13000/20625, Average Loss: 0.037954294215887786\n",
      "  Batch 14000/20625, Average Loss: 0.038572631046175955\n",
      "  Batch 15000/20625, Average Loss: 0.03869041807297617\n",
      "  Batch 16000/20625, Average Loss: 0.03824014975596219\n",
      "  Batch 17000/20625, Average Loss: 0.038554341763257984\n",
      "  Batch 18000/20625, Average Loss: 0.03784460769314319\n",
      "  Batch 19000/20625, Average Loss: 0.03841036877874285\n",
      "  Batch 20000/20625, Average Loss: 0.037929939225316046\n",
      "  Average Training Loss: 0.03854442919305328\n",
      "  Average Training Loss: 0.03854442919305328, Average Validation Loss: 0.03829500540871932\n",
      "  Average Test Loss: 0.03853845621307668\n",
      "Experiment C+B/C+B： 45/7  ---   Model: [4]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.12398252444341779\n",
      "  Batch 2000/20625, Average Loss: 0.08192735725082458\n",
      "  Batch 3000/20625, Average Loss: 0.05538464348949492\n",
      "  Batch 4000/20625, Average Loss: 0.047903850018978116\n",
      "  Batch 5000/20625, Average Loss: 0.042877843480557204\n",
      "  Batch 6000/20625, Average Loss: 0.04135787793248892\n",
      "  Batch 7000/20625, Average Loss: 0.040621015951037405\n",
      "  Batch 8000/20625, Average Loss: 0.03997723847441375\n",
      "  Batch 9000/20625, Average Loss: 0.03998882315680385\n",
      "  Batch 10000/20625, Average Loss: 0.0402338654845953\n",
      "  Batch 11000/20625, Average Loss: 0.03996975512988865\n",
      "  Batch 12000/20625, Average Loss: 0.04000999628286809\n",
      "  Batch 13000/20625, Average Loss: 0.03946744762919843\n",
      "  Batch 14000/20625, Average Loss: 0.03965680255368352\n",
      "  Batch 15000/20625, Average Loss: 0.0397299588881433\n",
      "  Batch 16000/20625, Average Loss: 0.040016359739005566\n",
      "  Batch 17000/20625, Average Loss: 0.03974770940840244\n",
      "  Batch 18000/20625, Average Loss: 0.039325586626306176\n",
      "  Batch 19000/20625, Average Loss: 0.03839706776570529\n",
      "  Batch 20000/20625, Average Loss: 0.0393479959666729\n",
      "  Average Training Loss: 0.047230791735242715\n",
      "  Average Training Loss: 0.047230791735242715, Average Validation Loss: 0.03903654229733602\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.037859620071947576\n",
      "  Batch 2000/20625, Average Loss: 0.03881179410964251\n",
      "  Batch 3000/20625, Average Loss: 0.03861811885610223\n",
      "  Batch 4000/20625, Average Loss: 0.03858692313916981\n",
      "  Batch 5000/20625, Average Loss: 0.03825601154565811\n",
      "  Batch 6000/20625, Average Loss: 0.03793308214098215\n",
      "  Batch 7000/20625, Average Loss: 0.037325841589830815\n",
      "  Batch 8000/20625, Average Loss: 0.03733514814730734\n",
      "  Batch 9000/20625, Average Loss: 0.038187451888807115\n",
      "  Batch 10000/20625, Average Loss: 0.03755409822054207\n",
      "  Batch 11000/20625, Average Loss: 0.03802841577772051\n",
      "  Batch 12000/20625, Average Loss: 0.037424015460535885\n",
      "  Batch 13000/20625, Average Loss: 0.03778016485739499\n",
      "  Batch 14000/20625, Average Loss: 0.03673544682189822\n",
      "  Batch 15000/20625, Average Loss: 0.03750114608183503\n",
      "  Batch 16000/20625, Average Loss: 0.036790379909798505\n",
      "  Batch 17000/20625, Average Loss: 0.037038362157531084\n",
      "  Batch 18000/20625, Average Loss: 0.03675919304043054\n",
      "  Batch 19000/20625, Average Loss: 0.03722888137865812\n",
      "  Batch 20000/20625, Average Loss: 0.035675112733617426\n",
      "  Average Training Loss: 0.03754374993425427\n",
      "  Average Training Loss: 0.03754374993425427, Average Validation Loss: 0.03669462346536429\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.03701234898157418\n",
      "  Batch 2000/20625, Average Loss: 0.036695274639874695\n",
      "  Batch 3000/20625, Average Loss: 0.03650069910753519\n",
      "  Batch 4000/20625, Average Loss: 0.036680347418412564\n",
      "  Batch 5000/20625, Average Loss: 0.03614152971189469\n",
      "  Batch 6000/20625, Average Loss: 0.03656270016822964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7000/20625, Average Loss: 0.036294480519369246\n",
      "  Batch 8000/20625, Average Loss: 0.036564868147484955\n",
      "  Batch 9000/20625, Average Loss: 0.0367141454750672\n",
      "  Batch 10000/20625, Average Loss: 0.036792088793590666\n",
      "  Batch 11000/20625, Average Loss: 0.03645332701038569\n",
      "  Batch 12000/20625, Average Loss: 0.03709377660788596\n",
      "  Batch 13000/20625, Average Loss: 0.036328694012016056\n",
      "  Batch 14000/20625, Average Loss: 0.036592101598158476\n",
      "  Batch 15000/20625, Average Loss: 0.03599725155811757\n",
      "  Batch 16000/20625, Average Loss: 0.03672302733547986\n",
      "  Batch 17000/20625, Average Loss: 0.0367204764271155\n",
      "  Batch 18000/20625, Average Loss: 0.03634305093158036\n",
      "  Batch 19000/20625, Average Loss: 0.03664307665079832\n",
      "  Batch 20000/20625, Average Loss: 0.03665166331268847\n",
      "  Average Training Loss: 0.036582809804786336\n",
      "  Average Training Loss: 0.036582809804786336, Average Validation Loss: 0.0363455804192584\n",
      "  Average Test Loss: 0.036538223071763895\n",
      "Experiment C+B/C+B： 46/7  ---   Model: [4, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.13244184655696153\n",
      "  Batch 2000/20625, Average Loss: 0.10798890309408307\n",
      "  Batch 3000/20625, Average Loss: 0.07047863336466252\n",
      "  Batch 4000/20625, Average Loss: 0.04828321490809322\n",
      "  Batch 5000/20625, Average Loss: 0.044154952267184855\n",
      "  Batch 6000/20625, Average Loss: 0.04168481740076095\n",
      "  Batch 7000/20625, Average Loss: 0.041113263972103596\n",
      "  Batch 8000/20625, Average Loss: 0.03958096651080996\n",
      "  Batch 9000/20625, Average Loss: 0.03990477119199932\n",
      "  Batch 10000/20625, Average Loss: 0.03939422360062599\n",
      "  Batch 11000/20625, Average Loss: 0.03964315215777606\n",
      "  Batch 12000/20625, Average Loss: 0.03951321192830801\n",
      "  Batch 13000/20625, Average Loss: 0.03839333927258849\n",
      "  Batch 14000/20625, Average Loss: 0.0384215797316283\n",
      "  Batch 15000/20625, Average Loss: 0.03882271707803011\n",
      "  Batch 16000/20625, Average Loss: 0.03831313472613692\n",
      "  Batch 17000/20625, Average Loss: 0.0384220204167068\n",
      "  Batch 18000/20625, Average Loss: 0.037970099763013423\n",
      "  Batch 19000/20625, Average Loss: 0.03729507010150701\n",
      "  Batch 20000/20625, Average Loss: 0.03751155965402722\n",
      "  Average Training Loss: 0.04910064100442511\n",
      "  Average Training Loss: 0.04910064100442511, Average Validation Loss: 0.03727338682396931\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.0375303052989766\n",
      "  Batch 2000/20625, Average Loss: 0.03781711234897375\n",
      "  Batch 3000/20625, Average Loss: 0.03691407451312989\n",
      "  Batch 4000/20625, Average Loss: 0.03742989913467318\n",
      "  Batch 5000/20625, Average Loss: 0.036965866411104796\n",
      "  Batch 6000/20625, Average Loss: 0.03645868241880089\n",
      "  Batch 7000/20625, Average Loss: 0.036787886750884354\n",
      "  Batch 8000/20625, Average Loss: 0.03689879837073386\n",
      "  Batch 9000/20625, Average Loss: 0.03626186244469136\n",
      "  Batch 10000/20625, Average Loss: 0.036619155632331965\n",
      "  Batch 11000/20625, Average Loss: 0.036220212536863985\n",
      "  Batch 12000/20625, Average Loss: 0.036480534283444284\n",
      "  Batch 13000/20625, Average Loss: 0.037151928917504844\n",
      "  Batch 14000/20625, Average Loss: 0.036797339436598124\n",
      "  Batch 15000/20625, Average Loss: 0.03647914360370487\n",
      "  Batch 16000/20625, Average Loss: 0.03632882507052273\n",
      "  Batch 17000/20625, Average Loss: 0.03641325278300792\n",
      "  Batch 18000/20625, Average Loss: 0.036166334341280165\n",
      "  Batch 19000/20625, Average Loss: 0.035838838476687666\n",
      "  Batch 20000/20625, Average Loss: 0.0361872857902199\n",
      "  Average Training Loss: 0.036689356325521616\n",
      "  Average Training Loss: 0.036689356325521616, Average Validation Loss: 0.03593977611515684\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.03626725685968995\n",
      "  Batch 2000/20625, Average Loss: 0.0361419778438285\n",
      "  Batch 3000/20625, Average Loss: 0.03629438301548362\n",
      "  Batch 4000/20625, Average Loss: 0.03622815215773881\n",
      "  Batch 5000/20625, Average Loss: 0.03675950953457505\n",
      "  Batch 6000/20625, Average Loss: 0.03610246320907026\n",
      "  Batch 7000/20625, Average Loss: 0.035408367964439094\n",
      "  Batch 8000/20625, Average Loss: 0.03612473727948964\n",
      "  Batch 9000/20625, Average Loss: 0.03599967009667307\n",
      "  Batch 10000/20625, Average Loss: 0.03607090527750552\n",
      "  Batch 11000/20625, Average Loss: 0.03573314085509628\n",
      "  Batch 12000/20625, Average Loss: 0.03648726513516158\n",
      "  Batch 13000/20625, Average Loss: 0.03563480123225599\n",
      "  Batch 14000/20625, Average Loss: 0.0357752930931747\n",
      "  Batch 15000/20625, Average Loss: 0.03614461085665971\n",
      "  Batch 16000/20625, Average Loss: 0.035689187416806815\n",
      "  Batch 17000/20625, Average Loss: 0.03575816283468157\n",
      "  Batch 18000/20625, Average Loss: 0.036232968406751755\n",
      "  Batch 19000/20625, Average Loss: 0.03608735883887857\n",
      "  Batch 20000/20625, Average Loss: 0.03605512504093349\n",
      "  Average Training Loss: 0.036019408977709035\n",
      "  Average Training Loss: 0.036019408977709035, Average Validation Loss: 0.035771221468828364\n",
      "  Average Test Loss: 0.036090551891965715\n",
      "Experiment C+B/C+B： 47/7  ---   Model: [5, 2]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.11011258295550942\n",
      "  Batch 2000/20625, Average Loss: 0.0659273342937231\n",
      "  Batch 3000/20625, Average Loss: 0.04720994309615344\n",
      "  Batch 4000/20625, Average Loss: 0.04255847856961191\n",
      "  Batch 5000/20625, Average Loss: 0.04224912958405912\n",
      "  Batch 6000/20625, Average Loss: 0.040791796997189524\n",
      "  Batch 7000/20625, Average Loss: 0.04049395613186062\n",
      "  Batch 8000/20625, Average Loss: 0.04014009118452668\n",
      "  Batch 9000/20625, Average Loss: 0.03928910982981324\n",
      "  Batch 10000/20625, Average Loss: 0.03964842700958252\n",
      "  Batch 11000/20625, Average Loss: 0.038811250341124835\n",
      "  Batch 12000/20625, Average Loss: 0.03840459429938346\n",
      "  Batch 13000/20625, Average Loss: 0.03779621028434485\n",
      "  Batch 14000/20625, Average Loss: 0.03753478175401687\n",
      "  Batch 15000/20625, Average Loss: 0.03839107125811279\n",
      "  Batch 16000/20625, Average Loss: 0.03811484674643725\n",
      "  Batch 17000/20625, Average Loss: 0.037755483333952726\n",
      "  Batch 18000/20625, Average Loss: 0.03742988611012697\n",
      "  Batch 19000/20625, Average Loss: 0.03702963685151189\n",
      "  Batch 20000/20625, Average Loss: 0.03729014363791794\n",
      "  Average Training Loss: 0.04412252311101465\n",
      "  Average Training Loss: 0.04412252311101465, Average Validation Loss: 0.03688948293610381\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.037079466754570606\n",
      "  Batch 2000/20625, Average Loss: 0.036521404521539805\n",
      "  Batch 3000/20625, Average Loss: 0.03664104886073619\n",
      "  Batch 4000/20625, Average Loss: 0.03579863474518061\n",
      "  Batch 5000/20625, Average Loss: 0.03664231751020998\n",
      "  Batch 6000/20625, Average Loss: 0.0358526511117816\n",
      "  Batch 7000/20625, Average Loss: 0.03680056896340102\n",
      "  Batch 8000/20625, Average Loss: 0.03554277487937361\n",
      "  Batch 9000/20625, Average Loss: 0.03613891339208931\n",
      "  Batch 10000/20625, Average Loss: 0.03601892975158989\n",
      "  Batch 11000/20625, Average Loss: 0.03595464276243001\n",
      "  Batch 12000/20625, Average Loss: 0.03607806883938611\n",
      "  Batch 13000/20625, Average Loss: 0.036502970413304865\n",
      "  Batch 14000/20625, Average Loss: 0.035611909409984946\n",
      "  Batch 15000/20625, Average Loss: 0.034997700430452826\n",
      "  Batch 16000/20625, Average Loss: 0.03494847429264337\n",
      "  Batch 17000/20625, Average Loss: 0.035695813515223565\n",
      "  Batch 18000/20625, Average Loss: 0.03516986490506679\n",
      "  Batch 19000/20625, Average Loss: 0.03565512378327548\n",
      "  Batch 20000/20625, Average Loss: 0.03499805413372815\n",
      "  Average Training Loss: 0.0359108658235633\n",
      "  Average Training Loss: 0.0359108658235633, Average Validation Loss: 0.0351254551698373\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.03504335492942482\n",
      "  Batch 2000/20625, Average Loss: 0.03554683486837894\n",
      "  Batch 3000/20625, Average Loss: 0.034822293642908335\n",
      "  Batch 4000/20625, Average Loss: 0.0351199190383777\n",
      "  Batch 5000/20625, Average Loss: 0.034615319272503255\n",
      "  Batch 6000/20625, Average Loss: 0.0349833971709013\n",
      "  Batch 7000/20625, Average Loss: 0.03530900847353041\n",
      "  Batch 8000/20625, Average Loss: 0.03468593149632215\n",
      "  Batch 9000/20625, Average Loss: 0.03469051769468933\n",
      "  Batch 10000/20625, Average Loss: 0.034781168068759145\n",
      "  Batch 11000/20625, Average Loss: 0.0349966931194067\n",
      "  Batch 12000/20625, Average Loss: 0.03419094018917531\n",
      "  Batch 13000/20625, Average Loss: 0.03418777672573924\n",
      "  Batch 14000/20625, Average Loss: 0.03517757248133421\n",
      "  Batch 15000/20625, Average Loss: 0.034546401171013714\n",
      "  Batch 16000/20625, Average Loss: 0.0338564777225256\n",
      "  Batch 17000/20625, Average Loss: 0.03484482287429273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 18000/20625, Average Loss: 0.03465653035510331\n",
      "  Batch 19000/20625, Average Loss: 0.034914357982575896\n",
      "  Batch 20000/20625, Average Loss: 0.03444431406445801\n",
      "  Average Training Loss: 0.03476454672962427\n",
      "  Average Training Loss: 0.03476454672962427, Average Validation Loss: 0.03424871057624526\n",
      "  Average Test Loss: 0.03448215434813519\n",
      "Experiment C+B/C+B： 48/7  ---   Model: [5, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.17229741276055574\n",
      "  Batch 2000/20625, Average Loss: 0.10669457444548607\n",
      "  Batch 3000/20625, Average Loss: 0.09561133842915297\n",
      "  Batch 4000/20625, Average Loss: 0.05989964560605585\n",
      "  Batch 5000/20625, Average Loss: 0.04852706408314407\n",
      "  Batch 6000/20625, Average Loss: 0.046154485516250135\n",
      "  Batch 7000/20625, Average Loss: 0.044259477213956414\n",
      "  Batch 8000/20625, Average Loss: 0.043279692424461245\n",
      "  Batch 9000/20625, Average Loss: 0.0431550631262362\n",
      "  Batch 10000/20625, Average Loss: 0.04205581468623131\n",
      "  Batch 11000/20625, Average Loss: 0.04208642423339188\n",
      "  Batch 12000/20625, Average Loss: 0.04177656072564423\n",
      "  Batch 13000/20625, Average Loss: 0.041390083480626345\n",
      "  Batch 14000/20625, Average Loss: 0.04173955841176212\n",
      "  Batch 15000/20625, Average Loss: 0.04094251378066838\n",
      "  Batch 16000/20625, Average Loss: 0.04144921636208892\n",
      "  Batch 17000/20625, Average Loss: 0.040715636929497125\n",
      "  Batch 18000/20625, Average Loss: 0.04051424071658403\n",
      "  Batch 19000/20625, Average Loss: 0.040580225964076816\n",
      "  Batch 20000/20625, Average Loss: 0.04024210486933589\n",
      "  Average Training Loss: 0.055199469204111534\n",
      "  Average Training Loss: 0.055199469204111534, Average Validation Loss: 0.039941789232006024\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.03905315628275275\n",
      "  Batch 2000/20625, Average Loss: 0.03918367870338261\n",
      "  Batch 3000/20625, Average Loss: 0.03975882692821324\n",
      "  Batch 4000/20625, Average Loss: 0.03876938826404512\n",
      "  Batch 5000/20625, Average Loss: 0.03912078268174082\n",
      "  Batch 6000/20625, Average Loss: 0.03841443285252899\n",
      "  Batch 7000/20625, Average Loss: 0.037894465805031356\n",
      "  Batch 8000/20625, Average Loss: 0.0376324665825814\n",
      "  Batch 9000/20625, Average Loss: 0.03649815131723881\n",
      "  Batch 10000/20625, Average Loss: 0.03729817380616442\n",
      "  Batch 11000/20625, Average Loss: 0.036337846033275126\n",
      "  Batch 12000/20625, Average Loss: 0.03579563838988543\n",
      "  Batch 13000/20625, Average Loss: 0.03616870533209294\n",
      "  Batch 14000/20625, Average Loss: 0.03593933265004307\n",
      "  Batch 15000/20625, Average Loss: 0.034975824823603036\n",
      "  Batch 16000/20625, Average Loss: 0.03575897484924644\n",
      "  Batch 17000/20625, Average Loss: 0.035448155276477336\n",
      "  Batch 18000/20625, Average Loss: 0.03527888629678637\n",
      "  Batch 19000/20625, Average Loss: 0.035243190051056444\n",
      "  Batch 20000/20625, Average Loss: 0.03462812711764127\n",
      "  Average Training Loss: 0.036890405690557126\n",
      "  Average Training Loss: 0.036890405690557126, Average Validation Loss: 0.03527299847033105\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.034449193086475136\n",
      "  Batch 2000/20625, Average Loss: 0.034618835248984396\n",
      "  Batch 3000/20625, Average Loss: 0.035273743018507955\n",
      "  Batch 4000/20625, Average Loss: 0.03411722001153976\n",
      "  Batch 5000/20625, Average Loss: 0.03358341389428824\n",
      "  Batch 6000/20625, Average Loss: 0.034674356604926286\n",
      "  Batch 7000/20625, Average Loss: 0.0345018282327801\n",
      "  Batch 8000/20625, Average Loss: 0.03415492403134704\n",
      "  Batch 9000/20625, Average Loss: 0.03361401327420026\n",
      "  Batch 10000/20625, Average Loss: 0.03430191028304398\n",
      "  Batch 11000/20625, Average Loss: 0.03419176874961704\n",
      "  Batch 12000/20625, Average Loss: 0.03341833887062967\n",
      "  Batch 13000/20625, Average Loss: 0.03392228822316974\n",
      "  Batch 14000/20625, Average Loss: 0.03370008158311248\n",
      "  Batch 15000/20625, Average Loss: 0.03411275367345661\n",
      "  Batch 16000/20625, Average Loss: 0.033623637268319725\n",
      "  Batch 17000/20625, Average Loss: 0.033808382492512466\n",
      "  Batch 18000/20625, Average Loss: 0.033684876712970435\n",
      "  Batch 19000/20625, Average Loss: 0.03360326771065593\n",
      "  Batch 20000/20625, Average Loss: 0.033944899486377834\n",
      "  Average Training Loss: 0.03403602684054411\n",
      "  Average Training Loss: 0.03403602684054411, Average Validation Loss: 0.033370608510911864\n",
      "  Average Test Loss: 0.033708733659824915\n",
      "Experiment C+B/C+B： 49/7  ---   Model: [6, 3]\n",
      "Epoch 1/3: \n",
      "  Batch 1000/20625, Average Loss: 0.11110575552657247\n",
      "  Batch 2000/20625, Average Loss: 0.06336257111094892\n",
      "  Batch 3000/20625, Average Loss: 0.04705178808234632\n",
      "  Batch 4000/20625, Average Loss: 0.04342929627932608\n",
      "  Batch 5000/20625, Average Loss: 0.04263839045260102\n",
      "  Batch 6000/20625, Average Loss: 0.042122619708068666\n",
      "  Batch 7000/20625, Average Loss: 0.04100714035145939\n",
      "  Batch 8000/20625, Average Loss: 0.040442766812629996\n",
      "  Batch 9000/20625, Average Loss: 0.03978502961155027\n",
      "  Batch 10000/20625, Average Loss: 0.0383618753394112\n",
      "  Batch 11000/20625, Average Loss: 0.038368341695517304\n",
      "  Batch 12000/20625, Average Loss: 0.03704051027167588\n",
      "  Batch 13000/20625, Average Loss: 0.037029301667585966\n",
      "  Batch 14000/20625, Average Loss: 0.03727446867246181\n",
      "  Batch 15000/20625, Average Loss: 0.036839617107994854\n",
      "  Batch 16000/20625, Average Loss: 0.03502421472314745\n",
      "  Batch 17000/20625, Average Loss: 0.03582557418290526\n",
      "  Batch 18000/20625, Average Loss: 0.03552086272090674\n",
      "  Batch 19000/20625, Average Loss: 0.035781181421130896\n",
      "  Batch 20000/20625, Average Loss: 0.0355975244184956\n",
      "  Average Training Loss: 0.04342672180384397\n",
      "  Average Training Loss: 0.04342672180384397, Average Validation Loss: 0.035209626803886035\n",
      "Epoch 2/3: \n",
      "  Batch 1000/20625, Average Loss: 0.0352446024287492\n",
      "  Batch 2000/20625, Average Loss: 0.035327737220562996\n",
      "  Batch 3000/20625, Average Loss: 0.03495525324717164\n",
      "  Batch 4000/20625, Average Loss: 0.035312490448355674\n",
      "  Batch 5000/20625, Average Loss: 0.03457090418040752\n",
      "  Batch 6000/20625, Average Loss: 0.035011930239386854\n",
      "  Batch 7000/20625, Average Loss: 0.0348870647707954\n",
      "  Batch 8000/20625, Average Loss: 0.03510657392069697\n",
      "  Batch 9000/20625, Average Loss: 0.034906475354917345\n",
      "  Batch 10000/20625, Average Loss: 0.03515275788493454\n",
      "  Batch 11000/20625, Average Loss: 0.03499514484591782\n",
      "  Batch 12000/20625, Average Loss: 0.034757471026852726\n",
      "  Batch 13000/20625, Average Loss: 0.034409500355832276\n",
      "  Batch 14000/20625, Average Loss: 0.03460285616479814\n",
      "  Batch 15000/20625, Average Loss: 0.03406677894294262\n",
      "  Batch 16000/20625, Average Loss: 0.0338015516391024\n",
      "  Batch 17000/20625, Average Loss: 0.03455197696853429\n",
      "  Batch 18000/20625, Average Loss: 0.03413616924080998\n",
      "  Batch 19000/20625, Average Loss: 0.034242628577165306\n",
      "  Batch 20000/20625, Average Loss: 0.03420449404511601\n",
      "  Average Training Loss: 0.03471376488366813\n",
      "  Average Training Loss: 0.03471376488366813, Average Validation Loss: 0.03442793814521751\n",
      "Epoch 3/3: \n",
      "  Batch 1000/20625, Average Loss: 0.03423338732123375\n",
      "  Batch 2000/20625, Average Loss: 0.03364052478969097\n",
      "  Batch 3000/20625, Average Loss: 0.03405108662787825\n",
      "  Batch 4000/20625, Average Loss: 0.03384466570150107\n",
      "  Batch 5000/20625, Average Loss: 0.03433138644509017\n",
      "  Batch 6000/20625, Average Loss: 0.03403115401323885\n",
      "  Batch 7000/20625, Average Loss: 0.0335225655278191\n",
      "  Batch 8000/20625, Average Loss: 0.033877453955821694\n",
      "  Batch 9000/20625, Average Loss: 0.03308767888601869\n",
      "  Batch 10000/20625, Average Loss: 0.03313533944822848\n",
      "  Batch 11000/20625, Average Loss: 0.0333748771562241\n",
      "  Batch 12000/20625, Average Loss: 0.03338968473207206\n",
      "  Batch 13000/20625, Average Loss: 0.0332044728519395\n",
      "  Batch 14000/20625, Average Loss: 0.032219653040170666\n",
      "  Batch 15000/20625, Average Loss: 0.03274615857470781\n",
      "  Batch 16000/20625, Average Loss: 0.03282371754571795\n",
      "  Batch 17000/20625, Average Loss: 0.032956710042431954\n",
      "  Batch 18000/20625, Average Loss: 0.03303277085255831\n",
      "  Batch 19000/20625, Average Loss: 0.03248953761626035\n",
      "  Batch 20000/20625, Average Loss: 0.03226321124471724\n",
      "  Average Training Loss: 0.03329840136014602\n",
      "  Average Training Loss: 0.03329840136014602, Average Validation Loss: 0.032469528494980686\n",
      "  Average Test Loss: 0.032704915306593196\n"
     ]
    }
   ],
   "source": [
    "result = experiment(datasetC, datasetB, hidden_layers_structure = [[2],[3],[4],[4,2],[5,2],[5,3],[6,3]], num_epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5393031e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp_name': ['C/C', 'C/C', 'C/C', 'C/C', 'C/C', 'C/C', 'C/C', 'B/B', 'B/B', 'B/B', 'B/B', 'B/B', 'B/B', 'B/B', 'C/B', 'C/B', 'C/B', 'C/B', 'C/B', 'C/B', 'C/B', 'B/C', 'B/C', 'B/C', 'B/C', 'B/C', 'B/C', 'B/C', 'C+B/C', 'C+B/C', 'C+B/C', 'C+B/C', 'C+B/C', 'C+B/C', 'C+B/C', 'C+B/B', 'C+B/B', 'C+B/B', 'C+B/B', 'C+B/B', 'C+B/B', 'C+B/B', 'C+B/C+B', 'C+B/C+B', 'C+B/C+B', 'C+B/C+B', 'C+B/C+B', 'C+B/C+B', 'C+B/C+B'], 'architecture': ['[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]', '[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]', '[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]', '[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]', '[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]', '[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]', '[2]', '[3]', '[4]', '[4, 2]', '[5, 2]', '[5, 3]', '[6, 3]'], 'train_loss': [0.030612140179388462, 0.026990445971233002, 0.024431249014975558, 0.023240425415508033, 0.02181847615918161, 0.022267605154040727, 0.022270072811582154, 0.0347321483042544, 0.025234313875779533, 0.026046276095437255, 0.03298174546252964, 0.026319456646152206, 0.028967753095361565, 0.02508365682796959, 0.030048184207153453, 0.027012814079315446, 0.024857118884741042, 0.02326118954224983, 0.02301661289932054, 0.02094030239217889, 0.022162453743161423, 0.033797165637310105, 0.026298940947262275, 0.02546796431488898, 0.032584378081641406, 0.02770771121793891, 0.02770506240269301, 0.02613888710836487, 0.04579887277896583, 0.041202168327178885, 0.03834824640867299, 0.036733842774253775, 0.035440235573966215, 0.035703523866723395, 0.03591459114433907, 0.04008057073009264, 0.03732527251184419, 0.03431667222139724, 0.032923321662569116, 0.03298813719996082, 0.03298157288028083, 0.03195672341355549, 0.04314322391658118, 0.03854442919305328, 0.036582809804786336, 0.036019408977709035, 0.03476454672962427, 0.03403602684054411, 0.03329840136014602], 'test_loss': [0.030458654015003896, 0.02738049515173182, 0.024443606376959647, 0.023191262999783985, 0.02155223697990356, 0.022076903239713702, 0.022547710732711314, 0.03443979244248059, 0.024505708842762403, 0.025336115982392795, 0.031452917292261254, 0.02540742519228633, 0.027989941381606633, 0.024047514765704055, 0.10526126922024356, 0.1008991718544361, 0.10639827019128528, 0.09921212825178331, 0.09909194441126472, 0.09850819385281717, 0.10178330658976949, 0.10421460702151443, 0.10507671134497422, 0.10257598152794818, 0.10272900219398255, 0.10751692344450382, 0.1025989491913338, 0.10929894171397858, 0.03751637967566617, 0.033896614544363865, 0.030370710190888415, 0.02880798435497209, 0.028690343103200767, 0.03126645033403994, 0.029246490329409576, 0.07849840364678635, 0.07117307037743699, 0.07269165218273446, 0.06576841797094071, 0.07100499476112597, 0.06451808751707973, 0.06552133956828136, 0.043236664982159716, 0.03853845621307668, 0.036538223071763895, 0.036090551891965715, 0.03448215434813519, 0.033708733659824915, 0.032704915306593196]}\n"
     ]
    }
   ],
   "source": [
    "with open('result.json', 'r') as file:\n",
    "    result = json.load(file)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c75219f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = ['C/C','B/B','C/B','B/C','B+C/C','B+C/B','B+C/B+C']\n",
    "architecture = [[2], [3], [4, 2], [5, 2], [5, 3], [6, 3]]\n",
    "\n",
    "# Create a 2x3 subplot layout\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 6))\n",
    "\n",
    "# Iterate through each architecture value\n",
    "for i, arch_value in enumerate(architecture[:2] + architecture[2:]):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    # Extract relevant data for the current architecture value\n",
    "    for index, value in enumerate(result['architecture']):\n",
    "        if value == str(arch_value):\n",
    "            train_losses.append(result['train_loss'][index])\n",
    "            test_losses.append(result['test_loss'][index])\n",
    "\n",
    "    # Plot the data on the corresponding subplot\n",
    "    row, col = divmod(i, 3)\n",
    "    axes[row, col].plot(exp_name, train_losses, linewidth=5, label='Train Loss', color=(88/255, 88/255, 162/255))\n",
    "    axes[row, col].plot(exp_name, test_losses, linewidth=5, label='Test Loss', color=(197/255, 197/255, 197/255))\n",
    "\n",
    "    # Set title and axis labels for each subplot\n",
    "    axes[row, col].set_title(str(tuple(arch_value)))\n",
    "    axes[row, col].set_ylabel('MSE')\n",
    "\n",
    "# Adjust layout to ensure proper spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9c992cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment C+B/C+B： 1/1  ---   Model: [256, 128, 64, 32]\n",
      "Epoch 1/200: \n",
      "  Batch 1000/20625, Average Loss: 0.07711996770463884\n",
      "  Batch 2000/20625, Average Loss: 0.04310298200137913\n",
      "  Batch 3000/20625, Average Loss: 0.03959714014828205\n",
      "  Batch 4000/20625, Average Loss: 0.038109377106651667\n",
      "  Batch 5000/20625, Average Loss: 0.03769437531474978\n",
      "  Batch 6000/20625, Average Loss: 0.03556003748439252\n",
      "  Batch 7000/20625, Average Loss: 0.0341432155482471\n",
      "  Batch 8000/20625, Average Loss: 0.03421982720028609\n",
      "  Batch 9000/20625, Average Loss: 0.03315283344686031\n",
      "  Batch 10000/20625, Average Loss: 0.03296886894199997\n",
      "  Batch 11000/20625, Average Loss: 0.03196925923600793\n",
      "  Batch 12000/20625, Average Loss: 0.032533905200660226\n",
      "  Batch 13000/20625, Average Loss: 0.03189970318414271\n",
      "  Batch 14000/20625, Average Loss: 0.0313164958874695\n",
      "  Batch 15000/20625, Average Loss: 0.031042170142754913\n",
      "  Batch 16000/20625, Average Loss: 0.031127173809334635\n",
      "  Batch 17000/20625, Average Loss: 0.029759315279312432\n",
      "  Batch 18000/20625, Average Loss: 0.030294442291371525\n",
      "  Batch 19000/20625, Average Loss: 0.02974632754921913\n",
      "  Batch 20000/20625, Average Loss: 0.02927676027826965\n",
      "  Average Training Loss: 0.03555365064223156\n",
      "  Average Training Loss: 0.03555365064223156, Average Validation Loss: 0.02941540112499907\n",
      "  Average Test Loss: 0.029786972932023422\n",
      "Epoch 2/200: \n",
      "  Batch 1000/20625, Average Loss: 0.02906706984248012\n",
      "  Batch 2000/20625, Average Loss: 0.029653276084922253\n",
      "  Batch 3000/20625, Average Loss: 0.02834865253791213\n",
      "  Batch 4000/20625, Average Loss: 0.028688787093386053\n",
      "  Batch 5000/20625, Average Loss: 0.028728113679215312\n",
      "  Batch 6000/20625, Average Loss: 0.028538724079728125\n",
      "  Batch 7000/20625, Average Loss: 0.02843825525511056\n",
      "  Batch 8000/20625, Average Loss: 0.02776082997303456\n",
      "  Batch 9000/20625, Average Loss: 0.027641749639064073\n",
      "  Batch 10000/20625, Average Loss: 0.027643847062252463\n",
      "  Batch 11000/20625, Average Loss: 0.02767275922745466\n",
      "  Batch 12000/20625, Average Loss: 0.02718445709720254\n",
      "  Batch 13000/20625, Average Loss: 0.026889770884532482\n",
      "  Batch 14000/20625, Average Loss: 0.026659988191910088\n",
      "  Batch 15000/20625, Average Loss: 0.02660302436724305\n",
      "  Batch 16000/20625, Average Loss: 0.026356373724527656\n",
      "  Batch 17000/20625, Average Loss: 0.025839678100310265\n",
      "  Batch 18000/20625, Average Loss: 0.026516979644075036\n",
      "  Batch 19000/20625, Average Loss: 0.02607749335747212\n",
      "  Batch 20000/20625, Average Loss: 0.025435547002591194\n",
      "  Average Training Loss: 0.02743511195058624\n",
      "  Average Training Loss: 0.02743511195058624, Average Validation Loss: 0.02668813812115735\n",
      "  Average Test Loss: 0.026828855072759093\n",
      "Epoch 3/200: \n",
      "  Batch 1000/20625, Average Loss: 0.02548473376734182\n",
      "  Batch 2000/20625, Average Loss: 0.02586361621133983\n",
      "  Batch 3000/20625, Average Loss: 0.024863638924434783\n",
      "  Batch 4000/20625, Average Loss: 0.025485329669900237\n",
      "  Batch 5000/20625, Average Loss: 0.02495165599882603\n",
      "  Batch 6000/20625, Average Loss: 0.024804049785248937\n",
      "  Batch 7000/20625, Average Loss: 0.024776984222233296\n",
      "  Batch 8000/20625, Average Loss: 0.02449211434321478\n",
      "  Batch 9000/20625, Average Loss: 0.024341616523917763\n",
      "  Batch 10000/20625, Average Loss: 0.02473048989614472\n",
      "  Batch 11000/20625, Average Loss: 0.024517267177812756\n",
      "  Batch 12000/20625, Average Loss: 0.02447187884384766\n",
      "  Batch 13000/20625, Average Loss: 0.02483825001632795\n",
      "  Batch 14000/20625, Average Loss: 0.023830785463098438\n",
      "  Batch 15000/20625, Average Loss: 0.024067394049838185\n",
      "  Batch 16000/20625, Average Loss: 0.024565924547147004\n",
      "  Batch 17000/20625, Average Loss: 0.023170531921088696\n",
      "  Batch 18000/20625, Average Loss: 0.023554331136401742\n",
      "  Batch 19000/20625, Average Loss: 0.023597037978470327\n",
      "  Batch 20000/20625, Average Loss: 0.023844125603791327\n",
      "  Average Training Loss: 0.02447333563801014\n",
      "  Average Training Loss: 0.02447333563801014, Average Validation Loss: 0.023223757583967556\n",
      "  Average Test Loss: 0.023395432868767628\n",
      "Epoch 4/200: \n",
      "  Batch 1000/20625, Average Loss: 0.023193355655763297\n",
      "  Batch 2000/20625, Average Loss: 0.02259174496214837\n",
      "  Batch 3000/20625, Average Loss: 0.02328239024337381\n",
      "  Batch 4000/20625, Average Loss: 0.02332149107987061\n",
      "  Batch 5000/20625, Average Loss: 0.022911200891248883\n",
      "  Batch 6000/20625, Average Loss: 0.022790170202497392\n",
      "  Batch 7000/20625, Average Loss: 0.022311030708253382\n",
      "  Batch 8000/20625, Average Loss: 0.02249169543152675\n",
      "  Batch 9000/20625, Average Loss: 0.022582690544892103\n",
      "  Batch 10000/20625, Average Loss: 0.02249866505805403\n",
      "  Batch 11000/20625, Average Loss: 0.02253469538828358\n",
      "  Batch 12000/20625, Average Loss: 0.021907231226563455\n",
      "  Batch 13000/20625, Average Loss: 0.02195048584649339\n",
      "  Batch 14000/20625, Average Loss: 0.022226775582879783\n",
      "  Batch 15000/20625, Average Loss: 0.022351864967029542\n",
      "  Batch 16000/20625, Average Loss: 0.02195563238952309\n",
      "  Batch 17000/20625, Average Loss: 0.02169489005068317\n",
      "  Batch 18000/20625, Average Loss: 0.02144290965376422\n",
      "  Batch 19000/20625, Average Loss: 0.02143361691245809\n",
      "  Batch 20000/20625, Average Loss: 0.021779326203279197\n",
      "  Average Training Loss: 0.022338573922013695\n",
      "  Average Training Loss: 0.022338573922013695, Average Validation Loss: 0.02101533231047322\n",
      "  Average Test Loss: 0.02112187600348037\n",
      "Epoch 5/200: \n",
      "  Batch 1000/20625, Average Loss: 0.021258567543700337\n",
      "  Batch 2000/20625, Average Loss: 0.020826376913581045\n",
      "  Batch 3000/20625, Average Loss: 0.02110708633624017\n",
      "  Batch 4000/20625, Average Loss: 0.021044774781446902\n",
      "  Batch 5000/20625, Average Loss: 0.020856485970318316\n",
      "  Batch 6000/20625, Average Loss: 0.020895276830997317\n",
      "  Batch 7000/20625, Average Loss: 0.020743040842935444\n",
      "  Batch 8000/20625, Average Loss: 0.02062480718875304\n",
      "  Batch 9000/20625, Average Loss: 0.020152490188833326\n",
      "  Batch 10000/20625, Average Loss: 0.020124646376352757\n",
      "  Batch 11000/20625, Average Loss: 0.02015550494287163\n",
      "  Batch 12000/20625, Average Loss: 0.020454548473469912\n",
      "  Batch 13000/20625, Average Loss: 0.02036159995337948\n",
      "  Batch 14000/20625, Average Loss: 0.019981638766359538\n",
      "  Batch 15000/20625, Average Loss: 0.01975955515168607\n",
      "  Batch 16000/20625, Average Loss: 0.020047948841936888\n",
      "  Batch 17000/20625, Average Loss: 0.01969468631548807\n",
      "  Batch 18000/20625, Average Loss: 0.019958464544266464\n",
      "  Batch 19000/20625, Average Loss: 0.019343071163166314\n",
      "  Batch 20000/20625, Average Loss: 0.01939873019885272\n",
      "  Average Training Loss: 0.020317899952693418\n",
      "  Average Training Loss: 0.020317899952693418, Average Validation Loss: 0.020134160801678146\n",
      "  Average Test Loss: 0.020211607901878266\n",
      "Epoch 6/200: \n",
      "  Batch 1000/20625, Average Loss: 0.019078220753930508\n",
      "  Batch 2000/20625, Average Loss: 0.01954838919546455\n",
      "  Batch 3000/20625, Average Loss: 0.018831071396358312\n",
      "  Batch 4000/20625, Average Loss: 0.01913707237225026\n",
      "  Batch 5000/20625, Average Loss: 0.01898324073944241\n",
      "  Batch 6000/20625, Average Loss: 0.01885475408937782\n",
      "  Batch 7000/20625, Average Loss: 0.018932274100836366\n",
      "  Batch 8000/20625, Average Loss: 0.01889258451620117\n",
      "  Batch 9000/20625, Average Loss: 0.01886516399960965\n",
      "  Batch 10000/20625, Average Loss: 0.018689108641352505\n",
      "  Batch 11000/20625, Average Loss: 0.01857844412466511\n",
      "  Batch 12000/20625, Average Loss: 0.018419731579255312\n",
      "  Batch 13000/20625, Average Loss: 0.01847660174360499\n",
      "  Batch 14000/20625, Average Loss: 0.01833919927245006\n",
      "  Batch 15000/20625, Average Loss: 0.017859202441293746\n",
      "  Batch 16000/20625, Average Loss: 0.018294280961621553\n",
      "  Batch 17000/20625, Average Loss: 0.017760047806892543\n",
      "  Batch 18000/20625, Average Loss: 0.018153900747187434\n",
      "  Batch 19000/20625, Average Loss: 0.017457200545817612\n",
      "  Batch 20000/20625, Average Loss: 0.017890402427408844\n",
      "  Average Training Loss: 0.018532063044201245\n",
      "  Average Training Loss: 0.018532063044201245, Average Validation Loss: 0.018143961791731035\n",
      "  Average Test Loss: 0.018131758445945733\n",
      "Epoch 7/200: \n",
      "  Batch 1000/20625, Average Loss: 0.017542598854750395\n",
      "  Batch 2000/20625, Average Loss: 0.017506368290632963\n",
      "  Batch 3000/20625, Average Loss: 0.01775077806506306\n",
      "  Batch 4000/20625, Average Loss: 0.017152293529827146\n",
      "  Batch 5000/20625, Average Loss: 0.017371861106250434\n",
      "  Batch 6000/20625, Average Loss: 0.01712868238519877\n",
      "  Batch 7000/20625, Average Loss: 0.017281446777749808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 8000/20625, Average Loss: 0.017563549098558725\n",
      "  Batch 9000/20625, Average Loss: 0.01740302893286571\n",
      "  Batch 10000/20625, Average Loss: 0.017260126777924597\n",
      "  Batch 11000/20625, Average Loss: 0.017266884870827198\n",
      "  Batch 12000/20625, Average Loss: 0.017094806951936336\n",
      "  Batch 13000/20625, Average Loss: 0.01712880195537582\n",
      "  Batch 14000/20625, Average Loss: 0.01705438599968329\n",
      "  Batch 15000/20625, Average Loss: 0.016824685053899884\n",
      "  Batch 16000/20625, Average Loss: 0.01672628661664203\n",
      "  Batch 17000/20625, Average Loss: 0.01649872393719852\n",
      "  Batch 18000/20625, Average Loss: 0.016613323317840694\n",
      "  Batch 19000/20625, Average Loss: 0.016529380472144112\n",
      "  Batch 20000/20625, Average Loss: 0.016372422492597252\n",
      "  Average Training Loss: 0.01707212083523698\n",
      "  Average Training Loss: 0.01707212083523698, Average Validation Loss: 0.016654354302554148\n",
      "  Average Test Loss: 0.01659486497063764\n",
      "Epoch 8/200: \n",
      "  Batch 1000/20625, Average Loss: 0.01636743049742654\n",
      "  Batch 2000/20625, Average Loss: 0.0158121356186457\n",
      "  Batch 3000/20625, Average Loss: 0.01637904148036614\n",
      "  Batch 4000/20625, Average Loss: 0.01626448215637356\n",
      "  Batch 5000/20625, Average Loss: 0.015813575826585293\n",
      "  Batch 6000/20625, Average Loss: 0.016025974137242882\n",
      "  Batch 7000/20625, Average Loss: 0.015826213458087296\n",
      "  Batch 8000/20625, Average Loss: 0.01573214833578095\n",
      "  Batch 9000/20625, Average Loss: 0.01600129011273384\n",
      "  Batch 10000/20625, Average Loss: 0.01576179088046774\n",
      "  Batch 11000/20625, Average Loss: 0.01605069963168353\n",
      "  Batch 12000/20625, Average Loss: 0.016019291777629407\n",
      "  Batch 13000/20625, Average Loss: 0.015821824531070887\n",
      "  Batch 14000/20625, Average Loss: 0.015484894039109348\n",
      "  Batch 15000/20625, Average Loss: 0.015860809745732694\n",
      "  Batch 16000/20625, Average Loss: 0.015417419229634107\n",
      "  Batch 17000/20625, Average Loss: 0.015510037275031209\n",
      "  Batch 18000/20625, Average Loss: 0.01559823149163276\n",
      "  Batch 19000/20625, Average Loss: 0.015600585701875388\n",
      "  Batch 20000/20625, Average Loss: 0.015423362183384597\n",
      "  Average Training Loss: 0.015814265030328973\n",
      "  Average Training Loss: 0.015814265030328973, Average Validation Loss: 0.015459982097084457\n",
      "  Average Test Loss: 0.015512968279711725\n",
      "Epoch 9/200: \n",
      "  Batch 1000/20625, Average Loss: 0.015170007698703558\n",
      "  Batch 2000/20625, Average Loss: 0.015023314804304392\n",
      "  Batch 3000/20625, Average Loss: 0.015007593114860355\n",
      "  Batch 4000/20625, Average Loss: 0.014979458697140216\n",
      "  Batch 5000/20625, Average Loss: 0.014943912215530873\n",
      "  Batch 6000/20625, Average Loss: 0.014730490687768907\n",
      "  Batch 7000/20625, Average Loss: 0.014805269960314036\n",
      "  Batch 8000/20625, Average Loss: 0.014975102571770548\n",
      "  Batch 9000/20625, Average Loss: 0.014621270862407982\n",
      "  Batch 10000/20625, Average Loss: 0.014632153353653848\n",
      "  Batch 11000/20625, Average Loss: 0.014875078079756349\n",
      "  Batch 12000/20625, Average Loss: 0.014560581520199775\n",
      "  Batch 13000/20625, Average Loss: 0.014682590254116803\n",
      "  Batch 14000/20625, Average Loss: 0.014721084047574549\n",
      "  Batch 15000/20625, Average Loss: 0.014441464990610256\n",
      "  Batch 16000/20625, Average Loss: 0.014362996965646744\n",
      "  Batch 17000/20625, Average Loss: 0.014555475806817413\n",
      "  Batch 18000/20625, Average Loss: 0.014544201800134033\n",
      "  Batch 19000/20625, Average Loss: 0.014364699892699718\n",
      "  Batch 20000/20625, Average Loss: 0.014382602671161294\n",
      "  Average Training Loss: 0.014703522380746223\n",
      "  Average Training Loss: 0.014703522380746223, Average Validation Loss: 0.014393492852341415\n",
      "  Average Test Loss: 0.014414812280712131\n",
      "Epoch 10/200: \n",
      "  Batch 1000/20625, Average Loss: 0.01375908904406242\n",
      "  Batch 2000/20625, Average Loss: 0.01387897117389366\n",
      "  Batch 3000/20625, Average Loss: 0.013834271501284093\n",
      "  Batch 4000/20625, Average Loss: 0.014036843575071543\n",
      "  Batch 5000/20625, Average Loss: 0.0139374931701459\n",
      "  Batch 6000/20625, Average Loss: 0.013967103283852339\n",
      "  Batch 7000/20625, Average Loss: 0.013872944539645687\n",
      "  Batch 8000/20625, Average Loss: 0.013588934562169015\n",
      "  Batch 9000/20625, Average Loss: 0.01376719350554049\n",
      "  Batch 10000/20625, Average Loss: 0.013822954562027007\n",
      "  Batch 11000/20625, Average Loss: 0.01369054407067597\n",
      "  Batch 12000/20625, Average Loss: 0.01385539908031933\n",
      "  Batch 13000/20625, Average Loss: 0.013624826073646545\n",
      "  Batch 14000/20625, Average Loss: 0.013418934800196439\n",
      "  Batch 15000/20625, Average Loss: 0.01383997110882774\n",
      "  Batch 16000/20625, Average Loss: 0.013458278788719327\n",
      "  Batch 17000/20625, Average Loss: 0.013623696653172373\n",
      "  Batch 18000/20625, Average Loss: 0.01354483864782378\n",
      "  Batch 19000/20625, Average Loss: 0.013379236194305122\n",
      "  Batch 20000/20625, Average Loss: 0.013398593930061906\n",
      "  Average Training Loss: 0.013706286715919322\n",
      "  Average Training Loss: 0.013706286715919322, Average Validation Loss: 0.013780276764752335\n",
      "  Average Test Loss: 0.013735970652575352\n",
      "Epoch 11/200: \n",
      "  Batch 1000/20625, Average Loss: 0.013004279244225472\n",
      "  Batch 2000/20625, Average Loss: 0.013058145519345999\n",
      "  Batch 3000/20625, Average Loss: 0.01326362979132682\n",
      "  Batch 4000/20625, Average Loss: 0.01321421325695701\n",
      "  Batch 5000/20625, Average Loss: 0.0128303487803787\n",
      "  Batch 6000/20625, Average Loss: 0.01329215920995921\n",
      "  Batch 7000/20625, Average Loss: 0.01313083828240633\n",
      "  Batch 8000/20625, Average Loss: 0.013033777621341868\n",
      "  Batch 9000/20625, Average Loss: 0.012904466769192368\n",
      "  Batch 10000/20625, Average Loss: 0.012943043101578951\n",
      "  Batch 11000/20625, Average Loss: 0.012610057066427543\n",
      "  Batch 12000/20625, Average Loss: 0.012896403021179139\n",
      "  Batch 13000/20625, Average Loss: 0.012691630677320064\n",
      "  Batch 14000/20625, Average Loss: 0.012837278288789094\n",
      "  Batch 15000/20625, Average Loss: 0.012742105399258435\n",
      "  Batch 16000/20625, Average Loss: 0.012606708654668182\n",
      "  Batch 17000/20625, Average Loss: 0.012753199987113476\n",
      "  Batch 18000/20625, Average Loss: 0.012581491795368492\n",
      "  Batch 19000/20625, Average Loss: 0.012692255102330819\n",
      "  Batch 20000/20625, Average Loss: 0.012601207827217877\n",
      "  Average Training Loss: 0.012873145787463043\n",
      "  Average Training Loss: 0.012873145787463043, Average Validation Loss: 0.013110226177773143\n",
      "  Average Test Loss: 0.013073236561696908\n",
      "Epoch 12/200: \n",
      "  Batch 1000/20625, Average Loss: 0.012333914798684419\n",
      "  Batch 2000/20625, Average Loss: 0.01218159906193614\n",
      "  Batch 3000/20625, Average Loss: 0.01231992230657488\n",
      "  Batch 4000/20625, Average Loss: 0.012292554541956633\n",
      "  Batch 5000/20625, Average Loss: 0.012403192207915708\n",
      "  Batch 6000/20625, Average Loss: 0.012191135308006779\n",
      "  Batch 7000/20625, Average Loss: 0.012224863746203482\n",
      "  Batch 8000/20625, Average Loss: 0.012330488524166867\n",
      "  Batch 9000/20625, Average Loss: 0.012459960965206846\n",
      "  Batch 10000/20625, Average Loss: 0.012003727924078703\n",
      "  Batch 11000/20625, Average Loss: 0.011980468224501238\n",
      "  Batch 12000/20625, Average Loss: 0.012019156246911734\n",
      "  Batch 13000/20625, Average Loss: 0.01227177319629118\n",
      "  Batch 14000/20625, Average Loss: 0.012021896843099967\n",
      "  Batch 15000/20625, Average Loss: 0.0122546409254428\n",
      "  Batch 16000/20625, Average Loss: 0.011937029267661273\n",
      "  Batch 17000/20625, Average Loss: 0.012222746627172456\n",
      "  Batch 18000/20625, Average Loss: 0.011821058520814404\n",
      "  Batch 19000/20625, Average Loss: 0.01187946582888253\n",
      "  Batch 20000/20625, Average Loss: 0.011937131240265443\n",
      "  Average Training Loss: 0.012145098730233131\n",
      "  Average Training Loss: 0.012145098730233131, Average Validation Loss: 0.012329809320881083\n",
      "  Average Test Loss: 0.012336174964497366\n",
      "Epoch 13/200: \n",
      "  Batch 1000/20625, Average Loss: 0.011602866475237533\n",
      "  Batch 2000/20625, Average Loss: 0.011860813937149942\n",
      "  Batch 3000/20625, Average Loss: 0.011461289470084011\n",
      "  Batch 4000/20625, Average Loss: 0.011820036455057562\n",
      "  Batch 5000/20625, Average Loss: 0.011504710891982541\n",
      "  Batch 6000/20625, Average Loss: 0.011958659681258723\n",
      "  Batch 7000/20625, Average Loss: 0.011838277680566535\n",
      "  Batch 8000/20625, Average Loss: 0.011471557195764035\n",
      "  Batch 9000/20625, Average Loss: 0.011399535397067666\n",
      "  Batch 10000/20625, Average Loss: 0.011504035233519972\n",
      "  Batch 11000/20625, Average Loss: 0.01135129465162754\n",
      "  Batch 12000/20625, Average Loss: 0.011352140647824853\n",
      "  Batch 13000/20625, Average Loss: 0.011348745625233277\n",
      "  Batch 14000/20625, Average Loss: 0.01147042309306562\n",
      "  Batch 15000/20625, Average Loss: 0.011300881245406345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 16000/20625, Average Loss: 0.01142967895232141\n",
      "  Batch 17000/20625, Average Loss: 0.011432777323760092\n",
      "  Batch 18000/20625, Average Loss: 0.011643248155713081\n",
      "  Batch 19000/20625, Average Loss: 0.011471142620313913\n",
      "  Batch 20000/20625, Average Loss: 0.011627759352559225\n",
      "  Average Training Loss: 0.011538902676692515\n",
      "  Average Training Loss: 0.011538902676692515, Average Validation Loss: 0.012080032961414749\n",
      "  Average Test Loss: 0.012093334216772585\n",
      "Epoch 14/200: \n",
      "  Batch 1000/20625, Average Loss: 0.011012484411010518\n",
      "  Batch 2000/20625, Average Loss: 0.01108866645861417\n",
      "  Batch 3000/20625, Average Loss: 0.011073197968536988\n",
      "  Batch 4000/20625, Average Loss: 0.011001715733436867\n",
      "  Batch 5000/20625, Average Loss: 0.01108947358140722\n",
      "  Batch 6000/20625, Average Loss: 0.01104210236459039\n",
      "  Batch 7000/20625, Average Loss: 0.011207495443755761\n",
      "  Batch 8000/20625, Average Loss: 0.01079943874012679\n",
      "  Batch 9000/20625, Average Loss: 0.01086803668201901\n",
      "  Batch 10000/20625, Average Loss: 0.011003438951913268\n",
      "  Batch 11000/20625, Average Loss: 0.010839041901985183\n",
      "  Batch 12000/20625, Average Loss: 0.011223882868420333\n",
      "  Batch 13000/20625, Average Loss: 0.010878791116643697\n",
      "  Batch 14000/20625, Average Loss: 0.010903284213505685\n",
      "  Batch 15000/20625, Average Loss: 0.01109425173420459\n",
      "  Batch 16000/20625, Average Loss: 0.010895930108148605\n",
      "  Batch 17000/20625, Average Loss: 0.010811341715510934\n",
      "  Batch 18000/20625, Average Loss: 0.010929396617459133\n",
      "  Batch 19000/20625, Average Loss: 0.010728361468296498\n",
      "  Batch 20000/20625, Average Loss: 0.010945982660865411\n",
      "  Average Training Loss: 0.010965796179742072\n",
      "  Average Training Loss: 0.010965796179742072, Average Validation Loss: 0.011421421454088683\n",
      "  Average Test Loss: 0.011377313787423526\n",
      "Epoch 15/200: \n",
      "  Batch 1000/20625, Average Loss: 0.010551039592828602\n",
      "  Batch 2000/20625, Average Loss: 0.010304938013665377\n",
      "  Batch 3000/20625, Average Loss: 0.010333162496099248\n",
      "  Batch 4000/20625, Average Loss: 0.010647880900418386\n",
      "  Batch 5000/20625, Average Loss: 0.010474975930526853\n",
      "  Batch 6000/20625, Average Loss: 0.010493082671891897\n",
      "  Batch 7000/20625, Average Loss: 0.01046992878941819\n",
      "  Batch 8000/20625, Average Loss: 0.010516398875275626\n",
      "  Batch 9000/20625, Average Loss: 0.010345897087594494\n",
      "  Batch 10000/20625, Average Loss: 0.010529974443838\n",
      "  Batch 11000/20625, Average Loss: 0.010348888667533173\n",
      "  Batch 12000/20625, Average Loss: 0.010578793698339722\n",
      "  Batch 13000/20625, Average Loss: 0.010326316468417645\n",
      "  Batch 14000/20625, Average Loss: 0.010686087684705853\n",
      "  Batch 15000/20625, Average Loss: 0.010293831566348673\n",
      "  Batch 16000/20625, Average Loss: 0.010576964165316895\n",
      "  Batch 17000/20625, Average Loss: 0.01069230180978775\n",
      "  Batch 18000/20625, Average Loss: 0.01062593201897107\n",
      "  Batch 19000/20625, Average Loss: 0.010336744316853583\n",
      "  Batch 20000/20625, Average Loss: 0.010508435410447419\n",
      "  Average Training Loss: 0.0104772547944082\n",
      "  Average Training Loss: 0.0104772547944082, Average Validation Loss: 0.011044220675029765\n",
      "  Average Test Loss: 0.011036420699390653\n",
      "Epoch 16/200: \n",
      "  Batch 1000/20625, Average Loss: 0.009991637743311002\n",
      "  Batch 2000/20625, Average Loss: 0.010035302877658978\n",
      "  Batch 3000/20625, Average Loss: 0.010056621343363076\n",
      "  Batch 4000/20625, Average Loss: 0.010055088635068387\n",
      "  Batch 5000/20625, Average Loss: 0.010209823604673146\n",
      "  Batch 6000/20625, Average Loss: 0.010219404813367873\n",
      "  Batch 7000/20625, Average Loss: 0.010075445880647748\n",
      "  Batch 8000/20625, Average Loss: 0.009993622378446162\n",
      "  Batch 9000/20625, Average Loss: 0.009933191773016006\n",
      "  Batch 10000/20625, Average Loss: 0.010122296893969178\n",
      "  Batch 11000/20625, Average Loss: 0.010264011508785188\n",
      "  Batch 12000/20625, Average Loss: 0.009978610758436843\n",
      "  Batch 13000/20625, Average Loss: 0.010060257208533586\n",
      "  Batch 14000/20625, Average Loss: 0.01004121279483661\n",
      "  Batch 15000/20625, Average Loss: 0.010002304010326042\n",
      "  Batch 16000/20625, Average Loss: 0.009989104599691927\n",
      "  Batch 17000/20625, Average Loss: 0.010037748406059109\n",
      "  Batch 18000/20625, Average Loss: 0.010076693201437592\n",
      "  Batch 19000/20625, Average Loss: 0.010019331462448463\n",
      "  Batch 20000/20625, Average Loss: 0.009902473157271743\n",
      "  Average Training Loss: 0.010056348635386111\n",
      "  Average Training Loss: 0.010056348635386111, Average Validation Loss: 0.010619606291184625\n",
      "  Average Test Loss: 0.010599694886164107\n",
      "Epoch 17/200: \n",
      "  Batch 1000/20625, Average Loss: 0.009570956436218693\n",
      "  Batch 2000/20625, Average Loss: 0.009610795969842002\n",
      "  Batch 3000/20625, Average Loss: 0.009587168113561348\n",
      "  Batch 4000/20625, Average Loss: 0.009671181814512238\n",
      "  Batch 5000/20625, Average Loss: 0.00972592486650683\n",
      "  Batch 6000/20625, Average Loss: 0.009558422384085134\n",
      "  Batch 7000/20625, Average Loss: 0.009912758432328701\n",
      "  Batch 8000/20625, Average Loss: 0.009713330106576904\n",
      "  Batch 9000/20625, Average Loss: 0.009877967052627355\n",
      "  Batch 10000/20625, Average Loss: 0.009585010116919875\n",
      "  Batch 11000/20625, Average Loss: 0.009731401439988985\n",
      "  Batch 12000/20625, Average Loss: 0.009701279373606667\n",
      "  Batch 13000/20625, Average Loss: 0.009542512335116044\n",
      "  Batch 14000/20625, Average Loss: 0.009569354887353256\n",
      "  Batch 15000/20625, Average Loss: 0.009683979856781662\n",
      "  Batch 16000/20625, Average Loss: 0.00968197008385323\n",
      "  Batch 17000/20625, Average Loss: 0.009669635196216404\n",
      "  Batch 18000/20625, Average Loss: 0.009645211883354932\n",
      "  Batch 19000/20625, Average Loss: 0.009800570181803778\n",
      "  Batch 20000/20625, Average Loss: 0.009616105472901836\n",
      "  Average Training Loss: 0.009676870108282927\n",
      "  Average Training Loss: 0.009676870108282927, Average Validation Loss: 0.01035718523946065\n",
      "  Average Test Loss: 0.010312756325875113\n",
      "Epoch 18/200: \n",
      "  Batch 1000/20625, Average Loss: 0.009131252137711272\n",
      "  Batch 2000/20625, Average Loss: 0.009209389811381698\n",
      "  Batch 3000/20625, Average Loss: 0.009297793064266443\n",
      "  Batch 4000/20625, Average Loss: 0.00924768429528922\n",
      "  Batch 5000/20625, Average Loss: 0.009186183944577351\n",
      "  Batch 6000/20625, Average Loss: 0.009305148525163532\n",
      "  Batch 7000/20625, Average Loss: 0.009356165003031493\n",
      "  Batch 8000/20625, Average Loss: 0.009216446883743628\n",
      "  Batch 9000/20625, Average Loss: 0.0091997688151896\n",
      "  Batch 10000/20625, Average Loss: 0.009396138140233233\n",
      "  Batch 11000/20625, Average Loss: 0.009310145674739033\n",
      "  Batch 12000/20625, Average Loss: 0.009482010293751956\n",
      "  Batch 13000/20625, Average Loss: 0.009267786992480978\n",
      "  Batch 14000/20625, Average Loss: 0.009320728569291533\n",
      "  Batch 15000/20625, Average Loss: 0.009499263403471559\n",
      "  Batch 16000/20625, Average Loss: 0.009281426868401468\n",
      "  Batch 17000/20625, Average Loss: 0.009251750328578055\n",
      "  Batch 18000/20625, Average Loss: 0.009428340558661147\n",
      "  Batch 19000/20625, Average Loss: 0.00946498720732052\n",
      "  Batch 20000/20625, Average Loss: 0.009347807676531374\n",
      "  Average Training Loss: 0.009315931197588868\n",
      "  Average Training Loss: 0.009315931197588868, Average Validation Loss: 0.010042452490277383\n",
      "  Average Test Loss: 0.010026850438206688\n",
      "Epoch 19/200: \n",
      "  Batch 1000/20625, Average Loss: 0.008776547922519968\n",
      "  Batch 2000/20625, Average Loss: 0.008966794430511073\n",
      "  Batch 3000/20625, Average Loss: 0.008917306097922847\n",
      "  Batch 4000/20625, Average Loss: 0.009207243018317967\n",
      "  Batch 5000/20625, Average Loss: 0.009119474683655426\n",
      "  Batch 6000/20625, Average Loss: 0.009145317312562838\n",
      "  Batch 7000/20625, Average Loss: 0.008857728806324303\n",
      "  Batch 8000/20625, Average Loss: 0.009025728473439812\n",
      "  Batch 9000/20625, Average Loss: 0.008968132148031145\n",
      "  Batch 10000/20625, Average Loss: 0.008990387655328958\n",
      "  Batch 11000/20625, Average Loss: 0.009089355072937906\n",
      "  Batch 12000/20625, Average Loss: 0.009158313630963675\n",
      "  Batch 13000/20625, Average Loss: 0.009111309529747815\n",
      "  Batch 14000/20625, Average Loss: 0.008891231725225225\n",
      "  Batch 15000/20625, Average Loss: 0.009028098060749471\n",
      "  Batch 16000/20625, Average Loss: 0.009135115664917976\n",
      "  Batch 17000/20625, Average Loss: 0.008858064050786197\n",
      "  Batch 18000/20625, Average Loss: 0.008966671560890973\n",
      "  Batch 19000/20625, Average Loss: 0.008947223256574944\n",
      "  Batch 20000/20625, Average Loss: 0.00908572601689957\n",
      "  Average Training Loss: 0.00901407089731797\n",
      "  Average Training Loss: 0.00901407089731797, Average Validation Loss: 0.00975683109549152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Test Loss: 0.00967313922104271\n",
      "Epoch 20/200: \n",
      "  Batch 1000/20625, Average Loss: 0.00875947995763272\n",
      "  Batch 2000/20625, Average Loss: 0.008748368530767039\n",
      "  Batch 3000/20625, Average Loss: 0.008656195683404803\n",
      "  Batch 4000/20625, Average Loss: 0.008701388653134927\n",
      "  Batch 5000/20625, Average Loss: 0.008689887989079579\n",
      "  Batch 6000/20625, Average Loss: 0.00894722718349658\n",
      "  Batch 7000/20625, Average Loss: 0.008736406311159954\n",
      "  Batch 8000/20625, Average Loss: 0.008793726931791753\n",
      "  Batch 9000/20625, Average Loss: 0.008793412543134763\n",
      "  Batch 10000/20625, Average Loss: 0.008670964572578668\n",
      "  Batch 11000/20625, Average Loss: 0.008714108486892656\n",
      "  Batch 12000/20625, Average Loss: 0.008636833078460767\n",
      "  Batch 13000/20625, Average Loss: 0.00885359422210604\n",
      "  Batch 14000/20625, Average Loss: 0.008910960229812189\n",
      "  Batch 15000/20625, Average Loss: 0.008737694617826492\n",
      "  Batch 16000/20625, Average Loss: 0.008647067603422329\n",
      "  Batch 17000/20625, Average Loss: 0.00887228032015264\n",
      "  Batch 18000/20625, Average Loss: 0.00880155873694457\n",
      "  Batch 19000/20625, Average Loss: 0.008774584262864664\n",
      "  Batch 20000/20625, Average Loss: 0.008598033447982743\n",
      "  Average Training Loss: 0.008748043576582815\n",
      "  Average Training Loss: 0.008748043576582815, Average Validation Loss: 0.009647751981463299\n",
      "  Average Test Loss: 0.009595346644519677\n",
      "Epoch 21/200: \n",
      "  Batch 1000/20625, Average Loss: 0.00837008396233432\n",
      "  Batch 2000/20625, Average Loss: 0.008533037220826373\n",
      "  Batch 3000/20625, Average Loss: 0.008548084790585563\n",
      "  Batch 4000/20625, Average Loss: 0.008526565786916762\n",
      "  Batch 5000/20625, Average Loss: 0.008508821416646242\n",
      "  Batch 6000/20625, Average Loss: 0.008520266340579837\n",
      "  Batch 7000/20625, Average Loss: 0.008449869707459584\n",
      "  Batch 8000/20625, Average Loss: 0.008689036660827697\n",
      "  Batch 9000/20625, Average Loss: 0.008398462402401491\n",
      "  Batch 10000/20625, Average Loss: 0.008432152630062775\n",
      "  Batch 11000/20625, Average Loss: 0.008482068249722943\n",
      "  Batch 12000/20625, Average Loss: 0.008464417605195194\n",
      "  Batch 13000/20625, Average Loss: 0.008425042438087985\n",
      "  Batch 14000/20625, Average Loss: 0.0084391502373619\n",
      "  Batch 15000/20625, Average Loss: 0.008506063650129363\n",
      "  Batch 16000/20625, Average Loss: 0.008562195910606533\n",
      "  Batch 17000/20625, Average Loss: 0.00853585279127583\n",
      "  Batch 18000/20625, Average Loss: 0.008609042495489121\n",
      "  Batch 19000/20625, Average Loss: 0.00856169608910568\n",
      "  Batch 20000/20625, Average Loss: 0.008659498856868595\n",
      "  Average Training Loss: 0.008507216728450449\n",
      "  Average Training Loss: 0.008507216728450449, Average Validation Loss: 0.009351889922828571\n",
      "  Average Test Loss: 0.009291679835014171\n",
      "Epoch 22/200: \n",
      "  Batch 1000/20625, Average Loss: 0.008203599784756078\n",
      "  Batch 2000/20625, Average Loss: 0.008224216681439429\n",
      "  Batch 3000/20625, Average Loss: 0.00825899925478734\n",
      "  Batch 4000/20625, Average Loss: 0.008355519342701883\n",
      "  Batch 5000/20625, Average Loss: 0.008238875611452385\n",
      "  Batch 6000/20625, Average Loss: 0.008282899108016863\n",
      "  Batch 7000/20625, Average Loss: 0.008347034668782725\n",
      "  Batch 8000/20625, Average Loss: 0.008305029037874192\n",
      "  Batch 9000/20625, Average Loss: 0.008269925840198993\n",
      "  Batch 10000/20625, Average Loss: 0.00842865142575465\n",
      "  Batch 11000/20625, Average Loss: 0.008015401256969198\n",
      "  Batch 12000/20625, Average Loss: 0.008396925266366453\n",
      "  Batch 13000/20625, Average Loss: 0.00835672724689357\n",
      "  Batch 14000/20625, Average Loss: 0.008227767016040161\n",
      "  Batch 15000/20625, Average Loss: 0.008322028611321003\n",
      "  Batch 16000/20625, Average Loss: 0.008158119863830507\n",
      "  Batch 17000/20625, Average Loss: 0.008263501457637176\n",
      "  Batch 18000/20625, Average Loss: 0.008315223475452512\n",
      "  Batch 19000/20625, Average Loss: 0.008367155283456669\n",
      "  Batch 20000/20625, Average Loss: 0.008341028678463772\n",
      "  Average Training Loss: 0.00828248249679578\n",
      "  Average Training Loss: 0.00828248249679578, Average Validation Loss: 0.00933590942450946\n",
      "  Average Test Loss: 0.009260294359219429\n",
      "Epoch 23/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007834724385756999\n",
      "  Batch 2000/20625, Average Loss: 0.008064676107605919\n",
      "  Batch 3000/20625, Average Loss: 0.008173107004258781\n",
      "  Batch 4000/20625, Average Loss: 0.008041596936760471\n",
      "  Batch 5000/20625, Average Loss: 0.008176122124539689\n",
      "  Batch 6000/20625, Average Loss: 0.007975619374890813\n",
      "  Batch 7000/20625, Average Loss: 0.008066661068005488\n",
      "  Batch 8000/20625, Average Loss: 0.008070162369171158\n",
      "  Batch 9000/20625, Average Loss: 0.008060457683866843\n",
      "  Batch 10000/20625, Average Loss: 0.008108849360607564\n",
      "  Batch 11000/20625, Average Loss: 0.007916667197598145\n",
      "  Batch 12000/20625, Average Loss: 0.008294270621379838\n",
      "  Batch 13000/20625, Average Loss: 0.008150044013047591\n",
      "  Batch 14000/20625, Average Loss: 0.008093192073982208\n",
      "  Batch 15000/20625, Average Loss: 0.008097477289615199\n",
      "  Batch 16000/20625, Average Loss: 0.008230758022749796\n",
      "  Batch 17000/20625, Average Loss: 0.008156116127269342\n",
      "  Batch 18000/20625, Average Loss: 0.007951225790893659\n",
      "  Batch 19000/20625, Average Loss: 0.00800562262814492\n",
      "  Batch 20000/20625, Average Loss: 0.008163291750941425\n",
      "  Average Training Loss: 0.008079078555688488\n",
      "  Average Training Loss: 0.008079078555688488, Average Validation Loss: 0.009152715217485086\n",
      "  Average Test Loss: 0.009113892972866997\n",
      "Epoch 24/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007769766008714214\n",
      "  Batch 2000/20625, Average Loss: 0.007783640033332631\n",
      "  Batch 3000/20625, Average Loss: 0.007919472947251052\n",
      "  Batch 4000/20625, Average Loss: 0.007841769352322444\n",
      "  Batch 5000/20625, Average Loss: 0.007922173773869872\n",
      "  Batch 6000/20625, Average Loss: 0.007910441629821435\n",
      "  Batch 7000/20625, Average Loss: 0.007942585435928778\n",
      "  Batch 8000/20625, Average Loss: 0.008060449150856585\n",
      "  Batch 9000/20625, Average Loss: 0.007870116013102233\n",
      "  Batch 10000/20625, Average Loss: 0.00798929181951098\n",
      "  Batch 11000/20625, Average Loss: 0.007921239758608863\n",
      "  Batch 12000/20625, Average Loss: 0.007936926603550092\n",
      "  Batch 13000/20625, Average Loss: 0.007977088944520802\n",
      "  Batch 14000/20625, Average Loss: 0.007858505222015083\n",
      "  Batch 15000/20625, Average Loss: 0.00799946659663692\n",
      "  Batch 16000/20625, Average Loss: 0.007869210768956691\n",
      "  Batch 17000/20625, Average Loss: 0.008001133961835875\n",
      "  Batch 18000/20625, Average Loss: 0.007915340373991057\n",
      "  Batch 19000/20625, Average Loss: 0.007797323705628514\n",
      "  Batch 20000/20625, Average Loss: 0.007763070081127807\n",
      "  Average Training Loss: 0.007902190906791524\n",
      "  Average Training Loss: 0.007902190906791524, Average Validation Loss: 0.008956168868613357\n",
      "  Average Test Loss: 0.008917219791988831\n",
      "Epoch 25/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007504056695033796\n",
      "  Batch 2000/20625, Average Loss: 0.00760560074960813\n",
      "  Batch 3000/20625, Average Loss: 0.007752579485299066\n",
      "  Batch 4000/20625, Average Loss: 0.0076998214991763235\n",
      "  Batch 5000/20625, Average Loss: 0.007702944840304553\n",
      "  Batch 6000/20625, Average Loss: 0.007616876944201067\n",
      "  Batch 7000/20625, Average Loss: 0.00761985155264847\n",
      "  Batch 8000/20625, Average Loss: 0.007847016220679506\n",
      "  Batch 9000/20625, Average Loss: 0.007628025545040146\n",
      "  Batch 10000/20625, Average Loss: 0.007811324318638071\n",
      "  Batch 11000/20625, Average Loss: 0.007861144683789461\n",
      "  Batch 12000/20625, Average Loss: 0.007787172104464844\n",
      "  Batch 13000/20625, Average Loss: 0.007695361567893997\n",
      "  Batch 14000/20625, Average Loss: 0.0077918872160371395\n",
      "  Batch 15000/20625, Average Loss: 0.007733820470515638\n",
      "  Batch 16000/20625, Average Loss: 0.007661129500018433\n",
      "  Batch 17000/20625, Average Loss: 0.007873913609655574\n",
      "  Batch 18000/20625, Average Loss: 0.007701786129269749\n",
      "  Batch 19000/20625, Average Loss: 0.007530791759956628\n",
      "  Batch 20000/20625, Average Loss: 0.007893266884610057\n",
      "  Average Training Loss: 0.007722361935387281\n",
      "  Average Training Loss: 0.007722361935387281, Average Validation Loss: 0.008775300981560413\n",
      "  Average Test Loss: 0.008698594712088633\n",
      "Epoch 26/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007588407348375768\n",
      "  Batch 2000/20625, Average Loss: 0.007551758226007223\n",
      "  Batch 3000/20625, Average Loss: 0.007553032875061035\n",
      "  Batch 4000/20625, Average Loss: 0.007489302157657221\n",
      "  Batch 5000/20625, Average Loss: 0.007482740750303492\n",
      "  Batch 6000/20625, Average Loss: 0.007575306214159355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 7000/20625, Average Loss: 0.007591376800090074\n",
      "  Batch 8000/20625, Average Loss: 0.007550880851689726\n",
      "  Batch 9000/20625, Average Loss: 0.0074781899952795355\n",
      "  Batch 10000/20625, Average Loss: 0.007667029371252284\n",
      "  Batch 11000/20625, Average Loss: 0.007445631647016853\n",
      "  Batch 12000/20625, Average Loss: 0.007668192120967433\n",
      "  Batch 13000/20625, Average Loss: 0.007728777953190729\n",
      "  Batch 14000/20625, Average Loss: 0.0074291851767338815\n",
      "  Batch 15000/20625, Average Loss: 0.007808160923188552\n",
      "  Batch 16000/20625, Average Loss: 0.0076089275174308565\n",
      "  Batch 17000/20625, Average Loss: 0.007760128676192835\n",
      "  Batch 18000/20625, Average Loss: 0.0074820784647017715\n",
      "  Batch 19000/20625, Average Loss: 0.00782426234986633\n",
      "  Batch 20000/20625, Average Loss: 0.007551413441542536\n",
      "  Average Training Loss: 0.0075925036780423285\n",
      "  Average Training Loss: 0.0075925036780423285, Average Validation Loss: 0.009062553932313817\n",
      "  Average Test Loss: 0.009003183710633906\n",
      "Epoch 27/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007250557714840397\n",
      "  Batch 2000/20625, Average Loss: 0.007295557664008811\n",
      "  Batch 3000/20625, Average Loss: 0.007381822362891398\n",
      "  Batch 4000/20625, Average Loss: 0.007352211691206321\n",
      "  Batch 5000/20625, Average Loss: 0.007324092400725931\n",
      "  Batch 6000/20625, Average Loss: 0.007393960253102705\n",
      "  Batch 7000/20625, Average Loss: 0.007516284863930195\n",
      "  Batch 8000/20625, Average Loss: 0.007533750430098735\n",
      "  Batch 9000/20625, Average Loss: 0.0073837071566376835\n",
      "  Batch 10000/20625, Average Loss: 0.007436155549483374\n",
      "  Batch 11000/20625, Average Loss: 0.007588177044410258\n",
      "  Batch 12000/20625, Average Loss: 0.00740453889942728\n",
      "  Batch 13000/20625, Average Loss: 0.007394719628384337\n",
      "  Batch 14000/20625, Average Loss: 0.0076658156120684\n",
      "  Batch 15000/20625, Average Loss: 0.007423482349375263\n",
      "  Batch 16000/20625, Average Loss: 0.00754474120424129\n",
      "  Batch 17000/20625, Average Loss: 0.0075749172314535825\n",
      "  Batch 18000/20625, Average Loss: 0.007566669028950855\n",
      "  Batch 19000/20625, Average Loss: 0.007513837661361321\n",
      "  Batch 20000/20625, Average Loss: 0.0073775417534634475\n",
      "  Average Training Loss: 0.007448699965982726\n",
      "  Average Training Loss: 0.007448699965982726, Average Validation Loss: 0.008537236361384676\n",
      "  Average Test Loss: 0.00847262236913993\n",
      "Epoch 28/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007148807203629985\n",
      "  Batch 2000/20625, Average Loss: 0.0071610071726609026\n",
      "  Batch 3000/20625, Average Loss: 0.007276639494346455\n",
      "  Batch 4000/20625, Average Loss: 0.007152237654197961\n",
      "  Batch 5000/20625, Average Loss: 0.007263389422325417\n",
      "  Batch 6000/20625, Average Loss: 0.007359713538666256\n",
      "  Batch 7000/20625, Average Loss: 0.00729964038496837\n",
      "  Batch 8000/20625, Average Loss: 0.007285280999261886\n",
      "  Batch 9000/20625, Average Loss: 0.007317672500386834\n",
      "  Batch 10000/20625, Average Loss: 0.007467673423001543\n",
      "  Batch 11000/20625, Average Loss: 0.007210705147590488\n",
      "  Batch 12000/20625, Average Loss: 0.007335528703406453\n",
      "  Batch 13000/20625, Average Loss: 0.007412642805837095\n",
      "  Batch 14000/20625, Average Loss: 0.007408297681715339\n",
      "  Batch 15000/20625, Average Loss: 0.007466527285752818\n",
      "  Batch 16000/20625, Average Loss: 0.007400102741783485\n",
      "  Batch 17000/20625, Average Loss: 0.00739904108014889\n",
      "  Batch 18000/20625, Average Loss: 0.007389723720261827\n",
      "  Batch 19000/20625, Average Loss: 0.007294376137666404\n",
      "  Batch 20000/20625, Average Loss: 0.007312064279802143\n",
      "  Average Training Loss: 0.0073196326216457015\n",
      "  Average Training Loss: 0.0073196326216457015, Average Validation Loss: 0.008429234727995057\n",
      "  Average Test Loss: 0.008362383956069029\n",
      "Epoch 29/200: \n",
      "  Batch 1000/20625, Average Loss: 0.007007791216718033\n",
      "  Batch 2000/20625, Average Loss: 0.00718061660323292\n",
      "  Batch 3000/20625, Average Loss: 0.007024691693950444\n",
      "  Batch 4000/20625, Average Loss: 0.007209952979814261\n",
      "  Batch 5000/20625, Average Loss: 0.007160983034642413\n",
      "  Batch 6000/20625, Average Loss: 0.007161788212833926\n",
      "  Batch 7000/20625, Average Loss: 0.007099656244972721\n",
      "  Batch 8000/20625, Average Loss: 0.007284201799193397\n",
      "  Batch 9000/20625, Average Loss: 0.00713453465513885\n",
      "  Batch 10000/20625, Average Loss: 0.007212308899033815\n",
      "  Batch 11000/20625, Average Loss: 0.007239544671494514\n",
      "  Batch 12000/20625, Average Loss: 0.007125261320266873\n",
      "  Batch 13000/20625, Average Loss: 0.00716291873762384\n",
      "  Batch 14000/20625, Average Loss: 0.0072480386646930125\n",
      "  Batch 15000/20625, Average Loss: 0.007178282448323444\n",
      "  Batch 16000/20625, Average Loss: 0.007263450717320666\n",
      "  Batch 17000/20625, Average Loss: 0.007331535954261199\n",
      "  Batch 18000/20625, Average Loss: 0.007334801923017949\n",
      "  Batch 19000/20625, Average Loss: 0.007272320356918499\n",
      "  Batch 20000/20625, Average Loss: 0.007319740194827318\n",
      "  Average Training Loss: 0.007194818368054588\n",
      "  Average Training Loss: 0.007194818368054588, Average Validation Loss: 0.00843917685594835\n",
      "  Average Test Loss: 0.00840463790587831\n",
      "Epoch 30/200: \n",
      "  Batch 1000/20625, Average Loss: 0.0067644870190415535\n",
      "  Batch 2000/20625, Average Loss: 0.00703781837457791\n",
      "  Batch 3000/20625, Average Loss: 0.006911769211525098\n",
      "  Batch 4000/20625, Average Loss: 0.007024028092855588\n",
      "  Batch 5000/20625, Average Loss: 0.006995096370344981\n",
      "  Batch 6000/20625, Average Loss: 0.007118175210664049\n",
      "  Batch 7000/20625, Average Loss: 0.007123009311966598\n",
      "  Batch 8000/20625, Average Loss: 0.007197003485634923\n",
      "  Batch 9000/20625, Average Loss: 0.007113996926695109\n",
      "  Batch 10000/20625, Average Loss: 0.00703839758457616\n",
      "  Batch 11000/20625, Average Loss: 0.007159282904816791\n",
      "  Batch 12000/20625, Average Loss: 0.007056786843924783\n",
      "  Batch 13000/20625, Average Loss: 0.007097602593246847\n",
      "  Batch 14000/20625, Average Loss: 0.007265532362041995\n",
      "  Batch 15000/20625, Average Loss: 0.007161188334692269\n",
      "  Batch 16000/20625, Average Loss: 0.007094585366547107\n",
      "  Batch 17000/20625, Average Loss: 0.007158886546036228\n",
      "  Batch 18000/20625, Average Loss: 0.006986561316996813\n",
      "  Batch 19000/20625, Average Loss: 0.00723350022546947\n",
      "  Batch 20000/20625, Average Loss: 0.0072383358285296705\n",
      "  Average Training Loss: 0.0070892145709501525\n",
      "  Average Training Loss: 0.0070892145709501525, Average Validation Loss: 0.008343064251151097\n",
      "  Average Test Loss: 0.008251829767126184\n",
      "Epoch 31/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006810257950099185\n",
      "  Batch 2000/20625, Average Loss: 0.0068727120873518285\n",
      "  Batch 3000/20625, Average Loss: 0.006962308276677504\n",
      "  Batch 4000/20625, Average Loss: 0.006881160717690364\n",
      "  Batch 5000/20625, Average Loss: 0.006970386211294681\n",
      "  Batch 6000/20625, Average Loss: 0.006915955539327115\n",
      "  Batch 7000/20625, Average Loss: 0.007082436896394938\n",
      "  Batch 8000/20625, Average Loss: 0.00704049555840902\n",
      "  Batch 9000/20625, Average Loss: 0.007038647606153973\n",
      "  Batch 10000/20625, Average Loss: 0.0068555661850841715\n",
      "  Batch 11000/20625, Average Loss: 0.007004276014631614\n",
      "  Batch 12000/20625, Average Loss: 0.007019556486629881\n",
      "  Batch 13000/20625, Average Loss: 0.007050745966145769\n",
      "  Batch 14000/20625, Average Loss: 0.0070841949554160235\n",
      "  Batch 15000/20625, Average Loss: 0.0070209146062843505\n",
      "  Batch 16000/20625, Average Loss: 0.0070874076039763164\n",
      "  Batch 17000/20625, Average Loss: 0.007016009659739211\n",
      "  Batch 18000/20625, Average Loss: 0.007033442965708673\n",
      "  Batch 19000/20625, Average Loss: 0.007080456441966817\n",
      "  Batch 20000/20625, Average Loss: 0.006984424067195505\n",
      "  Average Training Loss: 0.0069888915514968565\n",
      "  Average Training Loss: 0.0069888915514968565, Average Validation Loss: 0.008264052224469904\n",
      "  Average Test Loss: 0.008206860026104828\n",
      "Epoch 32/200: \n",
      "  Batch 1000/20625, Average Loss: 0.00685889052413404\n",
      "  Batch 2000/20625, Average Loss: 0.006781058511463925\n",
      "  Batch 3000/20625, Average Loss: 0.006776616845279932\n",
      "  Batch 4000/20625, Average Loss: 0.006891432154458016\n",
      "  Batch 5000/20625, Average Loss: 0.006718140604207292\n",
      "  Batch 6000/20625, Average Loss: 0.006910708781331778\n",
      "  Batch 7000/20625, Average Loss: 0.006783308133250102\n",
      "  Batch 8000/20625, Average Loss: 0.006929389003897086\n",
      "  Batch 9000/20625, Average Loss: 0.006877991367597133\n",
      "  Batch 10000/20625, Average Loss: 0.006956971765263006\n",
      "  Batch 11000/20625, Average Loss: 0.006899646789068356\n",
      "  Batch 12000/20625, Average Loss: 0.007090852162800729\n",
      "  Batch 13000/20625, Average Loss: 0.006804735257057473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 14000/20625, Average Loss: 0.006737844380317256\n",
      "  Batch 15000/20625, Average Loss: 0.007048763453960418\n",
      "  Batch 16000/20625, Average Loss: 0.006908954019192606\n",
      "  Batch 17000/20625, Average Loss: 0.0069739469785708936\n",
      "  Batch 18000/20625, Average Loss: 0.00706873252405785\n",
      "  Batch 19000/20625, Average Loss: 0.006875644134823233\n",
      "  Batch 20000/20625, Average Loss: 0.006926013118587434\n",
      "  Average Training Loss: 0.0068885561711070215\n",
      "  Average Training Loss: 0.0068885561711070215, Average Validation Loss: 0.008161774206562838\n",
      "  Average Test Loss: 0.008138703610166541\n",
      "Epoch 33/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006696363175520673\n",
      "  Batch 2000/20625, Average Loss: 0.006646675886819139\n",
      "  Batch 3000/20625, Average Loss: 0.00679831334867049\n",
      "  Batch 4000/20625, Average Loss: 0.006789836617885158\n",
      "  Batch 5000/20625, Average Loss: 0.0067539497753605245\n",
      "  Batch 6000/20625, Average Loss: 0.006875910273753107\n",
      "  Batch 7000/20625, Average Loss: 0.006900163450744003\n",
      "  Batch 8000/20625, Average Loss: 0.006688230903586373\n",
      "  Batch 9000/20625, Average Loss: 0.006804228621302173\n",
      "  Batch 10000/20625, Average Loss: 0.006829265612876043\n",
      "  Batch 11000/20625, Average Loss: 0.006789535163668915\n",
      "  Batch 12000/20625, Average Loss: 0.006762855345616117\n",
      "  Batch 13000/20625, Average Loss: 0.006821052530547604\n",
      "  Batch 14000/20625, Average Loss: 0.006775465141516179\n",
      "  Batch 15000/20625, Average Loss: 0.006879618383478373\n",
      "  Batch 16000/20625, Average Loss: 0.006911439629970119\n",
      "  Batch 17000/20625, Average Loss: 0.006757032301975414\n",
      "  Batch 18000/20625, Average Loss: 0.006781314139952883\n",
      "  Batch 19000/20625, Average Loss: 0.006773123979801312\n",
      "  Batch 20000/20625, Average Loss: 0.007033859513932839\n",
      "  Average Training Loss: 0.006808248401574339\n",
      "  Average Training Loss: 0.006808248401574339, Average Validation Loss: 0.008742536967347185\n",
      "  Average Test Loss: 0.008698582248448017\n",
      "Epoch 34/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006573923832038417\n",
      "  Batch 2000/20625, Average Loss: 0.0065922143342904745\n",
      "  Batch 3000/20625, Average Loss: 0.006571268836734817\n",
      "  Batch 4000/20625, Average Loss: 0.006671778060030192\n",
      "  Batch 5000/20625, Average Loss: 0.00671626400295645\n",
      "  Batch 6000/20625, Average Loss: 0.006771062803920359\n",
      "  Batch 7000/20625, Average Loss: 0.0066886391884181644\n",
      "  Batch 8000/20625, Average Loss: 0.006711440495681018\n",
      "  Batch 9000/20625, Average Loss: 0.006684123229468241\n",
      "  Batch 10000/20625, Average Loss: 0.006702751227421686\n",
      "  Batch 11000/20625, Average Loss: 0.006689119589747861\n",
      "  Batch 12000/20625, Average Loss: 0.006696084951749072\n",
      "  Batch 13000/20625, Average Loss: 0.006719378637499176\n",
      "  Batch 14000/20625, Average Loss: 0.006740488934563473\n",
      "  Batch 15000/20625, Average Loss: 0.006736973300110549\n",
      "  Batch 16000/20625, Average Loss: 0.00672298485704232\n",
      "  Batch 17000/20625, Average Loss: 0.006777177194366232\n",
      "  Batch 18000/20625, Average Loss: 0.00690197132434696\n",
      "  Batch 19000/20625, Average Loss: 0.006849211187101901\n",
      "  Batch 20000/20625, Average Loss: 0.006846594021772035\n",
      "  Average Training Loss: 0.006714326623742553\n",
      "  Average Training Loss: 0.006714326623742553, Average Validation Loss: 0.008005987128259177\n",
      "  Average Test Loss: 0.007932086919770454\n",
      "Epoch 35/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006565939082764089\n",
      "  Batch 2000/20625, Average Loss: 0.00642004405404441\n",
      "  Batch 3000/20625, Average Loss: 0.006483031609328464\n",
      "  Batch 4000/20625, Average Loss: 0.006548827690072358\n",
      "  Batch 5000/20625, Average Loss: 0.006601904419017955\n",
      "  Batch 6000/20625, Average Loss: 0.006519466444151476\n",
      "  Batch 7000/20625, Average Loss: 0.0068207973665557805\n",
      "  Batch 8000/20625, Average Loss: 0.006613431484205648\n",
      "  Batch 9000/20625, Average Loss: 0.0067216813506092874\n",
      "  Batch 10000/20625, Average Loss: 0.006612062386237085\n",
      "  Batch 11000/20625, Average Loss: 0.0066778875907184555\n",
      "  Batch 12000/20625, Average Loss: 0.006761259312974289\n",
      "  Batch 13000/20625, Average Loss: 0.006762829122599214\n",
      "  Batch 14000/20625, Average Loss: 0.006662987804040313\n",
      "  Batch 15000/20625, Average Loss: 0.006715610397513956\n",
      "  Batch 16000/20625, Average Loss: 0.006694168061250821\n",
      "  Batch 17000/20625, Average Loss: 0.006680992424022406\n",
      "  Batch 18000/20625, Average Loss: 0.006639880513190291\n",
      "  Batch 19000/20625, Average Loss: 0.006656779438490048\n",
      "  Batch 20000/20625, Average Loss: 0.006546378102619201\n",
      "  Average Training Loss: 0.0066416250028393485\n",
      "  Average Training Loss: 0.0066416250028393485, Average Validation Loss: 0.00823614116925489\n",
      "  Average Test Loss: 0.008200115594325383\n",
      "Epoch 36/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006528633389621973\n",
      "  Batch 2000/20625, Average Loss: 0.006395089589525014\n",
      "  Batch 3000/20625, Average Loss: 0.0065932097344193604\n",
      "  Batch 4000/20625, Average Loss: 0.006505628333659842\n",
      "  Batch 5000/20625, Average Loss: 0.0064638393466593695\n",
      "  Batch 6000/20625, Average Loss: 0.0065412390944547955\n",
      "  Batch 7000/20625, Average Loss: 0.006642420770716854\n",
      "  Batch 8000/20625, Average Loss: 0.006593971520429477\n",
      "  Batch 9000/20625, Average Loss: 0.006515214507700875\n",
      "  Batch 10000/20625, Average Loss: 0.0066488535103853795\n",
      "  Batch 11000/20625, Average Loss: 0.006402434582123533\n",
      "  Batch 12000/20625, Average Loss: 0.006557832929771393\n",
      "  Batch 13000/20625, Average Loss: 0.006568735141772777\n",
      "  Batch 14000/20625, Average Loss: 0.006637674058671109\n",
      "  Batch 15000/20625, Average Loss: 0.00653120511607267\n",
      "  Batch 16000/20625, Average Loss: 0.006778167752781883\n",
      "  Batch 17000/20625, Average Loss: 0.006607862690230832\n",
      "  Batch 18000/20625, Average Loss: 0.006713076515472494\n",
      "  Batch 19000/20625, Average Loss: 0.006633444629842416\n",
      "  Batch 20000/20625, Average Loss: 0.006540382887236774\n",
      "  Average Training Loss: 0.006573316966753566\n",
      "  Average Training Loss: 0.006573316966753566, Average Validation Loss: 0.008130042272872888\n",
      "  Average Test Loss: 0.008070581253057799\n",
      "Epoch 37/200: \n",
      "  Batch 1000/20625, Average Loss: 0.0061998049521353096\n",
      "  Batch 2000/20625, Average Loss: 0.006298615274485201\n",
      "  Batch 3000/20625, Average Loss: 0.006536982739344239\n",
      "  Batch 4000/20625, Average Loss: 0.006347072548931464\n",
      "  Batch 5000/20625, Average Loss: 0.006452434399630874\n",
      "  Batch 6000/20625, Average Loss: 0.006479674369678832\n",
      "  Batch 7000/20625, Average Loss: 0.006455180043587461\n",
      "  Batch 8000/20625, Average Loss: 0.006488749482901767\n",
      "  Batch 9000/20625, Average Loss: 0.00651004496240057\n",
      "  Batch 10000/20625, Average Loss: 0.006521024521440268\n",
      "  Batch 11000/20625, Average Loss: 0.006455148636130616\n",
      "  Batch 12000/20625, Average Loss: 0.006564797386294231\n",
      "  Batch 13000/20625, Average Loss: 0.006562287203967571\n",
      "  Batch 14000/20625, Average Loss: 0.0064874508704524485\n",
      "  Batch 15000/20625, Average Loss: 0.00659880649112165\n",
      "  Batch 16000/20625, Average Loss: 0.006553742987802252\n",
      "  Batch 17000/20625, Average Loss: 0.006578823247924447\n",
      "  Batch 18000/20625, Average Loss: 0.006504921561572701\n",
      "  Batch 19000/20625, Average Loss: 0.006665542055852711\n",
      "  Batch 20000/20625, Average Loss: 0.006499409925192595\n",
      "  Average Training Loss: 0.006491886757974598\n",
      "  Average Training Loss: 0.006491886757974598, Average Validation Loss: 0.00795609295675794\n",
      "  Average Test Loss: 0.007895724916583606\n",
      "Epoch 38/200: \n",
      "  Batch 1000/20625, Average Loss: 0.0062591195632703605\n",
      "  Batch 2000/20625, Average Loss: 0.0062026809002272785\n",
      "  Batch 3000/20625, Average Loss: 0.006384626706363633\n",
      "  Batch 4000/20625, Average Loss: 0.0064136048465734345\n",
      "  Batch 5000/20625, Average Loss: 0.006449348351219669\n",
      "  Batch 6000/20625, Average Loss: 0.006379467443563044\n",
      "  Batch 7000/20625, Average Loss: 0.006442108390154317\n",
      "  Batch 8000/20625, Average Loss: 0.006395938365953043\n",
      "  Batch 9000/20625, Average Loss: 0.006531078271218575\n",
      "  Batch 10000/20625, Average Loss: 0.006535460502374918\n",
      "  Batch 11000/20625, Average Loss: 0.006553639861289412\n",
      "  Batch 12000/20625, Average Loss: 0.006384066289290786\n",
      "  Batch 13000/20625, Average Loss: 0.006333971754764207\n",
      "  Batch 14000/20625, Average Loss: 0.0065134345386177305\n",
      "  Batch 15000/20625, Average Loss: 0.006427816573297605\n",
      "  Batch 16000/20625, Average Loss: 0.006382048818049952\n",
      "  Batch 17000/20625, Average Loss: 0.006599875798914582\n",
      "  Batch 18000/20625, Average Loss: 0.006529809968429618\n",
      "  Batch 19000/20625, Average Loss: 0.006445393140660599\n",
      "  Batch 20000/20625, Average Loss: 0.006488534942967817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.0064330494682219895\n",
      "  Average Training Loss: 0.0064330494682219895, Average Validation Loss: 0.007893495549312167\n",
      "  Average Test Loss: 0.007836246481367555\n",
      "Epoch 39/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006163168628234416\n",
      "  Batch 2000/20625, Average Loss: 0.006353388827294111\n",
      "  Batch 3000/20625, Average Loss: 0.006225422975374386\n",
      "  Batch 4000/20625, Average Loss: 0.006340486725093797\n",
      "  Batch 5000/20625, Average Loss: 0.006363531102309935\n",
      "  Batch 6000/20625, Average Loss: 0.006271355824312195\n",
      "  Batch 7000/20625, Average Loss: 0.006417051326367073\n",
      "  Batch 8000/20625, Average Loss: 0.0062508862880058585\n",
      "  Batch 9000/20625, Average Loss: 0.006275341949192807\n",
      "  Batch 10000/20625, Average Loss: 0.006358434455585666\n",
      "  Batch 11000/20625, Average Loss: 0.0064724795820657165\n",
      "  Batch 12000/20625, Average Loss: 0.006352399917086586\n",
      "  Batch 13000/20625, Average Loss: 0.006355119656538591\n",
      "  Batch 14000/20625, Average Loss: 0.0064763179563451555\n",
      "  Batch 15000/20625, Average Loss: 0.006375628673937172\n",
      "  Batch 16000/20625, Average Loss: 0.006337332968832925\n",
      "  Batch 17000/20625, Average Loss: 0.006478829739498906\n",
      "  Batch 18000/20625, Average Loss: 0.006366914487676695\n",
      "  Batch 19000/20625, Average Loss: 0.006383835841435939\n",
      "  Batch 20000/20625, Average Loss: 0.006455646537942812\n",
      "  Average Training Loss: 0.0063640409028439815\n",
      "  Average Training Loss: 0.0063640409028439815, Average Validation Loss: 0.0077515182122828\n",
      "  Average Test Loss: 0.007735734690719916\n",
      "Epoch 40/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006106184663251042\n",
      "  Batch 2000/20625, Average Loss: 0.00611746427428443\n",
      "  Batch 3000/20625, Average Loss: 0.006152789478190243\n",
      "  Batch 4000/20625, Average Loss: 0.006057482110103592\n",
      "  Batch 5000/20625, Average Loss: 0.006293714477214963\n",
      "  Batch 6000/20625, Average Loss: 0.0063346533813746645\n",
      "  Batch 7000/20625, Average Loss: 0.006387870674952865\n",
      "  Batch 8000/20625, Average Loss: 0.0063742667876649645\n",
      "  Batch 9000/20625, Average Loss: 0.0063384968561586\n",
      "  Batch 10000/20625, Average Loss: 0.006294309265911579\n",
      "  Batch 11000/20625, Average Loss: 0.00642903899308294\n",
      "  Batch 12000/20625, Average Loss: 0.00637524176761508\n",
      "  Batch 13000/20625, Average Loss: 0.006405087259365246\n",
      "  Batch 14000/20625, Average Loss: 0.006248753088642843\n",
      "  Batch 15000/20625, Average Loss: 0.006408070790348575\n",
      "  Batch 16000/20625, Average Loss: 0.006308054228313267\n",
      "  Batch 17000/20625, Average Loss: 0.006271665519452654\n",
      "  Batch 18000/20625, Average Loss: 0.0064639685882721095\n",
      "  Batch 19000/20625, Average Loss: 0.006295596103649587\n",
      "  Batch 20000/20625, Average Loss: 0.006428590564290061\n",
      "  Average Training Loss: 0.006309551511648478\n",
      "  Average Training Loss: 0.006309551511648478, Average Validation Loss: 0.007760182798277642\n",
      "  Average Test Loss: 0.007729502616496965\n",
      "Epoch 41/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006089086231542752\n",
      "  Batch 2000/20625, Average Loss: 0.006053242610418238\n",
      "  Batch 3000/20625, Average Loss: 0.006119621382327751\n",
      "  Batch 4000/20625, Average Loss: 0.0062410650574602185\n",
      "  Batch 5000/20625, Average Loss: 0.006111523824976757\n",
      "  Batch 6000/20625, Average Loss: 0.006111933463835158\n",
      "  Batch 7000/20625, Average Loss: 0.0062686449778266255\n",
      "  Batch 8000/20625, Average Loss: 0.006170427570817992\n",
      "  Batch 9000/20625, Average Loss: 0.006237344734836369\n",
      "  Batch 10000/20625, Average Loss: 0.006413246882730163\n",
      "  Batch 11000/20625, Average Loss: 0.006174245104659349\n",
      "  Batch 12000/20625, Average Loss: 0.006351198893273249\n",
      "  Batch 13000/20625, Average Loss: 0.006302453529788182\n",
      "  Batch 14000/20625, Average Loss: 0.006201670747948811\n",
      "  Batch 15000/20625, Average Loss: 0.006252303977496922\n",
      "  Batch 16000/20625, Average Loss: 0.006485285235801712\n",
      "  Batch 17000/20625, Average Loss: 0.006348480586661026\n",
      "  Batch 18000/20625, Average Loss: 0.006396358010242693\n",
      "  Batch 19000/20625, Average Loss: 0.006346854800009169\n",
      "  Batch 20000/20625, Average Loss: 0.0062347754444926975\n",
      "  Average Training Loss: 0.006248906566061531\n",
      "  Average Training Loss: 0.006248906566061531, Average Validation Loss: 0.0075959890217837\n",
      "  Average Test Loss: 0.007561455267924503\n",
      "Epoch 42/200: \n",
      "  Batch 1000/20625, Average Loss: 0.006010891165584326\n",
      "  Batch 2000/20625, Average Loss: 0.005992528394330293\n",
      "  Batch 3000/20625, Average Loss: 0.0061351168968249116\n",
      "  Batch 4000/20625, Average Loss: 0.006220470790751278\n",
      "  Batch 5000/20625, Average Loss: 0.006181781441206112\n",
      "  Batch 6000/20625, Average Loss: 0.006081685452722013\n",
      "  Batch 7000/20625, Average Loss: 0.006186840214184485\n",
      "  Batch 8000/20625, Average Loss: 0.0062161488962592555\n",
      "  Batch 9000/20625, Average Loss: 0.006084963507018984\n",
      "  Batch 10000/20625, Average Loss: 0.006332325971219689\n",
      "  Batch 11000/20625, Average Loss: 0.006179329330800101\n",
      "  Batch 12000/20625, Average Loss: 0.006278685919009149\n",
      "  Batch 13000/20625, Average Loss: 0.006293529863003642\n",
      "  Batch 14000/20625, Average Loss: 0.006240511921234429\n",
      "  Batch 15000/20625, Average Loss: 0.00628166037844494\n",
      "  Batch 16000/20625, Average Loss: 0.0062762571356724944\n",
      "  Batch 17000/20625, Average Loss: 0.0062729875358054415\n",
      "  Batch 18000/20625, Average Loss: 0.006232684197369963\n",
      "  Batch 19000/20625, Average Loss: 0.0062094881236553195\n",
      "  Batch 20000/20625, Average Loss: 0.006288982043042779\n",
      "  Average Training Loss: 0.006199646794079154\n",
      "  Average Training Loss: 0.006199646794079154, Average Validation Loss: 0.007617457055733367\n",
      "  Average Test Loss: 0.007579592456316963\n",
      "Epoch 43/200: \n",
      "  Batch 1000/20625, Average Loss: 0.0058867003814084455\n",
      "  Batch 2000/20625, Average Loss: 0.006079391108592972\n",
      "  Batch 3000/20625, Average Loss: 0.006062912258901633\n",
      "  Batch 4000/20625, Average Loss: 0.006045006442000158\n",
      "  Batch 5000/20625, Average Loss: 0.006156694441568107\n",
      "  Batch 6000/20625, Average Loss: 0.006018497556913644\n",
      "  Batch 7000/20625, Average Loss: 0.006104360006051138\n",
      "  Batch 8000/20625, Average Loss: 0.006160159787395969\n",
      "  Batch 9000/20625, Average Loss: 0.0062283888584934175\n",
      "  Batch 10000/20625, Average Loss: 0.006198154742014595\n",
      "  Batch 11000/20625, Average Loss: 0.006194722784217447\n",
      "  Batch 12000/20625, Average Loss: 0.006323705816408619\n",
      "  Batch 13000/20625, Average Loss: 0.006168151319259778\n",
      "  Batch 14000/20625, Average Loss: 0.00617767436360009\n",
      "  Batch 15000/20625, Average Loss: 0.006206345406593755\n",
      "  Batch 16000/20625, Average Loss: 0.006112202848540619\n",
      "  Batch 17000/20625, Average Loss: 0.006204631370143034\n",
      "  Batch 18000/20625, Average Loss: 0.006218943124869839\n",
      "  Batch 19000/20625, Average Loss: 0.006235888984636404\n",
      "  Batch 20000/20625, Average Loss: 0.006249979118118062\n",
      "  Average Training Loss: 0.00615648221810433\n",
      "  Average Training Loss: 0.00615648221810433, Average Validation Loss: 0.007609147442702124\n",
      "  Average Test Loss: 0.007566145302695131\n",
      "Epoch 44/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005899121130816638\n",
      "  Batch 2000/20625, Average Loss: 0.0060274667288176716\n",
      "  Batch 3000/20625, Average Loss: 0.005989636631449685\n",
      "  Batch 4000/20625, Average Loss: 0.006041783235268668\n",
      "  Batch 5000/20625, Average Loss: 0.006043271039612591\n",
      "  Batch 6000/20625, Average Loss: 0.006100333933485672\n",
      "  Batch 7000/20625, Average Loss: 0.00602108400175348\n",
      "  Batch 8000/20625, Average Loss: 0.006177876547444612\n",
      "  Batch 9000/20625, Average Loss: 0.00606816062843427\n",
      "  Batch 10000/20625, Average Loss: 0.006156978148501366\n",
      "  Batch 11000/20625, Average Loss: 0.006103708865121007\n",
      "  Batch 12000/20625, Average Loss: 0.006201673570205458\n",
      "  Batch 13000/20625, Average Loss: 0.006118107544956728\n",
      "  Batch 14000/20625, Average Loss: 0.006151696579763666\n",
      "  Batch 15000/20625, Average Loss: 0.0060825813291594385\n",
      "  Batch 16000/20625, Average Loss: 0.006137754887109622\n",
      "  Batch 17000/20625, Average Loss: 0.006115468795294873\n",
      "  Batch 18000/20625, Average Loss: 0.006127302698325366\n",
      "  Batch 19000/20625, Average Loss: 0.006201678268029354\n",
      "  Batch 20000/20625, Average Loss: 0.00620223676902242\n",
      "  Average Training Loss: 0.006099782840738243\n",
      "  Average Training Loss: 0.006099782840738243, Average Validation Loss: 0.007572690316263108\n",
      "  Average Test Loss: 0.007546502265725946\n",
      "Epoch 45/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005871301160659641\n",
      "  Batch 2000/20625, Average Loss: 0.005811571347760037\n",
      "  Batch 3000/20625, Average Loss: 0.005953298631706275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4000/20625, Average Loss: 0.00598563796991948\n",
      "  Batch 5000/20625, Average Loss: 0.005854786696843803\n",
      "  Batch 6000/20625, Average Loss: 0.005978152535390109\n",
      "  Batch 7000/20625, Average Loss: 0.005999151753028855\n",
      "  Batch 8000/20625, Average Loss: 0.006131901917280629\n",
      "  Batch 9000/20625, Average Loss: 0.006191429569153115\n",
      "  Batch 10000/20625, Average Loss: 0.006105613485677168\n",
      "  Batch 11000/20625, Average Loss: 0.00597875267942436\n",
      "  Batch 12000/20625, Average Loss: 0.0060977460901485755\n",
      "  Batch 13000/20625, Average Loss: 0.006248361433274113\n",
      "  Batch 14000/20625, Average Loss: 0.006072438563685864\n",
      "  Batch 15000/20625, Average Loss: 0.00601305018144194\n",
      "  Batch 16000/20625, Average Loss: 0.006131492314394563\n",
      "  Batch 17000/20625, Average Loss: 0.0061719445037888366\n",
      "  Batch 18000/20625, Average Loss: 0.006228906959528104\n",
      "  Batch 19000/20625, Average Loss: 0.006138787393691018\n",
      "  Batch 20000/20625, Average Loss: 0.006121625694679097\n",
      "  Average Training Loss: 0.006052369189409144\n",
      "  Average Training Loss: 0.006052369189409144, Average Validation Loss: 0.007704026467946892\n",
      "  Average Test Loss: 0.007655160572211514\n",
      "Epoch 46/200: \n",
      "  Batch 1000/20625, Average Loss: 0.0057995550860650835\n",
      "  Batch 2000/20625, Average Loss: 0.005926931960042566\n",
      "  Batch 3000/20625, Average Loss: 0.005985520893940702\n",
      "  Batch 4000/20625, Average Loss: 0.005925040337955579\n",
      "  Batch 5000/20625, Average Loss: 0.005882949935737998\n",
      "  Batch 6000/20625, Average Loss: 0.006019177217851393\n",
      "  Batch 7000/20625, Average Loss: 0.005956171511905268\n",
      "  Batch 8000/20625, Average Loss: 0.006037480905186385\n",
      "  Batch 9000/20625, Average Loss: 0.005987476462032646\n",
      "  Batch 10000/20625, Average Loss: 0.006110050416784361\n",
      "  Batch 11000/20625, Average Loss: 0.005999072177102789\n",
      "  Batch 12000/20625, Average Loss: 0.006181895649060607\n",
      "  Batch 13000/20625, Average Loss: 0.006044498115312308\n",
      "  Batch 14000/20625, Average Loss: 0.006049443605123088\n",
      "  Batch 15000/20625, Average Loss: 0.006106676069553942\n",
      "  Batch 16000/20625, Average Loss: 0.005985105342697352\n",
      "  Batch 17000/20625, Average Loss: 0.006080952591612004\n",
      "  Batch 18000/20625, Average Loss: 0.00599413813999854\n",
      "  Batch 19000/20625, Average Loss: 0.00610089382075239\n",
      "  Batch 20000/20625, Average Loss: 0.006043859721510671\n",
      "  Average Training Loss: 0.006016492114645062\n",
      "  Average Training Loss: 0.006016492114645062, Average Validation Loss: 0.007765123273424135\n",
      "  Average Test Loss: 0.007710476936571104\n",
      "Epoch 47/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005790573285194114\n",
      "  Batch 2000/20625, Average Loss: 0.005777838963083923\n",
      "  Batch 3000/20625, Average Loss: 0.00575746463611722\n",
      "  Batch 4000/20625, Average Loss: 0.00598031325568445\n",
      "  Batch 5000/20625, Average Loss: 0.005908458070829511\n",
      "  Batch 6000/20625, Average Loss: 0.0059847678476944565\n",
      "  Batch 7000/20625, Average Loss: 0.0058784988871775565\n",
      "  Batch 8000/20625, Average Loss: 0.005932854908867739\n",
      "  Batch 9000/20625, Average Loss: 0.006004623572807759\n",
      "  Batch 10000/20625, Average Loss: 0.006031068783253431\n",
      "  Batch 11000/20625, Average Loss: 0.005926621720194817\n",
      "  Batch 12000/20625, Average Loss: 0.00601489879982546\n",
      "  Batch 13000/20625, Average Loss: 0.005945101123536006\n",
      "  Batch 14000/20625, Average Loss: 0.006119737974484451\n",
      "  Batch 15000/20625, Average Loss: 0.005976567639270797\n",
      "  Batch 16000/20625, Average Loss: 0.006058636765810661\n",
      "  Batch 17000/20625, Average Loss: 0.0060326174647780135\n",
      "  Batch 18000/20625, Average Loss: 0.0060317291168030356\n",
      "  Batch 19000/20625, Average Loss: 0.00601059156935662\n",
      "  Batch 20000/20625, Average Loss: 0.006113564216881059\n",
      "  Average Training Loss: 0.005970196021161974\n",
      "  Average Training Loss: 0.005970196021161974, Average Validation Loss: 0.007438287681369107\n",
      "  Average Test Loss: 0.007424643419118443\n",
      "Epoch 48/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005732032002415508\n",
      "  Batch 2000/20625, Average Loss: 0.005777038522879593\n",
      "  Batch 3000/20625, Average Loss: 0.005840138197876513\n",
      "  Batch 4000/20625, Average Loss: 0.006023509723017923\n",
      "  Batch 5000/20625, Average Loss: 0.005861658725654706\n",
      "  Batch 6000/20625, Average Loss: 0.005900366941932589\n",
      "  Batch 7000/20625, Average Loss: 0.005984902651282028\n",
      "  Batch 8000/20625, Average Loss: 0.005895623111631721\n",
      "  Batch 9000/20625, Average Loss: 0.006071973187383264\n",
      "  Batch 10000/20625, Average Loss: 0.005940470747184008\n",
      "  Batch 11000/20625, Average Loss: 0.005857474056887441\n",
      "  Batch 12000/20625, Average Loss: 0.005964656455442309\n",
      "  Batch 13000/20625, Average Loss: 0.005897172387689352\n",
      "  Batch 14000/20625, Average Loss: 0.005921103907749057\n",
      "  Batch 15000/20625, Average Loss: 0.005931594619294629\n",
      "  Batch 16000/20625, Average Loss: 0.00605022823321633\n",
      "  Batch 17000/20625, Average Loss: 0.005984019578201696\n",
      "  Batch 18000/20625, Average Loss: 0.006007542672101409\n",
      "  Batch 19000/20625, Average Loss: 0.005914766816888004\n",
      "  Batch 20000/20625, Average Loss: 0.0060289015599992125\n",
      "  Average Training Loss: 0.005933567877367816\n",
      "  Average Training Loss: 0.005933567877367816, Average Validation Loss: 0.007725361178052911\n",
      "  Average Test Loss: 0.007679574656012561\n",
      "Epoch 49/200: \n",
      "  Batch 1000/20625, Average Loss: 0.0057231615036726\n",
      "  Batch 2000/20625, Average Loss: 0.005774833613424562\n",
      "  Batch 3000/20625, Average Loss: 0.005709993883501738\n",
      "  Batch 4000/20625, Average Loss: 0.005791000785073265\n",
      "  Batch 5000/20625, Average Loss: 0.005893179814796895\n",
      "  Batch 6000/20625, Average Loss: 0.005873949534259736\n",
      "  Batch 7000/20625, Average Loss: 0.005981411952059716\n",
      "  Batch 8000/20625, Average Loss: 0.005866959370207042\n",
      "  Batch 9000/20625, Average Loss: 0.005958143861615099\n",
      "  Batch 10000/20625, Average Loss: 0.005920556302880868\n",
      "  Batch 11000/20625, Average Loss: 0.005909044421510771\n",
      "  Batch 12000/20625, Average Loss: 0.005831350623397157\n",
      "  Batch 13000/20625, Average Loss: 0.00591804887924809\n",
      "  Batch 14000/20625, Average Loss: 0.00603665407304652\n",
      "  Batch 15000/20625, Average Loss: 0.005956936917616985\n",
      "  Batch 16000/20625, Average Loss: 0.00605468468577601\n",
      "  Batch 17000/20625, Average Loss: 0.006038995109964162\n",
      "  Batch 18000/20625, Average Loss: 0.006000422917772084\n",
      "  Batch 19000/20625, Average Loss: 0.005831435026833787\n",
      "  Batch 20000/20625, Average Loss: 0.005833527017384768\n",
      "  Average Training Loss: 0.005896703799904296\n",
      "  Average Training Loss: 0.005896703799904296, Average Validation Loss: 0.007401177492430489\n",
      "  Average Test Loss: 0.007328924068181317\n",
      "Epoch 50/200: \n",
      "  Batch 1000/20625, Average Loss: 0.00562766362563707\n",
      "  Batch 2000/20625, Average Loss: 0.005685863909777254\n",
      "  Batch 3000/20625, Average Loss: 0.005668310619192198\n",
      "  Batch 4000/20625, Average Loss: 0.0058098807639908046\n",
      "  Batch 5000/20625, Average Loss: 0.005926542304689065\n",
      "  Batch 6000/20625, Average Loss: 0.0059320715012727305\n",
      "  Batch 7000/20625, Average Loss: 0.005762904055649415\n",
      "  Batch 8000/20625, Average Loss: 0.0058694772965973245\n",
      "  Batch 9000/20625, Average Loss: 0.005754976644879207\n",
      "  Batch 10000/20625, Average Loss: 0.005817319236462936\n",
      "  Batch 11000/20625, Average Loss: 0.005905656269402243\n",
      "  Batch 12000/20625, Average Loss: 0.0058897560322657225\n",
      "  Batch 13000/20625, Average Loss: 0.005875130112399347\n",
      "  Batch 14000/20625, Average Loss: 0.0059302457323065026\n",
      "  Batch 15000/20625, Average Loss: 0.005888434691471048\n",
      "  Batch 16000/20625, Average Loss: 0.005928190997103229\n",
      "  Batch 17000/20625, Average Loss: 0.005899913752567955\n",
      "  Batch 18000/20625, Average Loss: 0.005881092652212828\n",
      "  Batch 19000/20625, Average Loss: 0.005966825787909329\n",
      "  Batch 20000/20625, Average Loss: 0.0059618755895644426\n",
      "  Average Training Loss: 0.005852745133666604\n",
      "  Average Training Loss: 0.005852745133666604, Average Validation Loss: 0.007548292886745791\n",
      "  Average Test Loss: 0.007469588231413817\n",
      "Epoch 51/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005537743920111097\n",
      "  Batch 2000/20625, Average Loss: 0.00575305247225333\n",
      "  Batch 3000/20625, Average Loss: 0.005727370164124295\n",
      "  Batch 4000/20625, Average Loss: 0.005743753475486301\n",
      "  Batch 5000/20625, Average Loss: 0.005848051511566155\n",
      "  Batch 6000/20625, Average Loss: 0.005864145967061632\n",
      "  Batch 7000/20625, Average Loss: 0.00570173805882223\n",
      "  Batch 8000/20625, Average Loss: 0.005799581309198402\n",
      "  Batch 9000/20625, Average Loss: 0.005878961554029957\n",
      "  Batch 10000/20625, Average Loss: 0.005799858967075125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 11000/20625, Average Loss: 0.005860859931446612\n",
      "  Batch 12000/20625, Average Loss: 0.005761988535756245\n",
      "  Batch 13000/20625, Average Loss: 0.0058170402881223705\n",
      "  Batch 14000/20625, Average Loss: 0.005831986113102175\n",
      "  Batch 15000/20625, Average Loss: 0.0058164939524722285\n",
      "  Batch 16000/20625, Average Loss: 0.005926021233433857\n",
      "  Batch 17000/20625, Average Loss: 0.005916220068582333\n",
      "  Batch 18000/20625, Average Loss: 0.005904140351805836\n",
      "  Batch 19000/20625, Average Loss: 0.005916716192616149\n",
      "  Batch 20000/20625, Average Loss: 0.005916388662764803\n",
      "  Average Training Loss: 0.005820073775137126\n",
      "  Average Training Loss: 0.005820073775137126, Average Validation Loss: 0.0074586422784959595\n",
      "  Average Test Loss: 0.00741287563439507\n",
      "Epoch 52/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005578014319180511\n",
      "  Batch 2000/20625, Average Loss: 0.005679196638870053\n",
      "  Batch 3000/20625, Average Loss: 0.00570263963076286\n",
      "  Batch 4000/20625, Average Loss: 0.005651459756772965\n",
      "  Batch 5000/20625, Average Loss: 0.005633969545131549\n",
      "  Batch 6000/20625, Average Loss: 0.005683600079151802\n",
      "  Batch 7000/20625, Average Loss: 0.005785277033341117\n",
      "  Batch 8000/20625, Average Loss: 0.005744432710809633\n",
      "  Batch 9000/20625, Average Loss: 0.005749578942311927\n",
      "  Batch 10000/20625, Average Loss: 0.005768831674940884\n",
      "  Batch 11000/20625, Average Loss: 0.00584879742260091\n",
      "  Batch 12000/20625, Average Loss: 0.005846751717734151\n",
      "  Batch 13000/20625, Average Loss: 0.005878122448571958\n",
      "  Batch 14000/20625, Average Loss: 0.005818608977366239\n",
      "  Batch 15000/20625, Average Loss: 0.005782405220787041\n",
      "  Batch 16000/20625, Average Loss: 0.005958596142940223\n",
      "  Batch 17000/20625, Average Loss: 0.005838107976829633\n",
      "  Batch 18000/20625, Average Loss: 0.005931218252051622\n",
      "  Batch 19000/20625, Average Loss: 0.005822777054621838\n",
      "  Batch 20000/20625, Average Loss: 0.006008231958840042\n",
      "  Average Training Loss: 0.005789600831512926\n",
      "  Average Training Loss: 0.005789600831512926, Average Validation Loss: 0.0072840036939352225\n",
      "  Average Test Loss: 0.007227685487130612\n",
      "Epoch 53/200: \n",
      "  Batch 1000/20625, Average Loss: 0.00567160924023483\n",
      "  Batch 2000/20625, Average Loss: 0.005488303478574381\n",
      "  Batch 3000/20625, Average Loss: 0.005795651730382815\n",
      "  Batch 4000/20625, Average Loss: 0.005788702283753082\n",
      "  Batch 5000/20625, Average Loss: 0.005757670236984268\n",
      "  Batch 6000/20625, Average Loss: 0.005640704968594946\n",
      "  Batch 7000/20625, Average Loss: 0.005783326823147945\n",
      "  Batch 8000/20625, Average Loss: 0.005715962161659263\n",
      "  Batch 9000/20625, Average Loss: 0.005820392841356806\n",
      "  Batch 10000/20625, Average Loss: 0.005794273413368501\n",
      "  Batch 11000/20625, Average Loss: 0.005701206297264435\n",
      "  Batch 12000/20625, Average Loss: 0.005796738881268538\n",
      "  Batch 13000/20625, Average Loss: 0.005767038497608155\n",
      "  Batch 14000/20625, Average Loss: 0.0058187903061043475\n",
      "  Batch 15000/20625, Average Loss: 0.00580499283154495\n",
      "  Batch 16000/20625, Average Loss: 0.005857271874556318\n",
      "  Batch 17000/20625, Average Loss: 0.005807472598971799\n",
      "  Batch 18000/20625, Average Loss: 0.005787306990241632\n",
      "  Batch 19000/20625, Average Loss: 0.005687108986312524\n",
      "  Batch 20000/20625, Average Loss: 0.005890973235014826\n",
      "  Average Training Loss: 0.0057610295716165145\n",
      "  Average Training Loss: 0.0057610295716165145, Average Validation Loss: 0.007491270503537221\n",
      "  Average Test Loss: 0.007465242481785426\n",
      "Epoch 54/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005432019202737138\n",
      "  Batch 2000/20625, Average Loss: 0.005680308287614025\n",
      "  Batch 3000/20625, Average Loss: 0.005567556001478806\n",
      "  Batch 4000/20625, Average Loss: 0.005531394957215525\n",
      "  Batch 5000/20625, Average Loss: 0.0056494783627567815\n",
      "  Batch 6000/20625, Average Loss: 0.005700663091731258\n",
      "  Batch 7000/20625, Average Loss: 0.00571098117553629\n",
      "  Batch 8000/20625, Average Loss: 0.005664924687705934\n",
      "  Batch 9000/20625, Average Loss: 0.005718788089463488\n",
      "  Batch 10000/20625, Average Loss: 0.005834970363648609\n",
      "  Batch 11000/20625, Average Loss: 0.005790657215053216\n",
      "  Batch 12000/20625, Average Loss: 0.0058059033585013825\n",
      "  Batch 13000/20625, Average Loss: 0.0058259296551113945\n",
      "  Batch 14000/20625, Average Loss: 0.005861076109809801\n",
      "  Batch 15000/20625, Average Loss: 0.0057797471672529355\n",
      "  Batch 16000/20625, Average Loss: 0.0057731679859571155\n",
      "  Batch 17000/20625, Average Loss: 0.005768293061759323\n",
      "  Batch 18000/20625, Average Loss: 0.005735465199220926\n",
      "  Batch 19000/20625, Average Loss: 0.005872801772435196\n",
      "  Batch 20000/20625, Average Loss: 0.005831354525871575\n",
      "  Average Training Loss: 0.005731773744071975\n",
      "  Average Training Loss: 0.005731773744071975, Average Validation Loss: 0.0072657888325516696\n",
      "  Average Test Loss: 0.007217840892530939\n",
      "Epoch 55/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005561682610656135\n",
      "  Batch 2000/20625, Average Loss: 0.005580618782085367\n",
      "  Batch 3000/20625, Average Loss: 0.005590040678391233\n",
      "  Batch 4000/20625, Average Loss: 0.005612862311420031\n",
      "  Batch 5000/20625, Average Loss: 0.005560835890704766\n",
      "  Batch 6000/20625, Average Loss: 0.005742787658236921\n",
      "  Batch 7000/20625, Average Loss: 0.005651656520552933\n",
      "  Batch 8000/20625, Average Loss: 0.005685918933711946\n",
      "  Batch 9000/20625, Average Loss: 0.0058495547713246195\n",
      "  Batch 10000/20625, Average Loss: 0.005850004847627133\n",
      "  Batch 11000/20625, Average Loss: 0.00574216427479405\n",
      "  Batch 12000/20625, Average Loss: 0.005709003426367417\n",
      "  Batch 13000/20625, Average Loss: 0.005670185592141934\n",
      "  Batch 14000/20625, Average Loss: 0.00575457579176873\n",
      "  Batch 15000/20625, Average Loss: 0.005652877188986168\n",
      "  Batch 16000/20625, Average Loss: 0.005745137776830234\n",
      "  Batch 17000/20625, Average Loss: 0.005734188298461959\n",
      "  Batch 18000/20625, Average Loss: 0.005751086529926397\n",
      "  Batch 19000/20625, Average Loss: 0.005641461235005409\n",
      "  Batch 20000/20625, Average Loss: 0.005854316662065684\n",
      "  Average Training Loss: 0.005700762941343992\n",
      "  Average Training Loss: 0.005700762941343992, Average Validation Loss: 0.007383826958789111\n",
      "  Average Test Loss: 0.007334107054794409\n",
      "Epoch 56/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005516703097033314\n",
      "  Batch 2000/20625, Average Loss: 0.005445026578614488\n",
      "  Batch 3000/20625, Average Loss: 0.005554477519937791\n",
      "  Batch 4000/20625, Average Loss: 0.005538515776745044\n",
      "  Batch 5000/20625, Average Loss: 0.005560189467738383\n",
      "  Batch 6000/20625, Average Loss: 0.005608993305708282\n",
      "  Batch 7000/20625, Average Loss: 0.005687334777903743\n",
      "  Batch 8000/20625, Average Loss: 0.005673968705232255\n",
      "  Batch 9000/20625, Average Loss: 0.0056774663330288605\n",
      "  Batch 10000/20625, Average Loss: 0.005781176350894384\n",
      "  Batch 11000/20625, Average Loss: 0.005690173299750313\n",
      "  Batch 12000/20625, Average Loss: 0.005783220004756003\n",
      "  Batch 13000/20625, Average Loss: 0.005659813871141523\n",
      "  Batch 14000/20625, Average Loss: 0.005710851521464065\n",
      "  Batch 15000/20625, Average Loss: 0.00568401074456051\n",
      "  Batch 16000/20625, Average Loss: 0.005808926887693815\n",
      "  Batch 17000/20625, Average Loss: 0.005715651123551652\n",
      "  Batch 18000/20625, Average Loss: 0.00579697726177983\n",
      "  Batch 19000/20625, Average Loss: 0.005701608557486907\n",
      "  Batch 20000/20625, Average Loss: 0.005786080531193874\n",
      "  Average Training Loss: 0.00567199105345503\n",
      "  Average Training Loss: 0.00567199105345503, Average Validation Loss: 0.007432591961673266\n",
      "  Average Test Loss: 0.007385235870430751\n",
      "Epoch 57/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005488640722469427\n",
      "  Batch 2000/20625, Average Loss: 0.005465151795651764\n",
      "  Batch 3000/20625, Average Loss: 0.005584275650093332\n",
      "  Batch 4000/20625, Average Loss: 0.005689530611271039\n",
      "  Batch 5000/20625, Average Loss: 0.005608325676061213\n",
      "  Batch 6000/20625, Average Loss: 0.0055699241211405025\n",
      "  Batch 7000/20625, Average Loss: 0.005627722226316109\n",
      "  Batch 8000/20625, Average Loss: 0.005575765971560031\n",
      "  Batch 9000/20625, Average Loss: 0.0055942568249301985\n",
      "  Batch 10000/20625, Average Loss: 0.005644501938484609\n",
      "  Batch 11000/20625, Average Loss: 0.005646336715901271\n",
      "  Batch 12000/20625, Average Loss: 0.0056337448818376285\n",
      "  Batch 13000/20625, Average Loss: 0.005734477252466604\n",
      "  Batch 14000/20625, Average Loss: 0.005711419031373225\n",
      "  Batch 15000/20625, Average Loss: 0.005659881046507507\n",
      "  Batch 16000/20625, Average Loss: 0.005703846574993804\n",
      "  Batch 17000/20625, Average Loss: 0.0056523822363233195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 18000/20625, Average Loss: 0.005641113119898364\n",
      "  Batch 19000/20625, Average Loss: 0.00572334521706216\n",
      "  Batch 20000/20625, Average Loss: 0.005742225496796891\n",
      "  Average Training Loss: 0.005638069389015436\n",
      "  Average Training Loss: 0.005638069389015436, Average Validation Loss: 0.007244367684394276\n",
      "  Average Test Loss: 0.00717481147355765\n",
      "Epoch 58/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005384328035870567\n",
      "  Batch 2000/20625, Average Loss: 0.005387878873152658\n",
      "  Batch 3000/20625, Average Loss: 0.0055319457632722335\n",
      "  Batch 4000/20625, Average Loss: 0.005493215206777677\n",
      "  Batch 5000/20625, Average Loss: 0.005524421532754786\n",
      "  Batch 6000/20625, Average Loss: 0.0056019576193066314\n",
      "  Batch 7000/20625, Average Loss: 0.005630331729887985\n",
      "  Batch 8000/20625, Average Loss: 0.005537737490725703\n",
      "  Batch 9000/20625, Average Loss: 0.005682523843483068\n",
      "  Batch 10000/20625, Average Loss: 0.005610956571530551\n",
      "  Batch 11000/20625, Average Loss: 0.00563193802954629\n",
      "  Batch 12000/20625, Average Loss: 0.005720817269990221\n",
      "  Batch 13000/20625, Average Loss: 0.005590345659293235\n",
      "  Batch 14000/20625, Average Loss: 0.005655172357335687\n",
      "  Batch 15000/20625, Average Loss: 0.005691814262070693\n",
      "  Batch 16000/20625, Average Loss: 0.005698195608681999\n",
      "  Batch 17000/20625, Average Loss: 0.0056085946572711695\n",
      "  Batch 18000/20625, Average Loss: 0.005799259502789937\n",
      "  Batch 19000/20625, Average Loss: 0.005668600089731626\n",
      "  Batch 20000/20625, Average Loss: 0.005605640389374457\n",
      "  Average Training Loss: 0.005607048792662946\n",
      "  Average Training Loss: 0.005607048792662946, Average Validation Loss: 0.007382996552694604\n",
      "  Average Test Loss: 0.007339795251668225\n",
      "Epoch 59/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005452533158590086\n",
      "  Batch 2000/20625, Average Loss: 0.005450913461158052\n",
      "  Batch 3000/20625, Average Loss: 0.005515909485751763\n",
      "  Batch 4000/20625, Average Loss: 0.005545772761804983\n",
      "  Batch 5000/20625, Average Loss: 0.005554408389842138\n",
      "  Batch 6000/20625, Average Loss: 0.005568177395150997\n",
      "  Batch 7000/20625, Average Loss: 0.005590987912961282\n",
      "  Batch 8000/20625, Average Loss: 0.00548116922203917\n",
      "  Batch 9000/20625, Average Loss: 0.005576864280505106\n",
      "  Batch 10000/20625, Average Loss: 0.005743265145923942\n",
      "  Batch 11000/20625, Average Loss: 0.005584819194162265\n",
      "  Batch 12000/20625, Average Loss: 0.005606960546225309\n",
      "  Batch 13000/20625, Average Loss: 0.005680415851064026\n",
      "  Batch 14000/20625, Average Loss: 0.005736531229456886\n",
      "  Batch 15000/20625, Average Loss: 0.005683614890323952\n",
      "  Batch 16000/20625, Average Loss: 0.0055264449923997745\n",
      "  Batch 17000/20625, Average Loss: 0.005713741108425893\n",
      "  Batch 18000/20625, Average Loss: 0.005675925963558257\n",
      "  Batch 19000/20625, Average Loss: 0.005530353444628417\n",
      "  Batch 20000/20625, Average Loss: 0.005592782449442894\n",
      "  Average Training Loss: 0.005595312714339658\n",
      "  Average Training Loss: 0.005595312714339658, Average Validation Loss: 0.007475761696262839\n",
      "  Average Test Loss: 0.007428229084351823\n",
      "Epoch 60/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005343638022895903\n",
      "  Batch 2000/20625, Average Loss: 0.005432757487287745\n",
      "  Batch 3000/20625, Average Loss: 0.005454099839436822\n",
      "  Batch 4000/20625, Average Loss: 0.0056108076944947245\n",
      "  Batch 5000/20625, Average Loss: 0.005538634985685348\n",
      "  Batch 6000/20625, Average Loss: 0.005537529344554059\n",
      "  Batch 7000/20625, Average Loss: 0.005480721390689723\n",
      "  Batch 8000/20625, Average Loss: 0.005589224291034043\n",
      "  Batch 9000/20625, Average Loss: 0.00552802552562207\n",
      "  Batch 10000/20625, Average Loss: 0.0054959832187742\n",
      "  Batch 11000/20625, Average Loss: 0.005593095416785218\n",
      "  Batch 12000/20625, Average Loss: 0.005629092431161552\n",
      "  Batch 13000/20625, Average Loss: 0.005639112937147729\n",
      "  Batch 14000/20625, Average Loss: 0.005680366712971591\n",
      "  Batch 15000/20625, Average Loss: 0.00561381520726718\n",
      "  Batch 16000/20625, Average Loss: 0.005613274616189301\n",
      "  Batch 17000/20625, Average Loss: 0.005603929604869336\n",
      "  Batch 18000/20625, Average Loss: 0.005721731432946399\n",
      "  Batch 19000/20625, Average Loss: 0.005584303761250339\n",
      "  Batch 20000/20625, Average Loss: 0.005600866129505448\n",
      "  Average Training Loss: 0.0055702705348531405\n",
      "  Average Training Loss: 0.0055702705348531405, Average Validation Loss: 0.007186765180809092\n",
      "  Average Test Loss: 0.0071515538818444\n",
      "Epoch 61/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005317542539909482\n",
      "  Batch 2000/20625, Average Loss: 0.005445693731773645\n",
      "  Batch 3000/20625, Average Loss: 0.005405130864935927\n",
      "  Batch 4000/20625, Average Loss: 0.00551248359773308\n",
      "  Batch 5000/20625, Average Loss: 0.005364504640339874\n",
      "  Batch 6000/20625, Average Loss: 0.005434300346300006\n",
      "  Batch 7000/20625, Average Loss: 0.005646604820387438\n",
      "  Batch 8000/20625, Average Loss: 0.005627276043058373\n",
      "  Batch 9000/20625, Average Loss: 0.005509528713999316\n",
      "  Batch 10000/20625, Average Loss: 0.0057510247111786155\n",
      "  Batch 11000/20625, Average Loss: 0.005586952940793708\n",
      "  Batch 12000/20625, Average Loss: 0.0055437705696094785\n",
      "  Batch 13000/20625, Average Loss: 0.005615417037392035\n",
      "  Batch 14000/20625, Average Loss: 0.005502063716994599\n",
      "  Batch 15000/20625, Average Loss: 0.00551587636466138\n",
      "  Batch 16000/20625, Average Loss: 0.005529658961226232\n",
      "  Batch 17000/20625, Average Loss: 0.005666549750370905\n",
      "  Batch 18000/20625, Average Loss: 0.005608603480271995\n",
      "  Batch 19000/20625, Average Loss: 0.005593222737661563\n",
      "  Batch 20000/20625, Average Loss: 0.005639560062787496\n",
      "  Average Training Loss: 0.005544122593511234\n",
      "  Average Training Loss: 0.005544122593511234, Average Validation Loss: 0.007244342503864913\n",
      "  Average Test Loss: 0.007227332707168886\n",
      "Epoch 62/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005320425728452392\n",
      "  Batch 2000/20625, Average Loss: 0.005334380381507799\n",
      "  Batch 3000/20625, Average Loss: 0.005410626812954433\n",
      "  Batch 4000/20625, Average Loss: 0.005431660086149349\n",
      "  Batch 5000/20625, Average Loss: 0.005596854535280727\n",
      "  Batch 6000/20625, Average Loss: 0.005505817853845656\n",
      "  Batch 7000/20625, Average Loss: 0.0053627906296169385\n",
      "  Batch 8000/20625, Average Loss: 0.005539796449709684\n",
      "  Batch 9000/20625, Average Loss: 0.005466553032281808\n",
      "  Batch 10000/20625, Average Loss: 0.005498571718810126\n",
      "  Batch 11000/20625, Average Loss: 0.005528115828870796\n",
      "  Batch 12000/20625, Average Loss: 0.005657867402769625\n",
      "  Batch 13000/20625, Average Loss: 0.005581562081584707\n",
      "  Batch 14000/20625, Average Loss: 0.005569679455598817\n",
      "  Batch 15000/20625, Average Loss: 0.005583758684922941\n",
      "  Batch 16000/20625, Average Loss: 0.00560555796069093\n",
      "  Batch 17000/20625, Average Loss: 0.005593493688385934\n",
      "  Batch 18000/20625, Average Loss: 0.0056461612556595354\n",
      "  Batch 19000/20625, Average Loss: 0.00562555901077576\n",
      "  Batch 20000/20625, Average Loss: 0.005550753949442879\n",
      "  Average Training Loss: 0.00552253839946493\n",
      "  Average Training Loss: 0.00552253839946493, Average Validation Loss: 0.007293455970252243\n",
      "  Average Test Loss: 0.007300663970873866\n",
      "Epoch 63/200: \n",
      "  Batch 1000/20625, Average Loss: 0.005357398689142428\n",
      "  Batch 2000/20625, Average Loss: 0.0053319212587084625\n",
      "  Batch 3000/20625, Average Loss: 0.0053890063187573105\n",
      "  Batch 4000/20625, Average Loss: 0.005401165739400312\n",
      "  Batch 5000/20625, Average Loss: 0.005485482604126446\n",
      "  Batch 6000/20625, Average Loss: 0.005389053921331651\n",
      "  Batch 7000/20625, Average Loss: 0.0054933170067379255\n",
      "  Batch 8000/20625, Average Loss: 0.005438809694373049\n",
      "  Batch 9000/20625, Average Loss: 0.00552745437703561\n",
      "  Batch 10000/20625, Average Loss: 0.005534666235675104\n",
      "  Batch 11000/20625, Average Loss: 0.0054921921022469174\n",
      "  Batch 12000/20625, Average Loss: 0.005586168855777941\n",
      "  Batch 13000/20625, Average Loss: 0.005544697835692204\n",
      "  Batch 14000/20625, Average Loss: 0.005484524685773067\n",
      "  Batch 15000/20625, Average Loss: 0.005680868572439067\n",
      "  Batch 16000/20625, Average Loss: 0.005498543559457176\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Jianfeng Yue\\AppData\\Local\\Temp\\ipykernel_15696\\3882745051.py\", line 1, in <module>\n",
      "    result = experiment_one(datasetC, datasetB, hidden_layers_structure = [[256,128,64,32]], num_epochs=200, type_name=\"C+B/C+B\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\OneDrive\\Desktop\\Exercises-MLCMS-Group-C\\Exercise-6\\utils\\experiment.py\", line 72, in experiment_one\n",
      "    train_and_test(\"C+B/C+B\", datasetC, datasetB, hidden_layers_structure, num_epochs)\n",
      "  File \"C:\\Users\\Jianfeng Yue\\OneDrive\\Desktop\\Exercises-MLCMS-Group-C\\Exercise-6\\utils\\experiment.py\", line 141, in train_and_test\n",
      "  File \"C:\\Users\\Jianfeng Yue\\OneDrive\\Desktop\\Exercises-MLCMS-Group-C\\Exercise-6\\utils\\experiment.py\", line 178, in train\n",
      "    # Move data to GPU and ensure consistent data types\n",
      "                                                    ^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2120, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\Jianfeng Yue\\anaconda3\\Lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "result = experiment_one(datasetC, datasetB, hidden_layers_structure = [[256,128,64,32]], num_epochs=200, type_name=\"C+B/C+B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9151f2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment C+B/B： 1/1  ---   Model: [5, 3]\n",
      "Epoch 1/10: \n",
      "  Batch 1000/5276, Average Loss: 0.1616691890247166\n",
      "  Batch 2000/5276, Average Loss: 0.10822822946310043\n",
      "  Batch 3000/5276, Average Loss: 0.07811494439095258\n",
      "  Batch 4000/5276, Average Loss: 0.04857494084443897\n",
      "  Batch 5000/5276, Average Loss: 0.044780291294679045\n",
      "  Average Training Loss: 0.08597857667111324\n",
      "  Average Training Loss: 0.08597857667111324, Average Validation Loss: 0.04290902728402833\n",
      "Epoch 2/10: \n",
      "  Batch 1000/5276, Average Loss: 0.04181843220908195\n",
      "  Batch 2000/5276, Average Loss: 0.04161665034852922\n",
      "  Batch 3000/5276, Average Loss: 0.040232873553410174\n",
      "  Batch 4000/5276, Average Loss: 0.04030068117659539\n",
      "  Batch 5000/5276, Average Loss: 0.03949101995117962\n",
      "  Average Training Loss: 0.040608727282098155\n",
      "  Average Training Loss: 0.040608727282098155, Average Validation Loss: 0.03989688701837559\n",
      "Epoch 3/10: \n",
      "  Batch 1000/5276, Average Loss: 0.039061731761321425\n",
      "  Batch 2000/5276, Average Loss: 0.038921914475969974\n",
      "  Batch 3000/5276, Average Loss: 0.03861303032003343\n",
      "  Batch 4000/5276, Average Loss: 0.03733795934729278\n",
      "  Batch 5000/5276, Average Loss: 0.036632307332009076\n",
      "  Average Training Loss: 0.0380148469467617\n",
      "  Average Training Loss: 0.0380148469467617, Average Validation Loss: 0.036366973836145816\n",
      "Epoch 4/10: \n",
      "  Batch 1000/5276, Average Loss: 0.036023482942953704\n",
      "  Batch 2000/5276, Average Loss: 0.03536814274732023\n",
      "  Batch 3000/5276, Average Loss: 0.03486449822876602\n",
      "  Batch 4000/5276, Average Loss: 0.034946140015497805\n",
      "  Batch 5000/5276, Average Loss: 0.03417396639380604\n",
      "  Average Training Loss: 0.035002878736994085\n",
      "  Average Training Loss: 0.035002878736994085, Average Validation Loss: 0.03456827448906254\n",
      "Epoch 5/10: \n",
      "  Batch 1000/5276, Average Loss: 0.03430130780767649\n",
      "  Batch 2000/5276, Average Loss: 0.034366234821267425\n",
      "  Batch 3000/5276, Average Loss: 0.03305104097165167\n",
      "  Batch 4000/5276, Average Loss: 0.0335094301449135\n",
      "  Batch 5000/5276, Average Loss: 0.03328393582534045\n",
      "  Average Training Loss: 0.033709302173547555\n",
      "  Average Training Loss: 0.033709302173547555, Average Validation Loss: 0.033639744240744\n",
      "Epoch 6/10: \n",
      "  Batch 1000/5276, Average Loss: 0.033026794608682394\n",
      "  Batch 2000/5276, Average Loss: 0.03306450876127928\n",
      "  Batch 3000/5276, Average Loss: 0.03293759950622916\n",
      "  Batch 4000/5276, Average Loss: 0.03288208278827369\n",
      "  Batch 5000/5276, Average Loss: 0.03306028103083372\n",
      "  Average Training Loss: 0.033027108941386774\n",
      "  Average Training Loss: 0.033027108941386774, Average Validation Loss: 0.032978044017167564\n",
      "Epoch 7/10: \n",
      "  Batch 1000/5276, Average Loss: 0.032659720811992886\n",
      "  Batch 2000/5276, Average Loss: 0.033034565350972114\n",
      "  Batch 3000/5276, Average Loss: 0.03216641263384372\n",
      "  Batch 4000/5276, Average Loss: 0.03205240433756262\n",
      "  Batch 5000/5276, Average Loss: 0.03237525630556047\n",
      "  Average Training Loss: 0.0324544425578216\n",
      "  Average Training Loss: 0.0324544425578216, Average Validation Loss: 0.032402171941941685\n",
      "Epoch 8/10: \n",
      "  Batch 1000/5276, Average Loss: 0.03254377412982285\n",
      "  Batch 2000/5276, Average Loss: 0.03187680215202272\n",
      "  Batch 3000/5276, Average Loss: 0.0320794120663777\n",
      "  Batch 4000/5276, Average Loss: 0.03186824834439903\n",
      "  Batch 5000/5276, Average Loss: 0.03134257167391479\n",
      "  Average Training Loss: 0.03191855419787258\n",
      "  Average Training Loss: 0.03191855419787258, Average Validation Loss: 0.031690689504886044\n",
      "Epoch 9/10: \n",
      "  Batch 1000/5276, Average Loss: 0.031698076873086393\n",
      "  Batch 2000/5276, Average Loss: 0.031208262830972673\n",
      "  Batch 3000/5276, Average Loss: 0.030718833766877653\n",
      "  Batch 4000/5276, Average Loss: 0.03093619414791465\n",
      "  Batch 5000/5276, Average Loss: 0.030971497721038757\n",
      "  Average Training Loss: 0.03105281885654028\n",
      "  Average Training Loss: 0.03105281885654028, Average Validation Loss: 0.031023592326488717\n",
      "Epoch 10/10: \n",
      "  Batch 1000/5276, Average Loss: 0.030650770546868444\n",
      "  Batch 2000/5276, Average Loss: 0.030005605608224867\n",
      "  Batch 3000/5276, Average Loss: 0.030543766126967967\n",
      "  Batch 4000/5276, Average Loss: 0.030396705310791732\n",
      "  Batch 5000/5276, Average Loss: 0.030730067101307212\n",
      "  Average Training Loss: 0.030454663852642068\n",
      "  Average Training Loss: 0.030454663852642068, Average Validation Loss: 0.030518079458349837\n",
      "  Average Test Loss: 0.05859186446218095\n"
     ]
    }
   ],
   "source": [
    "result = experiment_one(datasetC, datasetB, hidden_layers_structure = [[5,3]], num_epochs=10, type_name=\"C+B/B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "088aaa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment C+B/B： 1/1  ---   Model: [5, 3]\n",
      "Epoch 1/20: \n",
      "  Batch 1000/5276, Average Loss: 0.1298086926341057\n",
      "  Batch 2000/5276, Average Loss: 0.1036851458325982\n",
      "  Batch 3000/5276, Average Loss: 0.06891325598210096\n",
      "  Batch 4000/5276, Average Loss: 0.04675499512627721\n",
      "  Batch 5000/5276, Average Loss: 0.03964708896633238\n",
      "  Average Training Loss: 0.07572685244282774\n",
      "  Average Training Loss: 0.07572685244282774, Average Validation Loss: 0.038495624576811\n",
      "Epoch 2/20: \n",
      "  Batch 1000/5276, Average Loss: 0.037528816320933404\n",
      "  Batch 2000/5276, Average Loss: 0.03658011273480952\n",
      "  Batch 3000/5276, Average Loss: 0.03600835215952247\n",
      "  Batch 4000/5276, Average Loss: 0.035459113895893096\n",
      "  Batch 5000/5276, Average Loss: 0.03589820601325482\n",
      "  Average Training Loss: 0.03629255444418648\n",
      "  Average Training Loss: 0.03629255444418648, Average Validation Loss: 0.03563875876385734\n",
      "Epoch 3/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03546649936400354\n",
      "  Batch 2000/5276, Average Loss: 0.03479205879475921\n",
      "  Batch 3000/5276, Average Loss: 0.03459453060198575\n",
      "  Batch 4000/5276, Average Loss: 0.034532431547529996\n",
      "  Batch 5000/5276, Average Loss: 0.03443428488913924\n",
      "  Average Training Loss: 0.03479203116025146\n",
      "  Average Training Loss: 0.03479203116025146, Average Validation Loss: 0.03456197769487521\n",
      "Epoch 4/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03403332093730569\n",
      "  Batch 2000/5276, Average Loss: 0.034480194658041\n",
      "  Batch 3000/5276, Average Loss: 0.034099509133957324\n",
      "  Batch 4000/5276, Average Loss: 0.03362734361551702\n",
      "  Batch 5000/5276, Average Loss: 0.03301587749924511\n",
      "  Average Training Loss: 0.03385111406569774\n",
      "  Average Training Loss: 0.03385111406569774, Average Validation Loss: 0.03378938142990102\n",
      "Epoch 5/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03363284044759348\n",
      "  Batch 2000/5276, Average Loss: 0.03386162708606571\n",
      "  Batch 3000/5276, Average Loss: 0.03261065605934709\n",
      "  Batch 4000/5276, Average Loss: 0.03355309980269521\n",
      "  Batch 5000/5276, Average Loss: 0.033058466757647696\n",
      "  Average Training Loss: 0.03332102404110532\n",
      "  Average Training Loss: 0.03332102404110532, Average Validation Loss: 0.033387715394163596\n",
      "Epoch 6/20: \n",
      "  Batch 1000/5276, Average Loss: 0.033004485654644665\n",
      "  Batch 2000/5276, Average Loss: 0.033163718058727684\n",
      "  Batch 3000/5276, Average Loss: 0.03286748583894223\n",
      "  Batch 4000/5276, Average Loss: 0.033188362351618705\n",
      "  Batch 5000/5276, Average Loss: 0.03302439112309367\n",
      "  Average Training Loss: 0.03303881386969976\n",
      "  Average Training Loss: 0.03303881386969976, Average Validation Loss: 0.03352592969491916\n",
      "Epoch 7/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03299653697479516\n",
      "  Batch 2000/5276, Average Loss: 0.032584169655106965\n",
      "  Batch 3000/5276, Average Loss: 0.03301620029471815\n",
      "  Batch 4000/5276, Average Loss: 0.03272312527801841\n",
      "  Batch 5000/5276, Average Loss: 0.03283198483753949\n",
      "  Average Training Loss: 0.032855265757769425\n",
      "  Average Training Loss: 0.032855265757769425, Average Validation Loss: 0.03299512268433206\n",
      "Epoch 8/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03344747459795326\n",
      "  Batch 2000/5276, Average Loss: 0.03244976028241217\n",
      "  Batch 3000/5276, Average Loss: 0.0325568289058283\n",
      "  Batch 4000/5276, Average Loss: 0.032449579239822925\n",
      "  Batch 5000/5276, Average Loss: 0.032593815571628514\n",
      "  Average Training Loss: 0.032687595318699254\n",
      "  Average Training Loss: 0.032687595318699254, Average Validation Loss: 0.03285332980695444\n",
      "Epoch 9/20: \n",
      "  Batch 1000/5276, Average Loss: 0.032001902393996716\n",
      "  Batch 2000/5276, Average Loss: 0.03288145419117063\n",
      "  Batch 3000/5276, Average Loss: 0.03243246029969305\n",
      "  Batch 4000/5276, Average Loss: 0.032790555694140494\n",
      "  Batch 5000/5276, Average Loss: 0.03229978980589658\n",
      "  Average Training Loss: 0.03252493942027927\n",
      "  Average Training Loss: 0.03252493942027927, Average Validation Loss: 0.032841890690237706\n",
      "Epoch 10/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03225493921060115\n",
      "  Batch 2000/5276, Average Loss: 0.03228129419777542\n",
      "  Batch 3000/5276, Average Loss: 0.03264914904441685\n",
      "  Batch 4000/5276, Average Loss: 0.03302712085098028\n",
      "  Batch 5000/5276, Average Loss: 0.03195423975028098\n",
      "  Average Training Loss: 0.032400584499188576\n",
      "  Average Training Loss: 0.032400584499188576, Average Validation Loss: 0.03262083024746607\n",
      "Epoch 11/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03264114051125944\n",
      "  Batch 2000/5276, Average Loss: 0.03244496991485357\n",
      "  Batch 3000/5276, Average Loss: 0.032141785765066745\n",
      "  Batch 4000/5276, Average Loss: 0.032413077044300735\n",
      "  Batch 5000/5276, Average Loss: 0.032109867609106\n",
      "  Average Training Loss: 0.03225981308429038\n",
      "  Average Training Loss: 0.03225981308429038, Average Validation Loss: 0.03246324710886587\n",
      "Epoch 12/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03300243382062763\n",
      "  Batch 2000/5276, Average Loss: 0.032324711692519485\n",
      "  Batch 3000/5276, Average Loss: 0.031767520369030536\n",
      "  Batch 4000/5276, Average Loss: 0.03183355442341417\n",
      "  Batch 5000/5276, Average Loss: 0.03156037810165435\n",
      "  Average Training Loss: 0.03211844559399506\n",
      "  Average Training Loss: 0.03211844559399506, Average Validation Loss: 0.03252478535873528\n",
      "Epoch 13/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03137330113071948\n",
      "  Batch 2000/5276, Average Loss: 0.032646612216718496\n",
      "  Batch 3000/5276, Average Loss: 0.032018298416398465\n",
      "  Batch 4000/5276, Average Loss: 0.03198862133733928\n",
      "  Batch 5000/5276, Average Loss: 0.031997185764834284\n",
      "  Average Training Loss: 0.03198176559990722\n",
      "  Average Training Loss: 0.03198176559990722, Average Validation Loss: 0.032209862775563126\n",
      "Epoch 14/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03171759088523686\n",
      "  Batch 2000/5276, Average Loss: 0.031522189177572726\n",
      "  Batch 3000/5276, Average Loss: 0.031654499997384844\n",
      "  Batch 4000/5276, Average Loss: 0.03186809524241835\n",
      "  Batch 5000/5276, Average Loss: 0.03215033882111311\n",
      "  Average Training Loss: 0.03181193354101802\n",
      "  Average Training Loss: 0.03181193354101802, Average Validation Loss: 0.032130664726835775\n",
      "Epoch 15/20: \n",
      "  Batch 1000/5276, Average Loss: 0.031619729100726544\n",
      "  Batch 2000/5276, Average Loss: 0.03182235722383484\n",
      "  Batch 3000/5276, Average Loss: 0.03165981594473124\n",
      "  Batch 4000/5276, Average Loss: 0.03172501113824546\n",
      "  Batch 5000/5276, Average Loss: 0.031570519293658436\n",
      "  Average Training Loss: 0.03169261292126199\n",
      "  Average Training Loss: 0.03169261292126199, Average Validation Loss: 0.03205627148288975\n",
      "Epoch 16/20: \n",
      "  Batch 1000/5276, Average Loss: 0.031119274948723616\n",
      "  Batch 2000/5276, Average Loss: 0.03172210975270719\n",
      "  Batch 3000/5276, Average Loss: 0.03205939891375601\n",
      "  Batch 4000/5276, Average Loss: 0.03136862246412784\n",
      "  Batch 5000/5276, Average Loss: 0.03160165508463979\n",
      "  Average Training Loss: 0.03159570926006028\n",
      "  Average Training Loss: 0.03159570926006028, Average Validation Loss: 0.03187932650293779\n",
      "Epoch 17/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03157278288155794\n",
      "  Batch 2000/5276, Average Loss: 0.031239756661001594\n",
      "  Batch 3000/5276, Average Loss: 0.031482427639886734\n",
      "  Batch 4000/5276, Average Loss: 0.03174086985178292\n",
      "  Batch 5000/5276, Average Loss: 0.0315826277770102\n",
      "  Average Training Loss: 0.03153870202284278\n",
      "  Average Training Loss: 0.03153870202284278, Average Validation Loss: 0.03176025006635809\n",
      "Epoch 18/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03195953261014074\n",
      "  Batch 2000/5276, Average Loss: 0.03137291947100312\n",
      "  Batch 3000/5276, Average Loss: 0.031407374314032496\n",
      "  Batch 4000/5276, Average Loss: 0.031521135402843355\n",
      "  Batch 5000/5276, Average Loss: 0.03131623045075685\n",
      "  Average Training Loss: 0.03148043923199295\n",
      "  Average Training Loss: 0.03148043923199295, Average Validation Loss: 0.03180278899696492\n",
      "Epoch 19/20: \n",
      "  Batch 1000/5276, Average Loss: 0.0322908678650856\n",
      "  Batch 2000/5276, Average Loss: 0.030891832209192217\n",
      "  Batch 3000/5276, Average Loss: 0.03094832746591419\n",
      "  Batch 4000/5276, Average Loss: 0.031247132992371918\n",
      "  Batch 5000/5276, Average Loss: 0.03175767929200083\n",
      "  Average Training Loss: 0.03142499376298087\n",
      "  Average Training Loss: 0.03142499376298087, Average Validation Loss: 0.03177878497303576\n",
      "Epoch 20/20: \n",
      "  Batch 1000/5276, Average Loss: 0.03137730111274868\n",
      "  Batch 2000/5276, Average Loss: 0.03160305658169091\n",
      "  Batch 3000/5276, Average Loss: 0.03129666267242283\n",
      "  Batch 4000/5276, Average Loss: 0.030870031024329364\n",
      "  Batch 5000/5276, Average Loss: 0.03183120160829276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Average Training Loss: 0.031373129807373085\n",
      "  Average Training Loss: 0.031373129807373085, Average Validation Loss: 0.03171121100130009\n",
      "  Average Test Loss: 0.06329686171499281\n"
     ]
    }
   ],
   "source": [
    "result = experiment_one(datasetC, datasetB, hidden_layers_structure = [[5,3]], num_epochs=20, type_name=\"C+B/B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cc1b7476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment C+B/B： 1/1  ---   Model: [5, 3]\n",
      "Epoch 1/20: \n",
      "  Batch 1000/23979, Average Loss: 0.12847054001688957\n",
      "  Batch 2000/23979, Average Loss: 0.10016623606160283\n",
      "  Batch 3000/23979, Average Loss: 0.058420247328467664\n",
      "  Batch 4000/23979, Average Loss: 0.04544006202183664\n",
      "  Batch 5000/23979, Average Loss: 0.04161326500400901\n",
      "  Batch 6000/23979, Average Loss: 0.039837209679186344\n",
      "  Batch 7000/23979, Average Loss: 0.03845005033351481\n",
      "  Batch 8000/23979, Average Loss: 0.037889874906279146\n",
      "  Batch 9000/23979, Average Loss: 0.0372465218231082\n",
      "  Batch 10000/23979, Average Loss: 0.036355648196302354\n",
      "  Batch 11000/23979, Average Loss: 0.03660828479845077\n",
      "  Batch 12000/23979, Average Loss: 0.03611533244606108\n",
      "  Batch 13000/23979, Average Loss: 0.03643134461436421\n",
      "  Batch 14000/23979, Average Loss: 0.03578156047873199\n",
      "  Batch 15000/23979, Average Loss: 0.03587418440077454\n",
      "  Batch 16000/23979, Average Loss: 0.035095549819990995\n",
      "  Batch 17000/23979, Average Loss: 0.035170454065315424\n",
      "  Batch 18000/23979, Average Loss: 0.035028973407112064\n",
      "  Batch 19000/23979, Average Loss: 0.034313821015879514\n",
      "  Batch 20000/23979, Average Loss: 0.034684752131812276\n",
      "  Batch 21000/23979, Average Loss: 0.03513081537373364\n",
      "  Batch 22000/23979, Average Loss: 0.034458874851465225\n",
      "  Batch 23000/23979, Average Loss: 0.03439406149368733\n",
      "  Average Training Loss: 0.04406641567185496\n",
      "  Average Training Loss: 0.04406641567185496, Average Validation Loss: 0.034814132667698894\n",
      "Epoch 2/20: \n",
      "  Batch 1000/23979, Average Loss: 0.033398984733968975\n",
      "  Batch 2000/23979, Average Loss: 0.0343236422771588\n",
      "  Batch 3000/23979, Average Loss: 0.03337321925535798\n",
      "  Batch 4000/23979, Average Loss: 0.03402976003848016\n",
      "  Batch 5000/23979, Average Loss: 0.033711673382669685\n",
      "  Batch 6000/23979, Average Loss: 0.03359100985620171\n",
      "  Batch 7000/23979, Average Loss: 0.03412787858210504\n",
      "  Batch 8000/23979, Average Loss: 0.033451178052462636\n",
      "  Batch 9000/23979, Average Loss: 0.033674446486867966\n",
      "  Batch 10000/23979, Average Loss: 0.03387657683994621\n",
      "  Batch 11000/23979, Average Loss: 0.033566664644517005\n",
      "  Batch 12000/23979, Average Loss: 0.033267348561435935\n",
      "  Batch 13000/23979, Average Loss: 0.033621922518126665\n",
      "  Batch 14000/23979, Average Loss: 0.03337307387031615\n",
      "  Batch 15000/23979, Average Loss: 0.03292652919422835\n",
      "  Batch 16000/23979, Average Loss: 0.0329429575111717\n",
      "  Batch 17000/23979, Average Loss: 0.03314718844834715\n",
      "  Batch 18000/23979, Average Loss: 0.0331662477189675\n",
      "  Batch 19000/23979, Average Loss: 0.033040090650320056\n",
      "  Batch 20000/23979, Average Loss: 0.03287542212754488\n",
      "  Batch 21000/23979, Average Loss: 0.03311458926275373\n",
      "  Batch 22000/23979, Average Loss: 0.03277016011718661\n",
      "  Batch 23000/23979, Average Loss: 0.03228224429301917\n",
      "  Average Training Loss: 0.03336259884509427\n",
      "  Average Training Loss: 0.03336259884509427, Average Validation Loss: 0.03281178372361394\n",
      "Epoch 3/20: \n",
      "  Batch 1000/23979, Average Loss: 0.032770790642127394\n",
      "  Batch 2000/23979, Average Loss: 0.03229654767923057\n",
      "  Batch 3000/23979, Average Loss: 0.032604941132478416\n",
      "  Batch 4000/23979, Average Loss: 0.03230468362942338\n",
      "  Batch 5000/23979, Average Loss: 0.03249123313371092\n",
      "  Batch 6000/23979, Average Loss: 0.033053686004132034\n",
      "  Batch 7000/23979, Average Loss: 0.032045902449637655\n",
      "  Batch 8000/23979, Average Loss: 0.0320115722194314\n",
      "  Batch 9000/23979, Average Loss: 0.032653474683873356\n",
      "  Batch 10000/23979, Average Loss: 0.03286433758027851\n",
      "  Batch 11000/23979, Average Loss: 0.0323560979925096\n",
      "  Batch 12000/23979, Average Loss: 0.03248389294557273\n",
      "  Batch 13000/23979, Average Loss: 0.03223924775328487\n",
      "  Batch 14000/23979, Average Loss: 0.03216203286871314\n",
      "  Batch 15000/23979, Average Loss: 0.03229660539701581\n",
      "  Batch 16000/23979, Average Loss: 0.032464137994684276\n",
      "  Batch 17000/23979, Average Loss: 0.03194790338072926\n",
      "  Batch 18000/23979, Average Loss: 0.03220230512879789\n",
      "  Batch 19000/23979, Average Loss: 0.03226988777518272\n",
      "  Batch 20000/23979, Average Loss: 0.03247759134043008\n",
      "  Batch 21000/23979, Average Loss: 0.03166283695399761\n",
      "  Batch 22000/23979, Average Loss: 0.03179533233307302\n",
      "  Batch 23000/23979, Average Loss: 0.0323422046052292\n",
      "  Average Training Loss: 0.032313550206526494\n",
      "  Average Training Loss: 0.032313550206526494, Average Validation Loss: 0.03223188457935986\n",
      "Epoch 4/20: \n",
      "  Batch 1000/23979, Average Loss: 0.03167200412694365\n",
      "  Batch 2000/23979, Average Loss: 0.03183221212681383\n",
      "  Batch 3000/23979, Average Loss: 0.03189952582120895\n",
      "  Batch 4000/23979, Average Loss: 0.03172158102225512\n",
      "  Batch 5000/23979, Average Loss: 0.031507039303891364\n",
      "  Batch 6000/23979, Average Loss: 0.03189193738251925\n",
      "  Batch 7000/23979, Average Loss: 0.03179896906763315\n",
      "  Batch 8000/23979, Average Loss: 0.031577497789636255\n",
      "  Batch 9000/23979, Average Loss: 0.03210459881555289\n",
      "  Batch 10000/23979, Average Loss: 0.03207781123835594\n",
      "  Batch 11000/23979, Average Loss: 0.031608196205459536\n",
      "  Batch 12000/23979, Average Loss: 0.031540534885600206\n",
      "  Batch 13000/23979, Average Loss: 0.03228692282456905\n",
      "  Batch 14000/23979, Average Loss: 0.03157488663773984\n",
      "  Batch 15000/23979, Average Loss: 0.0317255275323987\n",
      "  Batch 16000/23979, Average Loss: 0.03184740127623081\n",
      "  Batch 17000/23979, Average Loss: 0.03125834571197629\n",
      "  Batch 18000/23979, Average Loss: 0.031078725762665273\n",
      "  Batch 19000/23979, Average Loss: 0.03142603351362049\n",
      "  Batch 20000/23979, Average Loss: 0.031311963996849955\n",
      "  Batch 21000/23979, Average Loss: 0.031004037904553115\n",
      "  Batch 22000/23979, Average Loss: 0.03129226224496961\n",
      "  Batch 23000/23979, Average Loss: 0.031184262930415572\n",
      "  Average Training Loss: 0.0315981433531137\n",
      "  Average Training Loss: 0.0315981433531137, Average Validation Loss: 0.031045980563541072\n",
      "Epoch 5/20: \n",
      "  Batch 1000/23979, Average Loss: 0.0308557569719851\n",
      "  Batch 2000/23979, Average Loss: 0.03017128877248615\n",
      "  Batch 3000/23979, Average Loss: 0.030206765648908913\n",
      "  Batch 4000/23979, Average Loss: 0.03025571899022907\n",
      "  Batch 5000/23979, Average Loss: 0.030971092493273317\n",
      "  Batch 6000/23979, Average Loss: 0.030223997486755253\n",
      "  Batch 7000/23979, Average Loss: 0.030344383121468126\n",
      "  Batch 8000/23979, Average Loss: 0.030064839098602532\n",
      "  Batch 9000/23979, Average Loss: 0.030070856018923224\n",
      "  Batch 10000/23979, Average Loss: 0.029949882198125125\n",
      "  Batch 11000/23979, Average Loss: 0.0296155947688967\n",
      "  Batch 12000/23979, Average Loss: 0.030626616542227567\n",
      "  Batch 13000/23979, Average Loss: 0.029987862723879516\n",
      "  Batch 14000/23979, Average Loss: 0.030117904724553227\n",
      "  Batch 15000/23979, Average Loss: 0.030910959700588138\n",
      "  Batch 16000/23979, Average Loss: 0.02963651323877275\n",
      "  Batch 17000/23979, Average Loss: 0.030036723273806274\n",
      "  Batch 18000/23979, Average Loss: 0.03020699576102197\n",
      "  Batch 19000/23979, Average Loss: 0.029870338634587823\n",
      "  Batch 20000/23979, Average Loss: 0.02985591531265527\n",
      "  Batch 21000/23979, Average Loss: 0.030279883821029217\n",
      "  Batch 22000/23979, Average Loss: 0.02937226752843708\n",
      "  Batch 23000/23979, Average Loss: 0.029936924006789923\n",
      "  Average Training Loss: 0.030157075541997265\n",
      "  Average Training Loss: 0.030157075541997265, Average Validation Loss: 0.02990812609334157\n",
      "Epoch 6/20: \n",
      "  Batch 1000/23979, Average Loss: 0.02984183658938855\n",
      "  Batch 2000/23979, Average Loss: 0.029951698268763722\n",
      "  Batch 3000/23979, Average Loss: 0.03010253288038075\n",
      "  Batch 4000/23979, Average Loss: 0.029982460007071495\n",
      "  Batch 5000/23979, Average Loss: 0.029351260423660278\n",
      "  Batch 6000/23979, Average Loss: 0.029482846180908382\n",
      "  Batch 7000/23979, Average Loss: 0.029653355123475195\n",
      "  Batch 8000/23979, Average Loss: 0.02933712552022189\n",
      "  Batch 9000/23979, Average Loss: 0.029425035532563926\n",
      "  Batch 10000/23979, Average Loss: 0.029767026846297086\n",
      "  Batch 11000/23979, Average Loss: 0.029376515380106865\n",
      "  Batch 12000/23979, Average Loss: 0.029074631377123298\n",
      "  Batch 13000/23979, Average Loss: 0.029377720365300775\n",
      "  Batch 14000/23979, Average Loss: 0.029779703984968365\n",
      "  Batch 15000/23979, Average Loss: 0.02903709067031741\n",
      "  Batch 16000/23979, Average Loss: 0.029444169944617897\n",
      "  Batch 17000/23979, Average Loss: 0.029500378038734197\n",
      "  Batch 18000/23979, Average Loss: 0.029277534211054446\n",
      "  Batch 19000/23979, Average Loss: 0.02946709172986448\n",
      "  Batch 20000/23979, Average Loss: 0.028750625401735307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 21000/23979, Average Loss: 0.029577743507921694\n",
      "  Batch 22000/23979, Average Loss: 0.028347486248239875\n",
      "  Batch 23000/23979, Average Loss: 0.02908583699539304\n",
      "  Average Training Loss: 0.029421241415445578\n",
      "  Average Training Loss: 0.029421241415445578, Average Validation Loss: 0.029361502650979164\n",
      "Epoch 7/20: \n",
      "  Batch 1000/23979, Average Loss: 0.029008431582711637\n",
      "  Batch 2000/23979, Average Loss: 0.028719141200184824\n",
      "  Batch 3000/23979, Average Loss: 0.02880688174813986\n",
      "  Batch 4000/23979, Average Loss: 0.0295560370227322\n",
      "  Batch 5000/23979, Average Loss: 0.02939521175902337\n",
      "  Batch 6000/23979, Average Loss: 0.029565492986701428\n",
      "  Batch 7000/23979, Average Loss: 0.029342502335086465\n",
      "  Batch 8000/23979, Average Loss: 0.029188428594730795\n",
      "  Batch 9000/23979, Average Loss: 0.029186086285859347\n",
      "  Batch 10000/23979, Average Loss: 0.028847674682736395\n",
      "  Batch 11000/23979, Average Loss: 0.028768260297365487\n",
      "  Batch 12000/23979, Average Loss: 0.028028117440640925\n",
      "  Batch 13000/23979, Average Loss: 0.02959061425924301\n",
      "  Batch 14000/23979, Average Loss: 0.028708652019500732\n",
      "  Batch 15000/23979, Average Loss: 0.028967767131514846\n",
      "  Batch 16000/23979, Average Loss: 0.02881547059863806\n",
      "  Batch 17000/23979, Average Loss: 0.028381593343801796\n",
      "  Batch 18000/23979, Average Loss: 0.029397683354094626\n",
      "  Batch 19000/23979, Average Loss: 0.028907153779640794\n",
      "  Batch 20000/23979, Average Loss: 0.028834657079540194\n",
      "  Batch 21000/23979, Average Loss: 0.02862480372702703\n",
      "  Batch 22000/23979, Average Loss: 0.028312717866152526\n",
      "  Batch 23000/23979, Average Loss: 0.028661890017800035\n",
      "  Average Training Loss: 0.02891020807437793\n",
      "  Average Training Loss: 0.02891020807437793, Average Validation Loss: 0.02877213785073738\n",
      "Epoch 8/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028966489244252443\n",
      "  Batch 2000/23979, Average Loss: 0.028458435205277056\n",
      "  Batch 3000/23979, Average Loss: 0.02862949670571834\n",
      "  Batch 4000/23979, Average Loss: 0.028235067847184835\n",
      "  Batch 5000/23979, Average Loss: 0.028734204051084816\n",
      "  Batch 6000/23979, Average Loss: 0.02837026697024703\n",
      "  Batch 7000/23979, Average Loss: 0.028421420677565038\n",
      "  Batch 8000/23979, Average Loss: 0.02890942445769906\n",
      "  Batch 9000/23979, Average Loss: 0.02851451689656824\n",
      "  Batch 10000/23979, Average Loss: 0.02837702865153551\n",
      "  Batch 11000/23979, Average Loss: 0.02853631546534598\n",
      "  Batch 12000/23979, Average Loss: 0.029183986376971005\n",
      "  Batch 13000/23979, Average Loss: 0.02863505308981985\n",
      "  Batch 14000/23979, Average Loss: 0.02916857495997101\n",
      "  Batch 15000/23979, Average Loss: 0.02854992999974638\n",
      "  Batch 16000/23979, Average Loss: 0.02885758716147393\n",
      "  Batch 17000/23979, Average Loss: 0.028163855148945004\n",
      "  Batch 18000/23979, Average Loss: 0.02845097948005423\n",
      "  Batch 19000/23979, Average Loss: 0.02836144624464214\n",
      "  Batch 20000/23979, Average Loss: 0.028472143174149096\n",
      "  Batch 21000/23979, Average Loss: 0.028980493034236134\n",
      "  Batch 22000/23979, Average Loss: 0.028829327737912535\n",
      "  Batch 23000/23979, Average Loss: 0.027991237283684314\n",
      "  Average Training Loss: 0.02858813959254061\n",
      "  Average Training Loss: 0.02858813959254061, Average Validation Loss: 0.028733297171858796\n",
      "Epoch 9/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028650876154191793\n",
      "  Batch 2000/23979, Average Loss: 0.028617558822967112\n",
      "  Batch 3000/23979, Average Loss: 0.028523103173822166\n",
      "  Batch 4000/23979, Average Loss: 0.028357752873562278\n",
      "  Batch 5000/23979, Average Loss: 0.028382232818752528\n",
      "  Batch 6000/23979, Average Loss: 0.028448862763121724\n",
      "  Batch 7000/23979, Average Loss: 0.02846521077957004\n",
      "  Batch 8000/23979, Average Loss: 0.02748043585475534\n",
      "  Batch 9000/23979, Average Loss: 0.028402655236888678\n",
      "  Batch 10000/23979, Average Loss: 0.02815196372475475\n",
      "  Batch 11000/23979, Average Loss: 0.027628894609864802\n",
      "  Batch 12000/23979, Average Loss: 0.028434185994789005\n",
      "  Batch 13000/23979, Average Loss: 0.028155964919365943\n",
      "  Batch 14000/23979, Average Loss: 0.02834057323774323\n",
      "  Batch 15000/23979, Average Loss: 0.029077918755821882\n",
      "  Batch 16000/23979, Average Loss: 0.028647255836054684\n",
      "  Batch 17000/23979, Average Loss: 0.028280132088810205\n",
      "  Batch 18000/23979, Average Loss: 0.028371785449329764\n",
      "  Batch 19000/23979, Average Loss: 0.028836195026524365\n",
      "  Batch 20000/23979, Average Loss: 0.028696595913730562\n",
      "  Batch 21000/23979, Average Loss: 0.0282721728971228\n",
      "  Batch 22000/23979, Average Loss: 0.028656650797463955\n",
      "  Batch 23000/23979, Average Loss: 0.02855918009299785\n",
      "  Average Training Loss: 0.028414537385871217\n",
      "  Average Training Loss: 0.028414537385871217, Average Validation Loss: 0.02850721945638168\n",
      "Epoch 10/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028380516741424798\n",
      "  Batch 2000/23979, Average Loss: 0.02873046319838613\n",
      "  Batch 3000/23979, Average Loss: 0.028249271751381456\n",
      "  Batch 4000/23979, Average Loss: 0.02824622174911201\n",
      "  Batch 5000/23979, Average Loss: 0.028655004656873644\n",
      "  Batch 6000/23979, Average Loss: 0.02783239776827395\n",
      "  Batch 7000/23979, Average Loss: 0.028194621688686312\n",
      "  Batch 8000/23979, Average Loss: 0.028292670777998866\n",
      "  Batch 9000/23979, Average Loss: 0.0283724673781544\n",
      "  Batch 10000/23979, Average Loss: 0.028675726101733745\n",
      "  Batch 11000/23979, Average Loss: 0.027927336203865706\n",
      "  Batch 12000/23979, Average Loss: 0.028549553222954275\n",
      "  Batch 13000/23979, Average Loss: 0.028039763811044394\n",
      "  Batch 14000/23979, Average Loss: 0.028427602406591177\n",
      "  Batch 15000/23979, Average Loss: 0.02845010043680668\n",
      "  Batch 16000/23979, Average Loss: 0.02859015634469688\n",
      "  Batch 17000/23979, Average Loss: 0.02832260249461979\n",
      "  Batch 18000/23979, Average Loss: 0.028181636876426636\n",
      "  Batch 19000/23979, Average Loss: 0.029195589931681754\n",
      "  Batch 20000/23979, Average Loss: 0.028214642728678883\n",
      "  Batch 21000/23979, Average Loss: 0.028255224869586527\n",
      "  Batch 22000/23979, Average Loss: 0.027816285985521973\n",
      "  Batch 23000/23979, Average Loss: 0.027806446705944836\n",
      "  Average Training Loss: 0.02831560272511064\n",
      "  Average Training Loss: 0.02831560272511064, Average Validation Loss: 0.02839507462427306\n",
      "Epoch 11/20: \n",
      "  Batch 1000/23979, Average Loss: 0.02795493893045932\n",
      "  Batch 2000/23979, Average Loss: 0.028246975408867\n",
      "  Batch 3000/23979, Average Loss: 0.027930527253076435\n",
      "  Batch 4000/23979, Average Loss: 0.028719247728586198\n",
      "  Batch 5000/23979, Average Loss: 0.028261788432952017\n",
      "  Batch 6000/23979, Average Loss: 0.0281730905296281\n",
      "  Batch 7000/23979, Average Loss: 0.028547512241639195\n",
      "  Batch 8000/23979, Average Loss: 0.02828844864014536\n",
      "  Batch 9000/23979, Average Loss: 0.02837663713749498\n",
      "  Batch 10000/23979, Average Loss: 0.028557490665465594\n",
      "  Batch 11000/23979, Average Loss: 0.028395984590053557\n",
      "  Batch 12000/23979, Average Loss: 0.028095354542136193\n",
      "  Batch 13000/23979, Average Loss: 0.028220010662451386\n",
      "  Batch 14000/23979, Average Loss: 0.02820404239743948\n",
      "  Batch 15000/23979, Average Loss: 0.02761878845561296\n",
      "  Batch 16000/23979, Average Loss: 0.028737252668943256\n",
      "  Batch 17000/23979, Average Loss: 0.028539337324909864\n",
      "  Batch 18000/23979, Average Loss: 0.02738647805340588\n",
      "  Batch 19000/23979, Average Loss: 0.028225688049104063\n",
      "  Batch 20000/23979, Average Loss: 0.02800165620353073\n",
      "  Batch 21000/23979, Average Loss: 0.02834941874211654\n",
      "  Batch 22000/23979, Average Loss: 0.028521759786643088\n",
      "  Batch 23000/23979, Average Loss: 0.028280778133310376\n",
      "  Average Training Loss: 0.028252403961649998\n",
      "  Average Training Loss: 0.028252403961649998, Average Validation Loss: 0.02834047840005587\n",
      "Epoch 12/20: \n",
      "  Batch 1000/23979, Average Loss: 0.02796317669376731\n",
      "  Batch 2000/23979, Average Loss: 0.02882573461905122\n",
      "  Batch 3000/23979, Average Loss: 0.02865438549919054\n",
      "  Batch 4000/23979, Average Loss: 0.028209181542508305\n",
      "  Batch 5000/23979, Average Loss: 0.02819830395746976\n",
      "  Batch 6000/23979, Average Loss: 0.028427810993045567\n",
      "  Batch 7000/23979, Average Loss: 0.028889177658129483\n",
      "  Batch 8000/23979, Average Loss: 0.028189035425893964\n",
      "  Batch 9000/23979, Average Loss: 0.028255489537492396\n",
      "  Batch 10000/23979, Average Loss: 0.028211612354964017\n",
      "  Batch 11000/23979, Average Loss: 0.02812681062426418\n",
      "  Batch 12000/23979, Average Loss: 0.028151617632247508\n",
      "  Batch 13000/23979, Average Loss: 0.02833440825622529\n",
      "  Batch 14000/23979, Average Loss: 0.028486498625949024\n",
      "  Batch 15000/23979, Average Loss: 0.028141734221950175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 16000/23979, Average Loss: 0.028141677890904247\n",
      "  Batch 17000/23979, Average Loss: 0.027660897412337362\n",
      "  Batch 18000/23979, Average Loss: 0.028203685553744435\n",
      "  Batch 19000/23979, Average Loss: 0.027633919565938414\n",
      "  Batch 20000/23979, Average Loss: 0.028140203473158182\n",
      "  Batch 21000/23979, Average Loss: 0.027983555810060352\n",
      "  Batch 22000/23979, Average Loss: 0.028174779187887907\n",
      "  Batch 23000/23979, Average Loss: 0.028136811026372017\n",
      "  Average Training Loss: 0.02820031115139891\n",
      "  Average Training Loss: 0.02820031115139891, Average Validation Loss: 0.028283015720070876\n",
      "Epoch 13/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028467169323004783\n",
      "  Batch 2000/23979, Average Loss: 0.028841550620272757\n",
      "  Batch 3000/23979, Average Loss: 0.028377679493278265\n",
      "  Batch 4000/23979, Average Loss: 0.02844914787262678\n",
      "  Batch 5000/23979, Average Loss: 0.02812730774562806\n",
      "  Batch 6000/23979, Average Loss: 0.027778722240589557\n",
      "  Batch 7000/23979, Average Loss: 0.027443490963429212\n",
      "  Batch 8000/23979, Average Loss: 0.027996866488829254\n",
      "  Batch 9000/23979, Average Loss: 0.028010283552110195\n",
      "  Batch 10000/23979, Average Loss: 0.028979158323723823\n",
      "  Batch 11000/23979, Average Loss: 0.028186120634898542\n",
      "  Batch 12000/23979, Average Loss: 0.028779575673863293\n",
      "  Batch 13000/23979, Average Loss: 0.027788473561406134\n",
      "  Batch 14000/23979, Average Loss: 0.028472252638079225\n",
      "  Batch 15000/23979, Average Loss: 0.02753864424023777\n",
      "  Batch 16000/23979, Average Loss: 0.02770176604669541\n",
      "  Batch 17000/23979, Average Loss: 0.027970194806344807\n",
      "  Batch 18000/23979, Average Loss: 0.02830721263680607\n",
      "  Batch 19000/23979, Average Loss: 0.028360177339985967\n",
      "  Batch 20000/23979, Average Loss: 0.027756252217106522\n",
      "  Batch 21000/23979, Average Loss: 0.02794948984775692\n",
      "  Batch 22000/23979, Average Loss: 0.027863625512458384\n",
      "  Batch 23000/23979, Average Loss: 0.028381989834364504\n",
      "  Average Training Loss: 0.028167410807032268\n",
      "  Average Training Loss: 0.028167410807032268, Average Validation Loss: 0.028160610256811373\n",
      "Epoch 14/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028073664722498507\n",
      "  Batch 2000/23979, Average Loss: 0.02812817483348772\n",
      "  Batch 3000/23979, Average Loss: 0.028147918374743314\n",
      "  Batch 4000/23979, Average Loss: 0.027827126217074692\n",
      "  Batch 5000/23979, Average Loss: 0.02859732498321682\n",
      "  Batch 6000/23979, Average Loss: 0.028200513347052036\n",
      "  Batch 7000/23979, Average Loss: 0.02795031267590821\n",
      "  Batch 8000/23979, Average Loss: 0.02781670008273795\n",
      "  Batch 9000/23979, Average Loss: 0.02845951761677861\n",
      "  Batch 10000/23979, Average Loss: 0.02826827368326485\n",
      "  Batch 11000/23979, Average Loss: 0.027833064316771926\n",
      "  Batch 12000/23979, Average Loss: 0.02836833996511996\n",
      "  Batch 13000/23979, Average Loss: 0.028670972945168613\n",
      "  Batch 14000/23979, Average Loss: 0.028099206519313157\n",
      "  Batch 15000/23979, Average Loss: 0.028516059511341153\n",
      "  Batch 16000/23979, Average Loss: 0.027193277544341983\n",
      "  Batch 17000/23979, Average Loss: 0.027861403205431998\n",
      "  Batch 18000/23979, Average Loss: 0.02839950727391988\n",
      "  Batch 19000/23979, Average Loss: 0.02777733982168138\n",
      "  Batch 20000/23979, Average Loss: 0.027989009339362384\n",
      "  Batch 21000/23979, Average Loss: 0.028373274113982917\n",
      "  Batch 22000/23979, Average Loss: 0.028148462567478418\n",
      "  Batch 23000/23979, Average Loss: 0.02909974411688745\n",
      "  Average Training Loss: 0.028157359818761364\n",
      "  Average Training Loss: 0.028157359818761364, Average Validation Loss: 0.028163702576091083\n",
      "Epoch 15/20: \n",
      "  Batch 1000/23979, Average Loss: 0.02852664809022099\n",
      "  Batch 2000/23979, Average Loss: 0.02827167603559792\n",
      "  Batch 3000/23979, Average Loss: 0.028061386353801936\n",
      "  Batch 4000/23979, Average Loss: 0.028030073361936955\n",
      "  Batch 5000/23979, Average Loss: 0.028476044743787496\n",
      "  Batch 6000/23979, Average Loss: 0.027903489753138274\n",
      "  Batch 7000/23979, Average Loss: 0.027942035366781057\n",
      "  Batch 8000/23979, Average Loss: 0.028199533243197947\n",
      "  Batch 9000/23979, Average Loss: 0.02824954227358103\n",
      "  Batch 10000/23979, Average Loss: 0.028252677957993\n",
      "  Batch 11000/23979, Average Loss: 0.028145363881252706\n",
      "  Batch 12000/23979, Average Loss: 0.02796927313879132\n",
      "  Batch 13000/23979, Average Loss: 0.028208428643643858\n",
      "  Batch 14000/23979, Average Loss: 0.02817925196979195\n",
      "  Batch 15000/23979, Average Loss: 0.027875001785811038\n",
      "  Batch 16000/23979, Average Loss: 0.028525060370564462\n",
      "  Batch 17000/23979, Average Loss: 0.028505791056901215\n",
      "  Batch 18000/23979, Average Loss: 0.028093552191741766\n",
      "  Batch 19000/23979, Average Loss: 0.028169455983676015\n",
      "  Batch 20000/23979, Average Loss: 0.028048326667398215\n",
      "  Batch 21000/23979, Average Loss: 0.02805385883897543\n",
      "  Batch 22000/23979, Average Loss: 0.02845540088415146\n",
      "  Batch 23000/23979, Average Loss: 0.027635336213745178\n",
      "  Average Training Loss: 0.028144251024040976\n",
      "  Average Training Loss: 0.028144251024040976, Average Validation Loss: 0.02842960698138596\n",
      "Epoch 16/20: \n",
      "  Batch 1000/23979, Average Loss: 0.02759459909144789\n",
      "  Batch 2000/23979, Average Loss: 0.02790504382085055\n",
      "  Batch 3000/23979, Average Loss: 0.028474232899025082\n",
      "  Batch 4000/23979, Average Loss: 0.028034681126009674\n",
      "  Batch 5000/23979, Average Loss: 0.027903858124278487\n",
      "  Batch 6000/23979, Average Loss: 0.02789558130595833\n",
      "  Batch 7000/23979, Average Loss: 0.028134590297937394\n",
      "  Batch 8000/23979, Average Loss: 0.028092587696388362\n",
      "  Batch 9000/23979, Average Loss: 0.027935745153576136\n",
      "  Batch 10000/23979, Average Loss: 0.027644982738886027\n",
      "  Batch 11000/23979, Average Loss: 0.02812887311074883\n",
      "  Batch 12000/23979, Average Loss: 0.028687543126754464\n",
      "  Batch 13000/23979, Average Loss: 0.028565697600133718\n",
      "  Batch 14000/23979, Average Loss: 0.02803241047170013\n",
      "  Batch 15000/23979, Average Loss: 0.028522797255776822\n",
      "  Batch 16000/23979, Average Loss: 0.027977932126261294\n",
      "  Batch 17000/23979, Average Loss: 0.02854469619691372\n",
      "  Batch 18000/23979, Average Loss: 0.028403377746231855\n",
      "  Batch 19000/23979, Average Loss: 0.028512008517049253\n",
      "  Batch 20000/23979, Average Loss: 0.02804202915728092\n",
      "  Batch 21000/23979, Average Loss: 0.027899727575015278\n",
      "  Batch 22000/23979, Average Loss: 0.02781678260769695\n",
      "  Batch 23000/23979, Average Loss: 0.028362387732835486\n",
      "  Average Training Loss: 0.028138683789469782\n",
      "  Average Training Loss: 0.028138683789469782, Average Validation Loss: 0.028143826916074383\n",
      "Epoch 17/20: \n",
      "  Batch 1000/23979, Average Loss: 0.027915983537212013\n",
      "  Batch 2000/23979, Average Loss: 0.028215386412106455\n",
      "  Batch 3000/23979, Average Loss: 0.027429770044982434\n",
      "  Batch 4000/23979, Average Loss: 0.028318488664925097\n",
      "  Batch 5000/23979, Average Loss: 0.028813160066492857\n",
      "  Batch 6000/23979, Average Loss: 0.028339835367165507\n",
      "  Batch 7000/23979, Average Loss: 0.0279595002932474\n",
      "  Batch 8000/23979, Average Loss: 0.028423277457244694\n",
      "  Batch 9000/23979, Average Loss: 0.0283217553710565\n",
      "  Batch 10000/23979, Average Loss: 0.02817187667451799\n",
      "  Batch 11000/23979, Average Loss: 0.028217442324385045\n",
      "  Batch 12000/23979, Average Loss: 0.02826528568007052\n",
      "  Batch 13000/23979, Average Loss: 0.028279140474274756\n",
      "  Batch 14000/23979, Average Loss: 0.02794602905213833\n",
      "  Batch 15000/23979, Average Loss: 0.027583199655171482\n",
      "  Batch 16000/23979, Average Loss: 0.02819705529883504\n",
      "  Batch 17000/23979, Average Loss: 0.028329137031454593\n",
      "  Batch 18000/23979, Average Loss: 0.028193735969718546\n",
      "  Batch 19000/23979, Average Loss: 0.028601168774068354\n",
      "  Batch 20000/23979, Average Loss: 0.02807634070888162\n",
      "  Batch 21000/23979, Average Loss: 0.02854032960906625\n",
      "  Batch 22000/23979, Average Loss: 0.027212797357700765\n",
      "  Batch 23000/23979, Average Loss: 0.02785882846871391\n",
      "  Average Training Loss: 0.028136625290599005\n",
      "  Average Training Loss: 0.028136625290599005, Average Validation Loss: 0.028197297624765187\n",
      "Epoch 18/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028539535717107354\n",
      "  Batch 2000/23979, Average Loss: 0.027955209714360534\n",
      "  Batch 3000/23979, Average Loss: 0.028319461860693992\n",
      "  Batch 4000/23979, Average Loss: 0.027773716061376037\n",
      "  Batch 5000/23979, Average Loss: 0.028105614505242557\n",
      "  Batch 6000/23979, Average Loss: 0.028586556821130215\n",
      "  Batch 7000/23979, Average Loss: 0.028462749079801144\n",
      "  Batch 8000/23979, Average Loss: 0.027978655023500325\n",
      "  Batch 9000/23979, Average Loss: 0.028398835361935198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 10000/23979, Average Loss: 0.027726919187698512\n",
      "  Batch 11000/23979, Average Loss: 0.028208635492250323\n",
      "  Batch 12000/23979, Average Loss: 0.02810528594814241\n",
      "  Batch 13000/23979, Average Loss: 0.02783232059562579\n",
      "  Batch 14000/23979, Average Loss: 0.02817724715732038\n",
      "  Batch 15000/23979, Average Loss: 0.02796593056898564\n",
      "  Batch 16000/23979, Average Loss: 0.027928048401139677\n",
      "  Batch 17000/23979, Average Loss: 0.028474394076503814\n",
      "  Batch 18000/23979, Average Loss: 0.028015530789270996\n",
      "  Batch 19000/23979, Average Loss: 0.02777188207861036\n",
      "  Batch 20000/23979, Average Loss: 0.02772146017290652\n",
      "  Batch 21000/23979, Average Loss: 0.02882117678178474\n",
      "  Batch 22000/23979, Average Loss: 0.02847335532400757\n",
      "  Batch 23000/23979, Average Loss: 0.027770984629169108\n",
      "  Average Training Loss: 0.028125026014901196\n",
      "  Average Training Loss: 0.028125026014901196, Average Validation Loss: 0.028722715681671996\n",
      "Epoch 19/20: \n",
      "  Batch 1000/23979, Average Loss: 0.028364180982112883\n",
      "  Batch 2000/23979, Average Loss: 0.028044008566997944\n",
      "  Batch 3000/23979, Average Loss: 0.028383380494080485\n",
      "  Batch 4000/23979, Average Loss: 0.0282012010817416\n",
      "  Batch 5000/23979, Average Loss: 0.027875535728409888\n",
      "  Batch 6000/23979, Average Loss: 0.027916016964241862\n",
      "  Batch 7000/23979, Average Loss: 0.028079005636274815\n",
      "  Batch 8000/23979, Average Loss: 0.02827030925359577\n",
      "  Batch 9000/23979, Average Loss: 0.0280318365637213\n",
      "  Batch 10000/23979, Average Loss: 0.027933903791941703\n",
      "  Batch 11000/23979, Average Loss: 0.02789066532300785\n",
      "  Batch 12000/23979, Average Loss: 0.02804092740267515\n",
      "  Batch 13000/23979, Average Loss: 0.028124578127171843\n",
      "  Batch 14000/23979, Average Loss: 0.02815778273995966\n",
      "  Batch 15000/23979, Average Loss: 0.027701503859832884\n",
      "  Batch 16000/23979, Average Loss: 0.02764922378025949\n",
      "  Batch 17000/23979, Average Loss: 0.02840376447699964\n",
      "  Batch 18000/23979, Average Loss: 0.027862516624853013\n",
      "  Batch 19000/23979, Average Loss: 0.028245846800506114\n",
      "  Batch 20000/23979, Average Loss: 0.0282105051856488\n",
      "  Batch 21000/23979, Average Loss: 0.02881109853181988\n",
      "  Batch 22000/23979, Average Loss: 0.027989878280088306\n",
      "  Batch 23000/23979, Average Loss: 0.027805153703317046\n",
      "  Average Training Loss: 0.02811451372852652\n",
      "  Average Training Loss: 0.02811451372852652, Average Validation Loss: 0.02825843892833086\n",
      "Early stopping at epoch 19. No improvement for 3 consecutive epochs.\n",
      "  Average Test Loss: 0.05621728579700655\n"
     ]
    }
   ],
   "source": [
    "result = experiment_one(datasetC, datasetB, hidden_layers_structure = [[5,3]], num_epochs=20, type_name=\"C+B/B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e7888d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment B/B： 1/1  ---   Model: [256, 128, 64, 32]\n",
      "Epoch 1/100: \n",
      "  Batch 1000/4804, Average Loss: 0.07041431397572159\n",
      "  Batch 2000/4804, Average Loss: 0.03435866070445627\n",
      "  Batch 3000/4804, Average Loss: 0.026077168472111226\n",
      "  Batch 4000/4804, Average Loss: 0.024725608385168015\n",
      "  Average Training Loss: 0.03628036365138645\n",
      "  Average Training Loss: 0.03628036365138645, Average Validation Loss: 0.02300904404381898\n",
      "Epoch 2/100: \n",
      "  Batch 1000/4804, Average Loss: 0.022576768238097428\n",
      "  Batch 2000/4804, Average Loss: 0.021889564252458513\n",
      "  Batch 3000/4804, Average Loss: 0.02161505646351725\n",
      "  Batch 4000/4804, Average Loss: 0.021813979926519095\n",
      "  Average Training Loss: 0.021792685170444682\n",
      "  Average Training Loss: 0.021792685170444682, Average Validation Loss: 0.02155299498308971\n",
      "Epoch 3/100: \n",
      "  Batch 1000/4804, Average Loss: 0.020565964926034214\n",
      "  Batch 2000/4804, Average Loss: 0.020538250632118435\n",
      "  Batch 3000/4804, Average Loss: 0.02043384804064408\n",
      "  Batch 4000/4804, Average Loss: 0.020255542315542698\n",
      "  Average Training Loss: 0.0203730430469331\n",
      "  Average Training Loss: 0.0203730430469331, Average Validation Loss: 0.02095200580026404\n",
      "Epoch 4/100: \n",
      "  Batch 1000/4804, Average Loss: 0.019766076497733594\n",
      "  Batch 2000/4804, Average Loss: 0.01973936924058944\n",
      "  Batch 3000/4804, Average Loss: 0.019406511864624918\n",
      "  Batch 4000/4804, Average Loss: 0.01903026822535321\n",
      "  Average Training Loss: 0.019342246694919917\n",
      "  Average Training Loss: 0.019342246694919917, Average Validation Loss: 0.018507836082424428\n",
      "Epoch 5/100: \n",
      "  Batch 1000/4804, Average Loss: 0.019015573870856315\n",
      "  Batch 2000/4804, Average Loss: 0.018589635004289447\n",
      "  Batch 3000/4804, Average Loss: 0.01858438938157633\n",
      "  Batch 4000/4804, Average Loss: 0.01821370672713965\n",
      "  Average Training Loss: 0.018551650091359773\n",
      "  Average Training Loss: 0.018551650091359773, Average Validation Loss: 0.018510136784323443\n",
      "Epoch 6/100: \n",
      "  Batch 1000/4804, Average Loss: 0.018305596339516342\n",
      "  Batch 2000/4804, Average Loss: 0.017924436600646004\n",
      "  Batch 3000/4804, Average Loss: 0.017498763248324393\n",
      "  Batch 4000/4804, Average Loss: 0.017672718193382026\n",
      "  Average Training Loss: 0.017816749176266\n",
      "  Average Training Loss: 0.017816749176266, Average Validation Loss: 0.017499023906765047\n",
      "Epoch 7/100: \n",
      "  Batch 1000/4804, Average Loss: 0.017055914626922457\n",
      "  Batch 2000/4804, Average Loss: 0.01755183432297781\n",
      "  Batch 3000/4804, Average Loss: 0.017186348726972937\n",
      "  Batch 4000/4804, Average Loss: 0.017310890873894095\n",
      "  Average Training Loss: 0.017278084258465942\n",
      "  Average Training Loss: 0.017278084258465942, Average Validation Loss: 0.01813811194412063\n",
      "Epoch 8/100: \n",
      "  Batch 1000/4804, Average Loss: 0.01690584982279688\n",
      "  Batch 2000/4804, Average Loss: 0.016464284527115524\n",
      "  Batch 3000/4804, Average Loss: 0.016849418406840413\n",
      "  Batch 4000/4804, Average Loss: 0.016697040965780615\n",
      "  Average Training Loss: 0.01665984065877545\n",
      "  Average Training Loss: 0.01665984065877545, Average Validation Loss: 0.016817247717654936\n",
      "Epoch 9/100: \n",
      "  Batch 1000/4804, Average Loss: 0.016190888207405805\n",
      "  Batch 2000/4804, Average Loss: 0.01628291096445173\n",
      "  Batch 3000/4804, Average Loss: 0.01612687163800001\n",
      "  Batch 4000/4804, Average Loss: 0.016379143543541433\n",
      "  Average Training Loss: 0.016214167438179942\n",
      "  Average Training Loss: 0.016214167438179942, Average Validation Loss: 0.01741034775933923\n",
      "Epoch 10/100: \n",
      "  Batch 1000/4804, Average Loss: 0.016037676251959055\n",
      "  Batch 2000/4804, Average Loss: 0.015911895116791128\n",
      "  Batch 3000/4804, Average Loss: 0.015566504649352282\n",
      "  Batch 4000/4804, Average Loss: 0.015925075219478457\n",
      "  Average Training Loss: 0.01579986778933161\n",
      "  Average Training Loss: 0.01579986778933161, Average Validation Loss: 0.016368275742045295\n",
      "Epoch 11/100: \n",
      "  Batch 1000/4804, Average Loss: 0.015291862447280436\n",
      "  Batch 2000/4804, Average Loss: 0.015420310814864934\n",
      "  Batch 3000/4804, Average Loss: 0.015266913590021431\n",
      "  Batch 4000/4804, Average Loss: 0.015585132443811744\n",
      "  Average Training Loss: 0.015352746139867517\n",
      "  Average Training Loss: 0.015352746139867517, Average Validation Loss: 0.014945024528048348\n",
      "Epoch 12/100: \n",
      "  Batch 1000/4804, Average Loss: 0.015032225892413407\n",
      "  Batch 2000/4804, Average Loss: 0.01476578468317166\n",
      "  Batch 3000/4804, Average Loss: 0.015001668832264841\n",
      "  Batch 4000/4804, Average Loss: 0.015227597157470883\n",
      "  Average Training Loss: 0.014990808411216756\n",
      "  Average Training Loss: 0.014990808411216756, Average Validation Loss: 0.014976177763593161\n",
      "Epoch 13/100: \n",
      "  Batch 1000/4804, Average Loss: 0.014693978209979832\n",
      "  Batch 2000/4804, Average Loss: 0.014603693725774065\n",
      "  Batch 3000/4804, Average Loss: 0.014575853535905481\n",
      "  Batch 4000/4804, Average Loss: 0.014768332422710955\n",
      "  Average Training Loss: 0.014630822231944613\n",
      "  Average Training Loss: 0.014630822231944613, Average Validation Loss: 0.014485464152010696\n",
      "Epoch 14/100: \n",
      "  Batch 1000/4804, Average Loss: 0.014300867252517491\n",
      "  Batch 2000/4804, Average Loss: 0.014320829479955137\n",
      "  Batch 3000/4804, Average Loss: 0.01392089320998639\n",
      "  Batch 4000/4804, Average Loss: 0.01421422788919881\n",
      "  Average Training Loss: 0.014269124272232526\n",
      "  Average Training Loss: 0.014269124272232526, Average Validation Loss: 0.014594843877744818\n",
      "Epoch 15/100: \n",
      "  Batch 1000/4804, Average Loss: 0.014247436334379018\n",
      "  Batch 2000/4804, Average Loss: 0.013931906313169748\n",
      "  Batch 3000/4804, Average Loss: 0.014006239685229956\n",
      "  Batch 4000/4804, Average Loss: 0.013701983654405921\n",
      "  Average Training Loss: 0.013928124197230649\n",
      "  Average Training Loss: 0.013928124197230649, Average Validation Loss: 0.014930304502584686\n",
      "Epoch 16/100: \n",
      "  Batch 1000/4804, Average Loss: 0.013569178630132229\n",
      "  Batch 2000/4804, Average Loss: 0.013424168159719557\n",
      "  Batch 3000/4804, Average Loss: 0.013488385829143227\n",
      "  Batch 4000/4804, Average Loss: 0.013678502281196415\n",
      "  Average Training Loss: 0.013539699171448776\n",
      "  Average Training Loss: 0.013539699171448776, Average Validation Loss: 0.014047178692468373\n",
      "Epoch 17/100: \n",
      "  Batch 1000/4804, Average Loss: 0.013141232887282967\n",
      "  Batch 2000/4804, Average Loss: 0.013292286177165807\n",
      "  Batch 3000/4804, Average Loss: 0.013195417167153209\n",
      "  Batch 4000/4804, Average Loss: 0.013202126297168434\n",
      "  Average Training Loss: 0.01313890003212274\n",
      "  Average Training Loss: 0.01313890003212274, Average Validation Loss: 0.013611765188219573\n",
      "Epoch 18/100: \n",
      "  Batch 1000/4804, Average Loss: 0.012847957753110676\n",
      "  Batch 2000/4804, Average Loss: 0.012854297791607678\n",
      "  Batch 3000/4804, Average Loss: 0.012735403357073665\n",
      "  Batch 4000/4804, Average Loss: 0.012637028352357447\n",
      "  Average Training Loss: 0.012778614166128348\n",
      "  Average Training Loss: 0.012778614166128348, Average Validation Loss: 0.013632140645990458\n",
      "Epoch 19/100: \n",
      "  Batch 1000/4804, Average Loss: 0.012068893664516508\n",
      "  Batch 2000/4804, Average Loss: 0.012508998742792755\n",
      "  Batch 3000/4804, Average Loss: 0.012629719136748463\n",
      "  Batch 4000/4804, Average Loss: 0.012352600997081027\n",
      "  Average Training Loss: 0.012375537952849042\n",
      "  Average Training Loss: 0.012375537952849042, Average Validation Loss: 0.012753488067487941\n",
      "Epoch 20/100: \n",
      "  Batch 1000/4804, Average Loss: 0.011769497029716148\n",
      "  Batch 2000/4804, Average Loss: 0.012239929595496506\n",
      "  Batch 3000/4804, Average Loss: 0.012062978531233967\n",
      "  Batch 4000/4804, Average Loss: 0.012005657601403073\n",
      "  Average Training Loss: 0.011960308451835938\n",
      "  Average Training Loss: 0.011960308451835938, Average Validation Loss: 0.012278882197269329\n",
      "Epoch 21/100: \n",
      "  Batch 1000/4804, Average Loss: 0.011762238590978085\n",
      "  Batch 2000/4804, Average Loss: 0.011579784209141507\n",
      "  Batch 3000/4804, Average Loss: 0.011656981139676646\n",
      "  Batch 4000/4804, Average Loss: 0.011751383151859045\n",
      "  Average Training Loss: 0.011666481702681166\n",
      "  Average Training Loss: 0.011666481702681166, Average Validation Loss: 0.012362844245285094\n",
      "Epoch 22/100: \n",
      "  Batch 1000/4804, Average Loss: 0.011375997728668154\n",
      "  Batch 2000/4804, Average Loss: 0.011357304229401052\n",
      "  Batch 3000/4804, Average Loss: 0.011415358903352171\n",
      "  Batch 4000/4804, Average Loss: 0.011023343209177255\n",
      "  Average Training Loss: 0.011271468897942968\n",
      "  Average Training Loss: 0.011271468897942968, Average Validation Loss: 0.012135843814880191\n",
      "Epoch 23/100: \n",
      "  Batch 1000/4804, Average Loss: 0.011090306981001049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 2000/4804, Average Loss: 0.010787296469090506\n",
      "  Batch 3000/4804, Average Loss: 0.011002828487427905\n",
      "  Batch 4000/4804, Average Loss: 0.010947890508454293\n",
      "  Average Training Loss: 0.010947752174726306\n",
      "  Average Training Loss: 0.010947752174726306, Average Validation Loss: 0.011425964036223524\n",
      "Epoch 24/100: \n",
      "  Batch 1000/4804, Average Loss: 0.010625278692459687\n",
      "  Batch 2000/4804, Average Loss: 0.010587128586601467\n",
      "  Batch 3000/4804, Average Loss: 0.010787090245168655\n",
      "  Batch 4000/4804, Average Loss: 0.010582336734514683\n",
      "  Average Training Loss: 0.010638304069325356\n",
      "  Average Training Loss: 0.010638304069325356, Average Validation Loss: 0.01136861880528139\n",
      "Epoch 25/100: \n",
      "  Batch 1000/4804, Average Loss: 0.010103421343723312\n",
      "  Batch 2000/4804, Average Loss: 0.010460703180870041\n",
      "  Batch 3000/4804, Average Loss: 0.010368582007475197\n",
      "  Batch 4000/4804, Average Loss: 0.010162951709935442\n",
      "  Average Training Loss: 0.010318070872850424\n",
      "  Average Training Loss: 0.010318070872850424, Average Validation Loss: 0.01107459498002639\n",
      "Epoch 26/100: \n",
      "  Batch 1000/4804, Average Loss: 0.00997281532571651\n",
      "  Batch 2000/4804, Average Loss: 0.009992567263543606\n",
      "  Batch 3000/4804, Average Loss: 0.01004930997500196\n",
      "  Batch 4000/4804, Average Loss: 0.009832804657518863\n",
      "  Average Training Loss: 0.01000068143573702\n",
      "  Average Training Loss: 0.01000068143573702, Average Validation Loss: 0.010694849377091274\n",
      "Epoch 27/100: \n",
      "  Batch 1000/4804, Average Loss: 0.009762696352787316\n",
      "  Batch 2000/4804, Average Loss: 0.009758817231515423\n",
      "  Batch 3000/4804, Average Loss: 0.009697890496114268\n",
      "  Batch 4000/4804, Average Loss: 0.009639019467169418\n",
      "  Average Training Loss: 0.00971106046115356\n",
      "  Average Training Loss: 0.00971106046115356, Average Validation Loss: 0.010543191169305504\n",
      "Epoch 28/100: \n",
      "  Batch 1000/4804, Average Loss: 0.009492818117840215\n",
      "  Batch 2000/4804, Average Loss: 0.009453606371534988\n",
      "  Batch 3000/4804, Average Loss: 0.00923359299916774\n",
      "  Batch 4000/4804, Average Loss: 0.009452218124642968\n",
      "  Average Training Loss: 0.009407939776494404\n",
      "  Average Training Loss: 0.009407939776494404, Average Validation Loss: 0.009855530055775524\n",
      "Epoch 29/100: \n",
      "  Batch 1000/4804, Average Loss: 0.009066509035183117\n",
      "  Batch 2000/4804, Average Loss: 0.009184097113320603\n",
      "  Batch 3000/4804, Average Loss: 0.009145359604386612\n",
      "  Batch 4000/4804, Average Loss: 0.009004167455947027\n",
      "  Average Training Loss: 0.009094771565347956\n",
      "  Average Training Loss: 0.009094771565347956, Average Validation Loss: 0.009868965001644123\n",
      "Epoch 30/100: \n",
      "  Batch 1000/4804, Average Loss: 0.008837858880404384\n",
      "  Batch 2000/4804, Average Loss: 0.008798461164114996\n",
      "  Batch 3000/4804, Average Loss: 0.008960321440594271\n",
      "  Batch 4000/4804, Average Loss: 0.008780569835333154\n",
      "  Average Training Loss: 0.00880894099159025\n",
      "  Average Training Loss: 0.00880894099159025, Average Validation Loss: 0.009563483870755658\n",
      "Epoch 31/100: \n",
      "  Batch 1000/4804, Average Loss: 0.00851775501575321\n",
      "  Batch 2000/4804, Average Loss: 0.008494594475487248\n",
      "  Batch 3000/4804, Average Loss: 0.00858340019523166\n",
      "  Batch 4000/4804, Average Loss: 0.008518752588192001\n",
      "  Average Training Loss: 0.008524771576218438\n",
      "  Average Training Loss: 0.008524771576218438, Average Validation Loss: 0.009548647318980329\n",
      "Epoch 32/100: \n",
      "  Batch 1000/4804, Average Loss: 0.008373067346168682\n",
      "  Batch 2000/4804, Average Loss: 0.008307676412397996\n",
      "  Batch 3000/4804, Average Loss: 0.008108365482883528\n",
      "  Batch 4000/4804, Average Loss: 0.008199145298684016\n",
      "  Average Training Loss: 0.008254472380034862\n",
      "  Average Training Loss: 0.008254472380034862, Average Validation Loss: 0.009242871239720615\n",
      "Epoch 33/100: \n",
      "  Batch 1000/4804, Average Loss: 0.008024305481696501\n",
      "  Batch 2000/4804, Average Loss: 0.007947336683981121\n",
      "  Batch 3000/4804, Average Loss: 0.007998955358518288\n",
      "  Batch 4000/4804, Average Loss: 0.007928561120061205\n",
      "  Average Training Loss: 0.007988434830797971\n",
      "  Average Training Loss: 0.007988434830797971, Average Validation Loss: 0.008876247822583595\n",
      "Epoch 34/100: \n",
      "  Batch 1000/4804, Average Loss: 0.007680548872565851\n",
      "  Batch 2000/4804, Average Loss: 0.0077979790852405135\n",
      "  Batch 3000/4804, Average Loss: 0.007717388157965616\n",
      "  Batch 4000/4804, Average Loss: 0.007622157701291144\n",
      "  Average Training Loss: 0.0077346541909024055\n",
      "  Average Training Loss: 0.0077346541909024055, Average Validation Loss: 0.008619550902324514\n",
      "Epoch 35/100: \n",
      "  Batch 1000/4804, Average Loss: 0.007335844977758825\n",
      "  Batch 2000/4804, Average Loss: 0.0074384670539293435\n",
      "  Batch 3000/4804, Average Loss: 0.0074092955528758464\n",
      "  Batch 4000/4804, Average Loss: 0.007489603647496551\n",
      "  Average Training Loss: 0.00745324898934958\n",
      "  Average Training Loss: 0.00745324898934958, Average Validation Loss: 0.008516956777987722\n",
      "Epoch 36/100: \n",
      "  Batch 1000/4804, Average Loss: 0.007177397479303181\n",
      "  Batch 2000/4804, Average Loss: 0.007122660181950778\n",
      "  Batch 3000/4804, Average Loss: 0.007318083379184827\n",
      "  Batch 4000/4804, Average Loss: 0.00740655480674468\n",
      "  Average Training Loss: 0.007226648366991446\n",
      "  Average Training Loss: 0.007226648366991446, Average Validation Loss: 0.008304630542767994\n",
      "Epoch 37/100: \n",
      "  Batch 1000/4804, Average Loss: 0.0069934048329014335\n",
      "  Batch 2000/4804, Average Loss: 0.006956635327776894\n",
      "  Batch 3000/4804, Average Loss: 0.007069583219708875\n",
      "  Batch 4000/4804, Average Loss: 0.0070417763364966955\n",
      "  Average Training Loss: 0.007010272653078619\n",
      "  Average Training Loss: 0.007010272653078619, Average Validation Loss: 0.007968909964684679\n",
      "Epoch 38/100: \n",
      "  Batch 1000/4804, Average Loss: 0.006628954199259169\n",
      "  Batch 2000/4804, Average Loss: 0.0067871225689304995\n",
      "  Batch 3000/4804, Average Loss: 0.006773357746191323\n",
      "  Batch 4000/4804, Average Loss: 0.006797549148788676\n",
      "  Average Training Loss: 0.006755271785050013\n",
      "  Average Training Loss: 0.006755271785050013, Average Validation Loss: 0.007770597491073564\n",
      "Epoch 39/100: \n",
      "  Batch 1000/4804, Average Loss: 0.00647996125346981\n",
      "  Batch 2000/4804, Average Loss: 0.006639488394022919\n",
      "  Batch 3000/4804, Average Loss: 0.006494022926548496\n",
      "  Batch 4000/4804, Average Loss: 0.0067112868325784805\n",
      "  Average Training Loss: 0.006574092239937664\n",
      "  Average Training Loss: 0.006574092239937664, Average Validation Loss: 0.0076836827409499575\n",
      "Epoch 40/100: \n",
      "  Batch 1000/4804, Average Loss: 0.0064016675426391886\n",
      "  Batch 2000/4804, Average Loss: 0.006291530977119692\n",
      "  Batch 3000/4804, Average Loss: 0.00639979442069307\n",
      "  Batch 4000/4804, Average Loss: 0.006440244243363849\n",
      "  Average Training Loss: 0.006379699676793315\n",
      "  Average Training Loss: 0.006379699676793315, Average Validation Loss: 0.008063937014045713\n",
      "Epoch 41/100: \n",
      "  Batch 1000/4804, Average Loss: 0.006155586208216846\n",
      "  Batch 2000/4804, Average Loss: 0.006141167536610737\n",
      "  Batch 3000/4804, Average Loss: 0.006143868376035243\n",
      "  Batch 4000/4804, Average Loss: 0.006245490282773971\n",
      "  Average Training Loss: 0.00616694207535475\n",
      "  Average Training Loss: 0.00616694207535475, Average Validation Loss: 0.00758912972919891\n",
      "Epoch 42/100: \n",
      "  Batch 1000/4804, Average Loss: 0.0059189946453552696\n",
      "  Batch 2000/4804, Average Loss: 0.00594306692306418\n",
      "  Batch 3000/4804, Average Loss: 0.006106080591445789\n",
      "  Batch 4000/4804, Average Loss: 0.006034584125038237\n",
      "  Average Training Loss: 0.00599307291296258\n",
      "  Average Training Loss: 0.00599307291296258, Average Validation Loss: 0.007241061291212189\n",
      "Epoch 43/100: \n",
      "  Batch 1000/4804, Average Loss: 0.00569851472042501\n",
      "  Batch 2000/4804, Average Loss: 0.005835104296682402\n",
      "  Batch 3000/4804, Average Loss: 0.005793633261695504\n",
      "  Batch 4000/4804, Average Loss: 0.005746217058505863\n",
      "  Average Training Loss: 0.005788566459254901\n",
      "  Average Training Loss: 0.005788566459254901, Average Validation Loss: 0.007042953670899149\n",
      "Epoch 44/100: \n",
      "  Batch 1000/4804, Average Loss: 0.005520985555485822\n",
      "  Batch 2000/4804, Average Loss: 0.005606598882935941\n",
      "  Batch 3000/4804, Average Loss: 0.005617101048352197\n",
      "  Batch 4000/4804, Average Loss: 0.0056607919902307915\n",
      "  Average Training Loss: 0.0056178235686089465\n",
      "  Average Training Loss: 0.0056178235686089465, Average Validation Loss: 0.006758218021711803\n",
      "Epoch 45/100: \n",
      "  Batch 1000/4804, Average Loss: 0.005382668231613934\n",
      "  Batch 2000/4804, Average Loss: 0.0053689239725936205\n",
      "  Batch 3000/4804, Average Loss: 0.005601505524478852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 4000/4804, Average Loss: 0.005405008230707608\n",
      "  Average Training Loss: 0.005444853664076322\n",
      "  Average Training Loss: 0.005444853664076322, Average Validation Loss: 0.0068784509853679395\n",
      "Epoch 46/100: \n",
      "  Batch 1000/4804, Average Loss: 0.005250630938448012\n",
      "  Batch 2000/4804, Average Loss: 0.005325668209698051\n",
      "  Batch 3000/4804, Average Loss: 0.005265310528688133\n",
      "  Batch 4000/4804, Average Loss: 0.005337684755562805\n",
      "  Average Training Loss: 0.0053048599949275235\n",
      "  Average Training Loss: 0.0053048599949275235, Average Validation Loss: 0.006759704864018419\n",
      "Epoch 47/100: \n",
      "  Batch 1000/4804, Average Loss: 0.005126682474394329\n",
      "  Batch 2000/4804, Average Loss: 0.005171467756154016\n",
      "  Batch 3000/4804, Average Loss: 0.005140641405363567\n",
      "  Batch 4000/4804, Average Loss: 0.005098939719377086\n",
      "  Average Training Loss: 0.005145837770263935\n",
      "  Average Training Loss: 0.005145837770263935, Average Validation Loss: 0.006339112081231373\n",
      "Epoch 48/100: \n",
      "  Batch 1000/4804, Average Loss: 0.004943605681066401\n",
      "  Batch 2000/4804, Average Loss: 0.004962872024509125\n",
      "  Batch 3000/4804, Average Loss: 0.005175808491301723\n",
      "  Batch 4000/4804, Average Loss: 0.005036034836899489\n",
      "  Average Training Loss: 0.005009680666577313\n",
      "  Average Training Loss: 0.005009680666577313, Average Validation Loss: 0.0065235651525874655\n",
      "Epoch 49/100: \n",
      "  Batch 1000/4804, Average Loss: 0.00474462326860521\n",
      "  Batch 2000/4804, Average Loss: 0.0049182016043923795\n",
      "  Batch 3000/4804, Average Loss: 0.00487839099438861\n",
      "  Batch 4000/4804, Average Loss: 0.004991111682960764\n",
      "  Average Training Loss: 0.00488243288109773\n",
      "  Average Training Loss: 0.00488243288109773, Average Validation Loss: 0.006393209016217184\n",
      "Epoch 50/100: \n",
      "  Batch 1000/4804, Average Loss: 0.004604355631629005\n",
      "  Batch 2000/4804, Average Loss: 0.004656762782367878\n",
      "  Batch 3000/4804, Average Loss: 0.004817292917403393\n",
      "  Batch 4000/4804, Average Loss: 0.004791544232401065\n",
      "  Average Training Loss: 0.004731938805941765\n",
      "  Average Training Loss: 0.004731938805941765, Average Validation Loss: 0.006485113519716378\n",
      "Early stopping at epoch 50. No improvement for 3 consecutive epochs.\n",
      "  Average Test Loss: 0.006363849103843156\n"
     ]
    }
   ],
   "source": [
    "result = experiment_one(datasetC, datasetB, hidden_layers_structure = [[256,128,64,32]], num_epochs=100, type_name=\"B/B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533bce1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
